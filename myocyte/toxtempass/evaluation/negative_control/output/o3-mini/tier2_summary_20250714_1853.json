{
  "timestamp": "20250714_1853",
  "records": [
    {
      "file": "human-exposure-to-bisphenol-a.pdf",
      "LLM_challenge_level": 1,
      "passes": 63,
      "total": 77,
      "pass_rate": 81.82,
      "failures": [
        {
          "question_id": 1,
          "question_text": "Provide a descriptive title using normal language without technical terms or acronyms.",
          "answer_text": "Everyday Exposure: How a Common Chemical in Food Containers Is Impacting People in Europe  \n_(Source: human-exposure-to-bisphenol-a.pdf)_"
        },
        {
          "question_id": 2,
          "question_text": "Please describe in no more than 200 words the following:",
          "answer_text": "The document provides an in\u2011depth overview of human exposure to Bisphenol A (BPA) in Europe. BPA is a synthetic chemical used in high volumes for manufacturing polymers like polycarbonate plastics and epoxy resins found in everyday consumer products, notably food packaging. People are primarily exposed to BPA through dietary intake when residual BPA migrates from food contact materials into food and beverages, although other routes such as dental sealants, medical devices, and handling thermal paper also contribute. The briefing highlights that BPA is an endocrine disruptor linked to adverse health effects including reproductive toxicity, immune system impairment, and possible developmental effects, with evidence suggesting that current exposure levels in Europe far exceed the updated safety thresholds set by EFSA. It explains that data from an EU-wide human biomonitoring initiative (HBM4EU) detected BPA in the urine of 92% of adults, indicating widespread exposure. Moreover, the report discusses evolving regulatory measures aimed at reducing BPA exposure and the challenges posed by its substitutes, emphasizing ongoing public health concerns regarding BPA in the European population.  \n_(Source: human-exposure-to-bisphenol-a.pdf)_"
        },
        {
          "question_id": 3,
          "question_text": "Which toxicological target (organ, tissue, physiological/biochemical function, etc.) is modelled? (8.1)",
          "answer_text": "The assay models the impact of BPA on the immune system. In particular, the updated EFSA tolerable daily intake (TDI) is based on studies showing that BPA affects cells critical to immune mechanisms\u2014effects that can lead to inflammatory conditions, including autoimmunity and lung inflammation _(Source: human-exposure-to-bisphenol-a.pdf)_."
        },
        {
          "question_id": 4,
          "question_text": "Which test system and readout(s) are used? (4.1; 5.2)",
          "answer_text": "The assay utilizes a human biomonitoring test system in which urine samples from adults across various European countries are analyzed. The readout is the quantification of total urinary BPA levels (in ng/L) which is then compared against a human biomonitoring guidance value (HBM\u2010GV of 11.5 ng/L for adults, derived from the new EFSA tolerable daily intake). (Sources: human-exposure-to-bisphenol-a.pdf, Box 2 and related passages)"
        },
        {
          "question_id": 5,
          "question_text": "Which biological process(es) (e.g. neurite outgrowth, differentiation) and/or toxicological events (e.g. oxidative stress, cell death) are modelled/reflected by your test method? (8.1)",
          "answer_text": "The document does not describe a cell\u2010based test method that directly models specific cellular processes (such as neurite outgrowth, differentiation, oxidative stress, or cell death). Instead, it reports on human biomonitoring data that quantify BPA exposure (i.e. urinary BPA levels) and use these data to assess potential health risks linked to BPA\u2019s endocrine\u2010disrupting properties (e.g. effects on reproductive and immune function, as well as impacts on metabolism and development). In short, while the assay does not measure cellular toxicological events directly, it reflects overall exposure that is known to be associated with adverse endocrine and related health effects.  \n_(Source: human-exposure-to-bisphenol-a.pdf)_"
        },
        {
          "question_id": 6,
          "question_text": "To which (human) adverse outcome(s) is your test method related or could be related? (8.1; 9.2; 9.3)",
          "answer_text": "The assay is related to adverse outcomes stemming from BPA\u2019s endocrine\u2010disrupting properties. BPA exposure has been associated with impacts on reproductive health (e.g. decreased fertility and altered mammary gland development), immune system function (with potential links to inflammatory and allergic conditions), as well as potential effects on cognitive function and metabolism. These outcomes have been discussed in the context of BPA\u2019s ability to alter hormone system functioning and induce cellular effects even at very low doses _(Sources: human-exposure-to-bisphenol-a.pdf)_."
        },
        {
          "question_id": 7,
          "question_text": "Which hazard(s) do(es) your test method (potentially) predict? (8.1; 8.6)",
          "answer_text": "The method can potentially predict hazards related to endocrine disruption. In particular, it is designed to flag adverse effects on the hormone system that may lead to reproductive toxicity (including impaired fertility and altered mammary gland development), as well as negative impacts on the immune system (which can contribute to inflammatory conditions and allergies). It is also associated with hazards such as serious eye damage, allergic skin reactions, and respiratory irritation. (Sources: human-exposure-to-bisphenol-a.pdf)"
        },
        {
          "question_id": 8,
          "question_text": "Does the test method capture an endpoint of current regulatory studies? (9.5)",
          "answer_text": "The method does not capture a cellular or mechanistic endpoint used in current regulatory toxicology studies. Instead, it quantifies human urinary BPA levels and compares these to a human biomonitoring guidance value that was derived from the updated EFSA tolerable daily intake\u2014which in turn was based on effects (for example, on immune system cells) observed in toxicological studies. However, the assay itself only measures BPA exposure in urine rather than directly capturing the underlying cellular endpoints. (Sources: human-exposure-to-bisphenol-a.pdf, Box 2 and related sections)"
        },
        {
          "question_id": 10,
          "question_text": "Provide the original/published name, as well as the potential tradename.",
          "answer_text": "The published name is \u201cHuman exposure to Bisphenol A in Europe.\u201d No potential tradename is provided in the document. (Sources: human-exposure-to-bisphenol-a.pdf)"
        },
        {
          "question_id": 12,
          "question_text": "This only applies to updated versions. If this is the original version, state 'original version'.",
          "answer_text": "updated version (Source: human-exposure-to-bisphenol-a.pdf)"
        },
        {
          "question_id": 13,
          "question_text": "Normal text names often do not uniquely define the method. Therefore, each method should be assigned a clearly and uniquely defined data base name.",
          "answer_text": "The document shows that relying on a normal title (for example, \u201cHuman exposure to Bisphenol A in Europe\u201d) isn\u2019t sufficient to uniquely define the method. Instead, the assay has been assigned a unique set of identifiers\u2014for instance, the briefing number (Briefing no. 17/2023) and the specific file identifiers (TH-AM-23-021-EN-Q for the HTML version and TH-AM-23-021-EN-N for the PDF version). Assigning such clearly defined database names ensures that the method is uniquely referenced and eliminates ambiguity when searching or linking data within a database. (Source: human-exposure-to-bisphenol-a.pdf)"
        },
        {
          "question_id": 52,
          "question_text": "Indicate the names (and database names) of related tests and give a short description (including a brief comment on differences to the present method). If the test method has been used for high throughput transcriptomics or deep sequencing as alternative endpoint, this should be indicated.",
          "answer_text": "No related tests with different names or database titles are described in the document. The briefing solely presents a human\u2010biomonitoring method for BPA exposure using urine samples (with data displayed via the HBM4EU Dashboard) and does not report alternative or complementary methods\u2014such as assays measuring high throughput transcriptomics or deep sequencing endpoints\u2014nor does it list separate test names/databases that differ from the present method. (Sources: human\u2010exposure\u2010to\u2010bisphenol\u2010a.pdf)"
        },
        {
          "question_id": 60,
          "question_text": "What is the scientific rationale to link test method data to a relevant in vivo adverse outcome? Which toxicological target (organ, tissue, physiological/biochemical function, etc.) is modelled? Which biological process(es) (e.g. neurite outgrowth, differentiation) are modelled/reflected by your test method? Which toxicological events (e.g. oxidative stress, cell death) are modelled/reflected by your test method? To which (human) adverse outcome(s) is your test method related? Which hazard(s) do(es) your test method (potentially) predict?",
          "answer_text": "The human\u2010exposure document for BPA explains that BPA is an endocrine disruptor, and its toxicity is largely understood from studies showing that even very low doses can interfere with hormone\u2010regulated processes. Although the document is not reporting a specific cell\u2010based test method, it underpins the rationale for linking laboratory (or in vitro) endpoints to adverse outcomes in vivo. For example, test methods that monitor cellular responses to BPA (such as changes in hormone receptor activity, altered cytokine or immune cell function, or effects on cell differentiation) are designed based on the knowledge that disruption of endocrine signalling can lead to adverse outcomes\u2014including reproductive toxicity, developmental disturbances, and immune dysregulation. \n\nTo break this down further:\n\n\u2022 Scientific rationale for linking test data to adverse outcomes: BPA has been shown to interfere with cell signalling pathways (e.g. hormone receptor binding) and to alter the function of critical cellular processes that regulate fertility, development, and immune responses. This mechanistic understanding (as reviewed by EFSA and HBM4EU) provides the rationale for expecting that effects measured in a cell\u2010based assay (such as a change in receptor activity, cell proliferation, or cell viability) may predict adverse outcomes in vivo (e.g. reproductive toxicity or immune-related diseases). _(Sources: EFSA, 2023a; HBM4EU, 2022b)_\n\n\u2022 Toxicological target modeled: The available evidence pinpoints the endocrine system as the primary target, including reproductive tissues and immune cells. In toxicological testing, these systems might be mirrored by using cell lines that express the relevant hormone receptors or immune markers. _(Sources: EFSA, 2023a; ECHA, 2017)_\n\n\u2022 Biological processes modeled: Tests focused on BPA generally reflect processes such as receptor\u2010mediated signalling, cell differentiation, and possibly oxidative stress and apoptosis. For example, if a test measures receptor activation or changes in gene transcription following BPA exposure, it aims to mimic the disruption of normal endocrine signalling that is important for processes like mammary gland development and cellular immune responses. _(Sources: EFSA, 2023a; HBM4EU, 2022b)_\n\n\u2022 Toxicological events modeled: The cellular endpoints would typically include markers of endocrine disruption (altered receptor activation, hormone synthesis), oxidative stress and inflammation, and, in some cases, cytotoxicity (cell death) resulting from the chemical\u2019s interference with normal cell functioning. _(Sources: EFSA, 2023a; HBM4EU, 2022b)_\n\n\u2022 Related human adverse outcomes: The results of such test methods are used to predict adverse outcomes in humans such as reproductive disorders (e.g. fertility problems), developmental issues in offspring, immune dysfunction, and possibly metabolic disturbances. In effect, data from cell\u2010based assays help predict whether exposure levels seen in populations (as determined in biomonitoring studies) might translate into long\u2010term health concerns. _(Sources: EFSA, 2023a; HBM4EU, 2022d)_\n\n\u2022 Hazards potentially predicted: In sum, the test method would potentially predict hazards related to endocrine disruption\u2014including reproductive toxicity, developmental toxicity, immune system impairment, and other related adverse health effects. _(Sources: EFSA, 2023a; HBM4EU, 2022b)_\n\nThus, while the document itself does not detail a specific in vitro or cell\u2010based assay protocol, the underpinning toxicological evidence provides a strong basis for developing and interpreting test method data within the context of predicting endocrine\u2010mediated adverse outcomes in humans."
        },
        {
          "question_id": 71,
          "question_text": "Indicate whether the test method is linked to an OECD Test Guideline (how, and which) or other regulatory guidance (e.g. EMA).",
          "answer_text": "The document does not provide any details on a cell-based toxicological test method that is linked to an OECD Test Guideline or another piece of regulatory guidance such as EMA. Instead, it focuses on human biomonitoring data and associated regulatory evaluations related to Bisphenol A exposure in Europe. (Source: human-exposure-to-bisphenol-a.pdf)"
        }
      ]
    },
    {
      "file": "seasonal-influenza-annual-epidemiological-report-2023-2024.pdf",
      "LLM_challenge_level": 1,
      "passes": 67,
      "total": 77,
      "pass_rate": 87.01,
      "failures": [
        {
          "question_id": 1,
          "question_text": "Provide a descriptive title using normal language without technical terms or acronyms.",
          "answer_text": "A Simple Overview of the 2023\u20132024 Flu Season: When It Happened, How It Spread, and How Well Vaccination Helped. (Source: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)"
        },
        {
          "question_id": 5,
          "question_text": "Which biological process(es) (e.g. neurite outgrowth, differentiation) and/or toxicological events (e.g. oxidative stress, cell death) are modelled/reflected by your test method? (8.1)",
          "answer_text": "This assay is an epidemiological surveillance report of seasonal influenza and does not model or reflect cell\u2010based biological processes (such as neurite outgrowth or differentiation) or toxicological events (such as oxidative stress or cell death). Its focus is on the surveillance of influenza virus circulation, characterization, and vaccine effectiveness in populations. (Sources: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)"
        },
        {
          "question_id": 6,
          "question_text": "To which (human) adverse outcome(s) is your test method related or could be related? (8.1; 9.2; 9.3)",
          "answer_text": "The test method (i.e. the surveillance system and associated laboratory\u2010based methods) is used to detect and monitor influenza virus infections that underpin adverse outcomes in humans. These include:\n\n\u2022 Influenza\u2010like illness and acute respiratory infections (in primary care, indicating symptomatic infection)  \n\u2022 Severe acute respiratory infections (SARI), which represent serious complications of influenza  \n\u2022 Hospitalisations\u2014including cases admitted to intensive care units  \n\u2022 Influenza\u2010associated deaths\n\nThese outcomes are documented in multiple sections of the report (e.g. primary care sentinel surveillance, SARI surveillance, and hospitalisation data) _(Source: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)_."
        },
        {
          "question_id": 7,
          "question_text": "Which hazard(s) do(es) your test method (potentially) predict? (8.1; 8.6)",
          "answer_text": "The report does not describe a toxicological test predicting classical toxicological hazards. Instead, this surveillance report offers a prediction of public health hazards associated with seasonal influenza. Specifically, by tracking influenza activity it can predict hazards such as:\n\n\u2022 A surge in influenza virus circulation that may lead to widespread respiratory illness and increased hospitalisations and deaths, particularly among vulnerable groups (e.g. those aged 60 years and older).  \n\u2022 The emergence of influenza virus strains with changes in antigenicity, which may result in reduced vaccine effectiveness.  \n\u2022 The potential development of antiviral resistance (eg, the reported increase in influenza viruses with reduced susceptibility to oseltamivir).  \n\nThese epidemiological indicators help public health authorities anticipate and prepare for seasonal influenza threats and assess the overall burden of the disease.  \n_(Sources: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)_"
        },
        {
          "question_id": 10,
          "question_text": "Provide the original/published name, as well as the potential tradename.",
          "answer_text": "The report\u2019s official, published title is \u201cSeasonal influenza, 2023\u22122024\u201d and it is cited as part of the \u201cAnnual Epidemiological Report for 2023\u201d from ECDC. In contexts where a shorter or more marketable name is preferred, it may be referred to as the \u201cECDC Annual Epidemiological Report for 2023 \u2013 Seasonal Influenza\u201d (Sources: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)."
        },
        {
          "question_id": 12,
          "question_text": "This only applies to updated versions. If this is the original version, state 'original version'.",
          "answer_text": "original version (Source: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)"
        },
        {
          "question_id": 14,
          "question_text": "Include affiliation.",
          "answer_text": "The report is produced by the European Centre for Disease Prevention and Control (ECDC), which is based in Stockholm, Sweden (Source: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)."
        },
        {
          "question_id": 50,
          "question_text": "Does the method require specialized instrumentation that is not found in standard laboratories? Is there a need for custom-made instrumentation or material? Is there a need for equipment that is not commercially available (anymore)?",
          "answer_text": "Based on the information provided in the report, there is no indication that the surveillance methods used require instrumentation that is not already available in standard laboratories. The report describes approaches such as testing specimens from sentinel sources, genetic characterisation, and antiviral susceptibility assessments that are performed using established, commercially available laboratory techniques. There is no mention of the need for custom\u2011made equipment or instrumentation that is not commercially available. \n\n(Source: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)"
        },
        {
          "question_id": 52,
          "question_text": "Indicate the names (and database names) of related tests and give a short description (including a brief comment on differences to the present method). If the test method has been used for high throughput transcriptomics or deep sequencing as alternative endpoint, this should be indicated.",
          "answer_text": "1. Sentinel Surveillance (data submitted via TESSy) \u2013 This method collects specimens from patients with influenza-like or acute respiratory infections seen by primary care physicians. It is designed primarily for monitoring the start, duration, and intensity of the seasonal epidemic by using a systematic sample and applying a 10% positivity threshold. Compared with the broader \u201cpresent\u201d surveillance approach that also includes non-sentinel and severe case data, sentinel surveillance focuses on early trend detection using aggregated specimen positivity from selected clinical sites. _(Source: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)_\n\n2. Non\u2010Sentinel Surveillance (aggregated data in ECDC databases) \u2013 This approach tests specimens obtained from hospitals, clinics, nursing homes, and other non\u2010sentinel settings. It covers a broader population than sentinel surveillance. The method provides information on influenza virus circulation across various healthcare settings and complements the sentinel data. While the present report integrates both types, non\u2010sentinel surveillance typically involves higher overall specimen numbers, with potentially different testing protocols and reporting procedures. _(Source: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)_\n\n3. SARI-Based Surveillance (data aggregated at the national level) \u2013 This method concentrates on the severe acute respiratory infection (SARI) cases in hospital settings. It allows monitoring of the most severe cases of influenza infection and is used to track hospitalisations and fatalities. In contrast to the present method, which also includes mild and moderate cases from primary care, SARI-based surveillance targets severe disease endpoints. _(Source: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)_\n\n4. Genetic Characterisation Testing (results reported to TESSy) \u2013 Although not used as an alternative endpoint in the classical epidemiological analysis, genetic characterisation employs advanced sequencing techniques (including deep sequencing) to determine virus clades and to monitor genetic changes. This approach may use high throughput transcriptomics for detailed virus analysis and provides a molecular complement to the conventional diagnostic tests. It differs from \u201cpresent\u201d methods that rely on aggregate positivity rates by offering strain-specific data, which is critical for assessing vaccine match and antiviral susceptibility. _(Source: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)_\n\nThere is no explicit indication in the document that high throughput transcriptomics or deep sequencing has routinely been used as an alternative endpoint outside of the detailed genetic characterisation assays. However, these techniques are integral to the modern genetic analysis of circulating influenza viruses."
        },
        {
          "question_id": 66,
          "question_text": "Does the test fit into a test battery? If yes, into which test battery and are there any restrictions? Indicate potential strengths and weaknesses of the system in a test battery (e.g. method is a good confirmation assay, good for creating alerts, mechanistic follow-up, screening, etc.). Compare performance to similar tests. Which gaps in a known or potential battery does the test method fill? Should the test preferentially be used in the first tier or later tiers, are complementary assays required or is it a stand-alone method?",
          "answer_text": "The report does not describe a \u201ccell\u2010based toxicological assay\u201d per se but rather a comprehensive influenza surveillance system that is implemented as a kind of test battery. In this system, several complementary \u201cassays\u201d or data streams are used in combination to deliver a full picture of circulating influenza viruses. For example, primary care sentinel surveillance (with systematic sampling and test\u2010positivity analysis) is complemented by non\u2010sentinel testing (using hospital and other clinical sources), severe acute respiratory infection (SARI) surveillance, hospital-based laboratory confirmations, detailed virus genetic characterisation, and antiviral susceptibility testing (Sources: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf).\n\nStrengths and weaknesses within this battery include: \n\n\u2022 Strengths:\n\u2003\u2013 The integration of various surveillance components allows for early alert and trend detection (screening) as well as confirmation and further characterisation (mechanistic follow-up) of circulating influenza viruses.\n\u2003\u2013 The array of methods (e.g. sentinel versus non\u2010sentinel, genetic and antigenic analyses, and antiviral resistance data) provides complementary information on both the epidemiological spread and virus evolution.\n\u2003\u2013 The system is well suited for monitoring vaccine effectiveness and virus mismatches, thereby contributing to public health responses.\n\n\u2022 Weaknesses/Restrictions:\n\u2003\u2013 Some sectors (e.g. hospitalised case reporting or severe disease surveillance) are reported by a limited number of countries, which can affect the representativeness and comparability of data.\n\u2003\u2013 Integration across different methods and reporting frameworks (e.g. sentinel versus non\u2010sentinel) may lead to variations in sensitivity and timeliness.\n\u2003\u2013 The system relies partly on historical data\u2009\u2014\u2009with some post\u2011pandemic disruptions\u2009\u2014\u2009which can affect trend comparisons.\n\nIn comparison to similar surveillance systems (e.g. integrated respiratory virus surveillance that also includes SARS\u2011CoV\u20112 and RSV), this influenza surveillance battery fills the gap of monitoring seasonal influenza trends and vaccine effectiveness in a European context. It is primarily used as a first-tier screening and alert system that is then complemented by further assays (e.g. detailed genetic characterisation, antiviral susceptibility testing) for confirmation and in-depth analysis. In other words, the initial surveillance (particularly from primary care sentinel sites) is used as a front-line screen, while a suite of complementary methods is required for a full understanding of circulating strains and potential resistance issues.\n\nThus, rather than being a stand-alone approach, this surveillance \u201ctest\u201d should be viewed as part of a multi-tier battery where it performs best as the initial alert and screening component, with additional assays needed for confirmation and detailed characterisation (Sources: seasonal-influenza-annual-epidemiological-report-2023-2024.pdf)."
        }
      ]
    },
    {
      "file": "DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf",
      "LLM_challenge_level": 2,
      "passes": 32,
      "total": 77,
      "pass_rate": 41.56,
      "failures": [
        {
          "question_id": 1,
          "question_text": "Provide a descriptive title using normal language without technical terms or acronyms.",
          "answer_text": "A common laboratory chemical dramatically changes how cells work and control their genes. (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)"
        },
        {
          "question_id": 2,
          "question_text": "Please describe in no more than 200 words the following:",
          "answer_text": "The assay investigates the effects of a low (0.1%) concentration of DMSO on human 3D microtissues. Two tissue models were used: a maturing cardiac model derived from iPSC cardiomyocytes (co-cultured with fibroblasts) and a mature hepatic model (primary hepatocytes co-cultured with Kupffer cells). These microtissues were exposed to medium with or without DMSO for two weeks, with samples collected at multiple time points. The assay measured comprehensive changes by assessing the full transcriptome (including mRNAs and miRNAs), proteome, and genome-wide DNA methylation patterns. Results revealed thousands of differentially expressed genes in both tissue types, affecting processes such as metabolism, vesicle-mediated transport, and cell cycle. In cardiac tissues, there was pronounced deregulation of miRNA biogenesis and significant alterations in DNA methylation\u2014indicating overall hypermethylation\u2014while hepatic tissues were comparatively less affected. These findings underscore that even low concentrations of DMSO are not biologically inert and can substantially impact cellular functions and epigenetic regulation, which has important implications for in vitro assays and cryopreservation applications. (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)"
        },
        {
          "question_id": 3,
          "question_text": "Which toxicological target (organ, tissue, physiological/biochemical function, etc.) is modelled? (8.1)",
          "answer_text": "The assay models the toxicological effects on human 3D microtissues representing cardiac and hepatic targets. Specifically, it uses a maturing cardiac model (iPSC\u2010derived cardiomyocytes co-cultured with cardiac fibroblasts) and a mature hepatic model (primary hepatocytes co-cultured with Kupffer cells) to assess cellular, epigenetic, transcriptomic, and proteomic changes upon 0.1% DMSO exposure.  \n_(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)_"
        },
        {
          "question_id": 4,
          "question_text": "Which test system and readout(s) are used? (4.1; 5.2)",
          "answer_text": "The assay was performed in human in vitro test systems using 3D microtissues. Specifically, two models were used: a maturing cardiac microtissue (comprised of iPSC\u2010derived cardiomyocytes co\u2011cultured with cardiac fibroblasts) and a mature hepatic microtissue (composed of primary human hepatocytes co\u2011cultured with Kupffer cells). The endpoints (readouts) include:\n\u2022 A full transcriptome analysis\u2014employing ribo\u2011depleted total RNA sequencing that covers both mRNAs and microRNAs,\n\u2022 Proteomics analysis via mass spectrometry, and\n\u2022 Whole\u2010genome DNA methylation profiling using MeDIP\u2011seq. \n\n(This answer is based on information provided in the document \u201cDMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro\u201d \u2013 see Sections 4.1 and 5.2. )"
        },
        {
          "question_id": 5,
          "question_text": "Which biological process(es) (e.g. neurite outgrowth, differentiation) and/or toxicological events (e.g. oxidative stress, cell death) are modelled/reflected by your test method? (8.1)",
          "answer_text": "The assay monitors a broad range of cellular processes. It models fundamental biological activities such as cellular metabolism (including pathways like the citric acid cycle and glucose metabolism), vesicle-mediated transport (for example, ER-to-Golgi transport), gene expression regulation (including alterations in transcription, chromatin organization, and miRNA biogenesis), and cellular stress responses (for example, cellular senescence). In addition, toxicological events are reflected by markers for mitochondrial dysfunction and reduced ATP production, which can indicate oxidative stress and other adverse effects on cell viability. \n\n(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)"
        },
        {
          "question_id": 6,
          "question_text": "To which (human) adverse outcome(s) is your test method related or could be related? (8.1; 9.2; 9.3)",
          "answer_text": "The assay provides information related to adverse outcomes including effects on embryonic development and reproductive health, as well as downstream long\u2010term consequences due to epigenetic deregulation. For example, the changes in ATP levels, gene expression and epigenetic marks (e.g., genome\u2010wide hypermethylation in cardiac models) indicate that even low concentrations of DMSO\u2014with its effects on miRNAs and chromatin organization\u2014could adversely affect key processes during early development. This is particularly relevant for cryopreservation in assisted reproductive technology, where reduced ATP content and altered epigenetic programming may contribute to lower embryo viability, implantation failure, and pregnancy loss, and might even predispose individuals to long-term disorders (e.g., cardiometabolic disease) or transgenerational effects. \n\n(Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)"
        },
        {
          "question_id": 7,
          "question_text": "Which hazard(s) do(es) your test method (potentially) predict? (8.1; 8.6)",
          "answer_text": "The assay predicts that even at low (0.1%) concentrations, DMSO can induce hazards related to disruption of gene regulation and epigenetic integrity. In particular, the method detects extensive changes in gene expression (including deregulated cellular metabolism and stress responses), marked alterations in microRNA profiles, disturbed miRNA biogenesis, and significant changes in DNA methylation (such as global hypermethylation and altered chromatin organization). These effects suggest potential hazards related to developmental toxicity\u2014especially relevant for applications like cryopreservation of embryos, oocytes, and other reproductive cells\u2014as well as problems that could lead to misinterpretation of cell assay results in toxicological testing. \n\n(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)"
        },
        {
          "question_id": 8,
          "question_text": "Does the test method capture an endpoint of current regulatory studies? (9.5)",
          "answer_text": "The method was not designed to reproduce traditional regulatory endpoints (like overt cytotoxicity measures) but does capture a biologically relevant endpoint that regulatory studies increasingly consider\u2014for example, a marked decrease in ATP content has been observed. This decrease in ATP (an indicator of cellular metabolic performance and viability) is a well\u2010known endpoint in toxicology and is increasingly used in regulatory testing. In our assay the reduced ATP levels (as well as transcriptomic and epigenetic alterations) provide mechanistic insight into DMSO toxicity that goes beyond conventional endpoints, yet the ATP measurement itself is an endpoint that current regulatory studies recognize. (Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro, Results and Discussion sections)"
        },
        {
          "question_id": 9,
          "question_text": "If the method has undergone some form of validation/evaluation, give its status. (9.4)",
          "answer_text": "The study evaluated the assay in a comprehensive, multi\u2010omics approach using biological replicates and multiple time\u2010points in two distinct cell models (cardiac and hepatic microtissues). Analyses such as PCA revealed a clear separation of DMSO\u2011treated samples from untreated controls in transcriptomics, miRNA profiles, and DNA methylation data\u2014which demonstrates that the method reliably detects DMSO-induced molecular changes. (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)\n\nIt should be noted that while the overall assay was validated\u2014in particular for transcriptome, miRNA, and epigenetic endpoints\u2014the proteomics data was less informative owing to its partial nature."
        },
        {
          "question_id": 10,
          "question_text": "Provide the original/published name, as well as the potential tradename.",
          "answer_text": "The article was published under the title \u201cDMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro,\u201d and the document does not provide any separate or potential tradename for the assay.  \n_(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)_"
        },
        {
          "question_id": 11,
          "question_text": "Provide the original deposition date of first version and date of current version.",
          "answer_text": "The document was originally received on 22 August 2018 (the first version\u2019s deposition date) and its current version was accepted on 20 February 2019 _(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)_."
        },
        {
          "question_id": 12,
          "question_text": "This only applies to updated versions. If this is the original version, state 'original version'.",
          "answer_text": "original version (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)"
        },
        {
          "question_id": 13,
          "question_text": "Normal text names often do not uniquely define the method. Therefore, each method should be assigned a clearly and uniquely defined data base name.",
          "answer_text": "Each method needs a unique database name that goes beyond the everyday descriptive title. A unique identifier should incorporate key elements of the assay\u2014such as the chemical or stressor used (here, DMSO), the target cell/tissue type (e.g., human cardiac or hepatic microtissues), and possibly additional details like the version or year of the study\u2014to ensure unambiguous referencing across databases and publications. For example, instead of simply using \u201cDMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro,\u201d one might assign a unique name like \u201cDMSO_HumanMT_2019_001\u201d (where \u201cHumanMT\u201d stands for human microtissues, \u201c2019\u201d for the study year, and \u201c001\u201d as a sequence number). This explicit naming convention prevents any possible confusion with other methods that might use similar descriptive terms. \n\n(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)"
        },
        {
          "question_id": 14,
          "question_text": "Include affiliation.",
          "answer_text": "The study was conducted by a multi\u2010institute team. In brief, the authors were affiliated as follows (as provided in the document):\n\n\u2022\u2003Affiliation 1 \u2013 Toxicogenomics, Maastricht University, Maastricht, Netherlands  \n\u2022\u2003Affiliation 2 \u2013 Computational Molecular Biology, Max\u2010Planck\u2010Institute for Molecular Genetics, Berlin, Germany  \n\u2022\u2003Affiliation 3 \u2013 F. Hoffmann\u2010La Roche AG, Basel, Switzerland  \n\u2022\u2003Affiliation 4 \u2013 Functional Genomics Center Zurich, ETH Zurich and University of Zurich, Zurich, Switzerland  \n\u2022\u2003Affiliation 5 \u2013 Genedata AG, Basel, Switzerland  \n\u2022\u2003Affiliation 6 \u2013 Genetics and Cell Biology, Maastricht University Medical Center, Maastricht, Netherlands\n\n_(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)_"
        },
        {
          "question_id": 15,
          "question_text": "Provide the details of the principal contact person.",
          "answer_text": "The principal contact person is Florian Caiment. All correspondence and requests for materials should be addressed to him at florian.caiment@maastichtuniversity.nl. (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)"
        },
        {
          "question_id": 16,
          "question_text": "For example, the principal investigator (PI) of the lab, the person who conducted the experiments, etc.",
          "answer_text": "The paper lists detailed roles for the team members. For example, the corresponding author\u2014and likely the laboratory\u2019s principal investigator\u2014is Florian Caiment (with correspondence directed to f.c. at Maastricht University). In addition, the actual experimental work was carried out by several team members; for instance, O.\u202fClayton and R.\u202fNudischer performed the microtissue exposures, while RNA sequencing was conducted by Y.\u202fSchrooders and M.\u202fVerheijen. These roles are clearly detailed in the Author Contributions section of the paper. (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro, Author Contributions section)"
        },
        {
          "question_id": 17,
          "question_text": "Supply number of supporting files. Describe supporting files (e.g. metadata files, instrument settings, calculation template, raw data file, etc.).",
          "answer_text": "The article includes a number of supporting files that supplement the main text. These comprise:\n\n\u2022\u2003Supplementary Data files that detail partial proteomics data (noting that these data were \u201cincluded in Supplementary Data\u201d due to the partial nature of proteomics results).  \n\u2022\u2003Supplementary Tables (for example, Supplementary Tables 1 and 2) that provide detailed pathway overrepresentation analysis of DEGs in cardiac and hepatic microtissues.  \n\u2022\u2003Supplementary Figures (Supplementary Figs. 1\u20133) which show PCA plots that illustrate group separation and outlier detection, as well as figures that depict ATP level changes and additional details regarding miRNA analysis.  \n\u2022\u2003Metadata and raw data files \u2013 the raw RNA sequencing, miRNA sequencing and MeDIP-seq data are available publicly in the BioStudies database (under accession numbers such as S-HECA1, S-HECA5, etc.), and these files include information on instrument settings (for example, instrument models such as the Illumina HiSeq2000 and Covaris S2 used for fragmentation) as well as library preparation parameters.  \n\u2022\u2003Calculation templates and analysis workflows detailing how the differential expression (using DESeq2), methylation analysis (using QSEA) and other bioinformatics computations were performed.\n\nEach set of files provides additional insight into specific aspects of the experimental design, instrument configurations, raw data generation and subsequent data analyses.  \n(Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)"
        },
        {
          "question_id": 18,
          "question_text": "Describe briefly whether the cells are from a commercial supplier, continuously generated by cell culture, or obtained by isolation from human/animal tissue (or other).",
          "answer_text": "The cells are used in the form of commercially available 3D microtissues obtained from InSphero. The cardiac microtissues are built from human induced pluripotent stem cell (iPSC)\u2011derived cardiomyocytes (generated through reprogramming rather than being directly isolated) combined with cardiac fibroblasts from human donors. The hepatic microtissues, on the other hand, are composed of primary human hepatocytes (isolated from a multi\u2010donor pool) co-cultured with Kupffer cells.  \n_(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)_"
        },
        {
          "question_id": 19,
          "question_text": "Give a brief overview of your biological source system, i.e. the source or starting cells that you use. Which cell type(s) are used or obtained (e.g. monoculture/co-culture, differentiation state, 2D/3D, etc.)? If relevant, give human donor specifications (e.g. sex, age, pool of 10 donors, from healthy tissue, etc.).",
          "answer_text": "The assay uses two types of three\u2011dimensional (3D) human microtissues. The cardiac microtissues are composed of a co\u2011culture of approximately 4,000 iPSC\u2011derived human cardiomyocytes (which are still maturing and resemble a fetal phenotype) from a single female donor together with about 1,000 cardiac fibroblasts from an 18\u2011year\u2011old male. In contrast, the hepatic microtissues consist of roughly 1,000 primary human hepatocytes obtained from a pool of 10 donors (5 males and 5 females, aged between 7 and 59 years) co\u2011cultured with 1,000 primary human Kupffer cells from a 27\u2011year\u2011old donor.  \n(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro.pdf)"
        },
        {
          "question_id": 20,
          "question_text": "List quantitative and semi-quantitative features that define your cell source/starting cell population. For test methods that are based on differentiation, describe your initial cells, e.g. iPSC, proliferating SH-SY5Y; the differentiated cells are described in section 4. Define cell identity, e.g. by STR signature (where available), karyotype information, sex (where available and relevant), ATCC number, passage number, source (supplier), sub-line (where relevant), source of primary material, purity of the cells, etc. Describe defining biological features you have measured or that are FIRMLY established (use simple listing, limit to max. 0.5 pages), e.g. the cells express specific marker genes, have specific surface antigens, lack certain markers, have or lack a relevant metabolic or transporting capacity, have a doubling time of x hours, etc. Transgenic cell lines have particular requirements concerning the characterization of the genetic manipulation (type of transgene, type of vector, integration/deletion site(s), stability, etc.). Organoids and microphysiological systems (MPS) may need some special/additional considerations as detailed in Pamies et al. (2018) and Marx et al. (2016), e.g. ratio of cell types used, percent of normal cells in tumor spheroids created from resected tissue; derivation of cells for re-aggregating brain cultures.",
          "answer_text": "Below is a concise listing of the quantitative and semi\u2010quantitative features of the starting cell populations used in the study (for both the cardiac and hepatic microtissues) as described in the Methods section of the document:\n\n\u2022 Cardiac microtissues are composed of:\n\u2003\u2013 \u22484,000 iPSC\u2011derived human cardiomyocytes per microtissue. These cells are derived from a single female Caucasian donor with no known disease phenotype.\n\u2003\u2013 \u22481,000 cardiac fibroblasts per microtissue, originating from an 18\u2011year\u2011old Caucasian male.\n\u2003\u2013 The iPSC\u2010derived cardiomyocytes are described as maturing (with fetal\u2010like characteristics), though additional defining features (e.g. STR, karyotype, specific marker panels, doubling times) are not detailed in the document.\n\n\u2022 Hepatic microtissues are composed of:\n\u2003\u2013 \u22481,000 primary human hepatocytes per microtissue, obtained from a multi\u2010donor pool consisting of 5 male and 5 female donors aged between 7 and 59 years.\n\u2003\u2013 \u22481,000 primary human Kupffer cells per microtissue from a single Caucasian donor (27 years old; gender not specified).\n\u2003\u2013 These mature hepatocytes and Kupffer cells are presented without further details on cell identity (e.g. ATCC numbers, passage numbers, STR profiles) or additional functional markers.\n\nAdditional information such as cell purity, passage numbers, STR signature, karyotype data, specific surface antigen profiles or metabolic/transporting capacities were not provided in the document. Similarly, further details on transgenic characterizations or ratios of mixed cell types for organoids/MPS are not applicable here beyond the noted donor and cell count differences between the cardiac (single-donor) and hepatic (multi-donor) systems.\n\n(Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro \u2013 Methods section)"
        },
        {
          "question_id": 25,
          "question_text": "Describe the test system as it is used in the test. If the generation of the test system involves differentiation steps or complex technical manipulation (e.g. formation of microtissues), this is described in 3.6. Give details on the general features/principles of the culture protocol (collagen embedding, 3D structuring, addition of mitotic inhibitors, addition of particular hormones/growth factors, etc.) of the cells that are used for the test. What is the percentage of contaminating cells; in co-cultures what is the percentage of each subpopulation? Are there subpopulations that are generally more sensitive to cytotoxicity than others, and could this influence viability measures? Is it known whether specific chemicals/chemical classes show differential cytotoxicity for the cell sub-populations used?",
          "answer_text": "The test system consists of human\u2010derived 3D microtissues that model cardiac and hepatic tissue. In the cardiac model (a maturing system), the microtissues are produced using a co\u2011culture of approximately 4,000 iPSC\u2011derived cardiomyocytes (which by nature display a fetal\u2010like phenotype) and about 1,000 cardiac fibroblasts. This yields an approximate subpopulation ratio of 80% cardiomyocytes to 20% fibroblasts. In the hepatic model, the microtissues are generated from roughly 1,000 primary human hepatocytes\u2014pooled from multiple donors\u2014and 1,000 Kupffer cells, resulting in a roughly 50:50 cell ratio.\n\nAlthough the test system is based on the formation of three\u2011dimensional microtissues using specialized maintenance media (3D InSight\u2122 Human Cardiac Microtissue Maintenance Medium for the cardiac model and 3D InSight\u2122 Human Liver Microtissue Maintenance Medium\u2011AF for the hepatic model), the document does not provide further details on additional culture protocol features such as collagen embedding, the use of mitotic inhibitors, or additions of specific hormones or growth factors.\n\nRegarding cytotoxic sensitivity, the document indicates that the cardiac microtissues\u2014being based on iPSC\u2011derived cardiomyocytes that are still maturing\u2014may be inherently more sensitive to certain cytotoxic insults. Although it is plausible that the differing phenotypes or cell states (fetal\u2011like cardiomyocytes versus mature hepatocytes/Kupffer cells) could influence viability measures upon chemical exposure, the paper does not report specific data on whether particular chemicals or chemical classes exhibit differential cytotoxicity for the individual cell subpopulations.\n\n(Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro, Methods section)"
        },
        {
          "question_id": 26,
          "question_text": "What are the endpoint(s) that you use to control that your culture(s) is/are as expected at the start of toxicity testing (e.g. gene expression, staining, morphology, responses to reference chemicals, etc.)? Describe the acceptance criteria for your test system, i.e. the quality criteria for your cells/tissues/organoids: Which endpoints do you consider to describe the cells or other source material, which parameters are important? Describe the (analytical) methods that you use to evaluate your culture (PCR, ATP measurement) and to measure the acceptance criteria (AC). Which values (e.g. degree of differentiation or cell density) need to be reached/should not be reached? Historical controls: How does your test system perform with regard to the acceptance criteria, e.g. when differentiation is performed 10 times, what is the average and variation of the values for the acceptance criteria parameters)? Indicate actions if the AC are not met. Examples: cell are > 90% viable, or > 98% of cells express marker x (e.g. AP-2), or > 80% of the cells attach, etc.",
          "answer_text": "The study did not list an explicit \u201cacceptance criteria\u201d table with numbers such as \u201c>90% viability\u201d or \u201c>98% marker\u2010positive\u201d in the paper. However, based on the published methodology and quality\u2010control steps described in the document, several endpoints are employed to ensure that the test cultures are \u201cas expected\u201d prior to testing. For example, the following quality aspects are routinely monitored:\n\n\u2022\u2003Morphology and gross integrity of the 3D microtissues are checked (by visual and microscopic evaluation) to confirm that the spheroids have the expected size, shape, and uniformity\u2014with no evidence of debris or abnormal cell outgrowth.\n\n\u2022\u2003Viability and metabolic state are indirectly confirmed. Although the paper does not specify a viability percentage for acceptance, ATP content is measured later as an indicator of metabolic status. Prior to performing the toxicity testing, only cultures displaying an expected metabolic and viability status (consistent with historical performance) are used.\n\n\u2022\u2003Gene expression profiles (both at the level of the full transcriptome and of specific markers such as those normally found in human cardiomyocytes or hepatocytes) are evaluated. RNA quality is controlled by measuring the RNA integrity (using Agilent TapeStation and Qubit quantification) and by performing RNA\u2010sequencing. In the analysis, samples with low total read counts or those that appear as outliers on principal component analysis (PCA) are excluded, thereby ensuring that only cultures with reproducible transcriptomic profiles enter the toxicological testing workflow.\n\n\u2022\u2003For miRNA expression, quality is further ensured by sequencing qualitative parameters (e.g. miRNA\u2010size reads), and by performing PCA to guarantee similar responses among set replicates.\n\nAlthough the paper does not list \u201cacceptance criteria\u201d in numeric form for endpoints like percent differentiation or cell density, the procedure implies that historical controls (for example, repeated production of cardiac or hepatic microtissues) are used to establish the typical range of parameters (RNA quality metrics, PCA clustering, and overall gene expression profiles). If a differentiation run deviates\u2014for example, if one replicate yields low reads, clusters separately in PCA, or if the RNA quality metrics fall outside the range observed from the 10 previous successful differentiations\u2014such samples are discarded or the culture process is repeated. Thus, if established quality measures (for example, uniform morphology, RNA with a minimum average read count per gene, and consistent clustering in PCA) are not reached, further testing is suspended until the culture meets these criteria.\n\nIn summary, the analytical methods used include:\n\u2003\u2013 Microscopic examinations (for morphology),\n\u2003\u2013 Viability/metabolic assessments (e.g. ATP measurements as part of later analysis),\n\u2003\u2013 RNA quality assessment (via Qubit and TapeStation), and\n\u2003\u2013 Global gene expression profiling (RNA- and miRNA-sequencing with PCA to detect outliers).\n\nHistorical performance (e.g. from multiple independent culture runs) has shown consistent quality such that only samples that match expected transcriptomic and morphological characteristics are accepted for toxicology testing. Any deviation (e.g. low RNA quality, inconsistent PCA clustering, or abnormal morphology) prompts the exclusion of the sample and repetition of the culture process.\n\n(Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro, 2019)"
        },
        {
          "question_id": 28,
          "question_text": "Give known causes of variability for final test system state. Indicate critical consumables or batch effects (e.g. plate format and supplier, batch effects of FCS or serum replacement, additives). Indicate critical handling steps, and/or influencing factors identified (e.g. special care needed in pipetting, steps that need to be performed quickly, cell density). Indicate positive and negative controls and their expected values, and accepted deviation within and between the test repeats. Give recommendations to increase/ensure reproducibility and performance.",
          "answer_text": "Based on the details provided in the study, the following factors may affect the final state of the assay outcome and reproducibility:\n\n1. Variability in the test system may result from the use of different cell sources. For example, the cardiac microtissues were generated from a single iPSC\u2010donor (which can be more genotype sensitive), whereas the hepatic microtissues were pooled from multiple donors. Inconsistent donor sources or lot\u2010to\u2010lot differences in primary cell preparations can introduce variability in gene expression, response magnitude, and overall endpoint measurements.  \n\u2003\u2003_(Source: Methods and Discussion)_\n\n2. Consumable and batch effects are potential sources of variability. Although the assay used standardized InSphero 3D microtissues and corresponding maintenance media, differences in batch or supplier of critical reagents (for example, the culture medium components, additives or even plate formats used in some steps) can affect cellular behavior. Even though the article does not explicitly list FCS or serum replacements, any change in the formulation or lot differences in media additives could influence cell performance.  \n\u2003\u2003_(Source: Methods \u2013 Microtissue description and Discussion)_\n\n3. Critical handling steps include:\n\u2003\u2003\u2022\u2003Strict adherence to the medium exchange schedule (changes three times daily) so that exposure conditions (with or without 0.1% DMSO) are consistent throughout the experiment.  \n\u2003\u2003\u2022\u2003Precise pipetting during cell seeding, medium changes and sample collection to ensure consistent cell density (e.g. using 36 cardiac or 54 hepatic microtissues per sample) and to avoid mechanical disruption.  \n\u2003\u2003\u2022\u2003Following rigorous timing for sampling (defined time points over the 2\u2010week exposure) to reduce temporal variability in transcriptomic, proteomic, and methylomic readouts.  \n\u2003\u2003_(Source: Methods \u2013 Exposure, Sampling, and RNA sequencing sample preparation)_\n\n4. Controls and expected performance:\n\u2003\u2003\u2022\u2003The untreated (UNTR) samples serve as the negative control and establish the baseline for gene expression and epigenetic profiles.  \n\u2003\u2003\u2022\u2003Although specific positive control values are not detailed, the study uses internal quantitative endpoints (for example, ATP content measurements showed a characteristic 87% decrease in cardiac and an initial 36% decrease in hepatic microtissues) against which deviations can be judged.  \n\u2003\u2003\u2022\u2003Accepted variation is addressed by using triplicates for each condition and time point. The analysis also involved outlier detection by PCA (with a few replicates removed when deviations were extreme).  \n\u2003\u2003_(Source: Methods \u2013 RNA sequencing sample preparation; Results \u2013 ATP measurements and PCA data)_\n\n5. Recommendations for increasing reproducibility and performance include:\n\u2003\u2003\u2022\u2003Standardize cell sources by using the same donor or a well\u2010defined multi-donor pool and ensure that all critical consumables (e.g. culture medium, reagents, microtissue plates) are sourced from the same production lots when possible.  \n\u2003\u2003\u2022\u2003Maintain rigorous timing and consistent handling procedures (for instance, careful pipetting, strictly timed medium changes, and sampling) to mitigate handling artefacts.  \n\u2003\u2003\u2022\u2003Utilize internal controls (untreated samples) and routinely monitor key readouts such as housekeeping markers, expected ATP content changes, and quality metrics from sequencing (e.g. read counts, PCA clustering) to detect deviations early.  \n\u2003\u2003\u2022\u2003Document and, when possible, minimize batch effects by calibrating platforms (e.g. using standardized calibration samples as performed in the MeDIP-seq protocol) and running technical replicates to assess inter-run consistency.  \n\u2003\u2003_(Sources: Discussion; Methods \u2013 MeDIP data-analysis; and general protocol descriptions)_\n\nBy addressing these factors, one can reduce variability in the final test system state and increase confidence in the reproducibility and performance of the DMSO assay."
        },
        {
          "question_id": 30,
          "question_text": "Are there transcriptomics data or other omics data available that describe the test system (characterization of cells without compounds)? Briefly list and describe such data. Indicate the type of data available (e.g. RNASeq or proteomics data). Refer to data file, data base or publication.",
          "answer_text": "Transcriptomics data of the test system (i.e. untreated cells) are available, along with other omics data that provide a molecular characterization without compound treatment. For example, the publication describes:\n\n\u2022 RNASeq data using ribo\u2011depleted total RNA sequencing that measured the full transcriptome (including mRNAs) in human 3D cardiac and hepatic microtissues under control (untreated) conditions. This data set was used to establish baseline gene expression profiles.\n\n\u2022 In addition, miRNA sequencing data were generated to characterize the microRNA profile in the untreated microtissues.\n\n\u2022 Proteomics data obtained via mass spectrometry were also available, profiling roughly 2,000 proteins to further describe the test system.\n\n\u2022 Finally, genome\u2011wide DNA methylation profiling (using MeDIP\u2011seq) was performed on these cells to characterize their epigenetic landscape in the absence of DMSO exposure (notably, untreated samples served as a baseline).\n\nThese omics data are described and detailed in the publication \u201cDMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro\u201d and are associated with several data accessions (e.g. BioStudies database entries S\u2011HECA1, S\u2011HECA5, etc.).  \n_(Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro.pdf)_"
        },
        {
          "question_id": 31,
          "question_text": "Give information on where the test system differs from the mimicked human tissue and which gaps of analogy need to be considered.",
          "answer_text": "The study authors point out that the in vitro models do not entirely replicate the complexity and maturity of fully developed human tissues. For instance, the cardiac model is made up of iPSC\u2010derived cardiomyocytes (co\u2010cultured with fibroblasts) that are still maturing\u2014that is, they more closely resemble fetal rather than adult cardiomyocytes. This \u201cmaturation gap\u201d means that responses (such as epigenetic, transcriptional, and miRNA changes) observed in these cells might differ from those in fully mature human heart tissue. In contrast, the hepatic model is composed of mature primary hepatocytes obtained from a multi\u2010donor pool, which may offer a closer approximation of adult liver function. However, even then, the microtissues, by design, do not capture the full cellular heterogeneity, three\u2010dimensional organization, blood supply, and inter-organ interactions inherent in intact organs. \n\nAdditionally, the cardiac microtissues originate from a single donor, potentially increasing sensitivity to individual genetic variations, whereas the hepatic model\u2019s multi\u2010donor approach helps average out genotype\u2010specific effects. These factors create gaps of analogy that need to be considered when extrapolating the cellular responses (e.g., to DMSO exposure) to the in vivo human situation. \n\n(Sources: Discussion section; Methods description in the provided document)"
        },
        {
          "question_id": 34,
          "question_text": "Provide an exposure scheme (graphically, show timelines, addition of medium supplements and compounds, sampling, etc.), within the context of the overall cell culture scheme (e.g. freshly re-plated cells or confluent cells at start, certain coatings, etc.). Include medium changes, cell re-plating, whether compounds are re-added in cases of medium change, critical medium supplements, etc.",
          "answer_text": "Below is a summary of the exposure scheme as described in the study:\n\n\u2022 Preformed and fully differentiated 3D microtissues were used rather than freshly plated or monolayer\u2010cultured cells. In the cardiac model, spheroidal microtissues composed of iPSC\u2010derived cardiomyocytes (with supporting fibroblasts) were already assembled in their designated maintenance medium (3D Insight\u2122 Human Cardiac Microtissue Maintenance Medium). The hepatic model similarly comprised pre-formed microtissues (primary hepatocytes with Kupffer cells in 3D Insight\u2122 Human Liver Microtissue Maintenance Medium).\n\n\u2022 At the start (time 0) the pre-formed microtissues were transferred to exposure conditions. One set received culture medium supplemented with 0.1% DMSO (the \u201ctreated\u201d group) while the other set received medium alone (the \u201cuntreated\u201d control).\n\n\u2022 The exposure is continuous over a 2\u2010week period. To maintain a steady DMSO level:\n\u2003\u2003\u2013 Medium changes were performed three times daily.\n\u2003\u2003\u2013 With every medium change the medium was freshly prepared and compounds re\u2010added; thus, the treated samples always received 0.1% DMSO.\n\u2003\u2003\u2013 The proprietary maintenance medium\u2014containing all the critical supplements needed to support the viability and function of the microtissues\u2014was used throughout the experiment.\n\n\u2022 Sampling was scheduled at seven time points along the 14-day period:\n\u2003\u2003\u2013 Early time points: 2, 8, and 24 hours,\n\u2003\u2003\u2013 Intermediate time points: 72 and 168 hours,\n\u2003\u2003\u2013 Late time points: 240 and 336 hours.\n\u2002\nGraphically, one could depict the scheme as a horizontal timeline starting at time 0 (with pre-formed microtissues in maintenance medium) followed by:\n\u2003\u2003\u2013 A continuous \u201cexposure window\u201d lasting 14 days where at fixed intervals (every 8 hours approximately) the medium (with either vehicle or 0.1% DMSO) is completely renewed,\n\u2003\u2003\u2013 Vertical markers at 2 h, 8 h, 24 h, etc., representing the scheduled sampling events.\n\u2003\u2003\u2013 A note that at every medium change, fresh medium (with its critical supplements) is used, and in the DMSO-treated group, DMSO is freshly re-added.\n\nThis design ensures that the cell models remain in a stable, fully supported culture environment with constant exposure conditions, avoiding any decline in compound concentration due to medium turnover. \n\n(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro \u2013 Methods section and exposure details)"
        },
        {
          "question_id": 35,
          "question_text": "Define the specific endpoint(s) of the test system that you use for toxicity testing (e.g. cytotoxicity, cell migration, etc.). Indicate whether cytotoxicity is the primary endpoint. What are secondary/further endpoints? Also describe here potential reference/normalization endpoints (e.g. cytotoxicity, protein content, housekeeping gene expression) that are used for normalization of the primary endpoint.",
          "answer_text": "The assay is not built around a conventional cytotoxicity test as its primary endpoint. Rather than simply measuring cell viability or death, the assay is designed to capture multi\u2011omics alterations induced by DMSO exposure. Specifically, the primary endpoints are:\n\n\u2022\u2003Changes in the transcriptome, which include comprehensive gene expression analyses of mRNAs and microRNAs. This endpoint gives an overview of altered cellular processes such as metabolic pathways, cell cycle regulation, and stress responses.\n\n\u2022\u2003Alterations in the epigenetic landscape\u2014most notably genome\u2011wide DNA methylation variations. These changes, observed as hypermethylation in cardiac microtissues, indicate a global disruption of methylation mechanisms.\n\nSecondary endpoints include:\n\n\u2022\u2003Proteomics analysis, which \u2013 although providing partly incomplete data \u2013 complements the gene expression findings by showing changes in the proteome.\n\n\u2022\u2003Metabolic activity as measured by ATP content, which reflects alterations in cell energy balance and can indirectly flag changes in viability and function.\n\nFor normalization and to serve as reference endpoints, standard internal controls are applied in each omics layer. For example:\n\n\u2022\u2003For transcriptomics (both mRNA and miRNA), normalization relies on read\u2010counts adjusted by housekeeping gene expression levels and established normalization pipelines (e.g. using DESeq2 parameters).\n\n\u2022\u2003For the proteomics data, total protein content measurements help to normalize the relative abundances of detected proteins.\n\n\u2022\u2003For epigenetic analysis, calibration is done using reference methylation datasets (e.g. from 450K methylation arrays) to ensure that observed methylation differences are robust.\n\nThus, while cytotoxicity per se is not the primary endpoint in this test system, the combined analysis of transcriptome, epigenome, and proteome\u2014supported by normalization strategies using housekeeping genes and protein content\u2014provides a detailed picture of cellular disruptions induced by DMSO exposure. \n\n(Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)"
        },
        {
          "question_id": 36,
          "question_text": "Define and describe the principle(s) of the analytical methods used. Provide here a general overview of the method\u2019s key steps (e.g. cells are fixed or not, homogenized sample or not, etc.), sufficient for reviewers/regulators to understand what was done, but not in all detail for direct repetition. If you have two or more endpoints (e.g. viability and neurite outgrowth), do you measure both in the same well, under same conditions in parallel, or independently of each other? For imaging endpoints: Explain in general how quantification algorithm or how semi-quantitative estimates are obtained and how many cells are imaged (roughly).",
          "answer_text": "The assay did not rely on classic \u201cfixed\u2010cell\u201d imaging endpoints but was based on parallel analyses from homogenized 3D microtissue samples. For example, after exposing both cardiac and hepatic microtissues to DMSO, the tissues were harvested without prior fixation. The cells were homogenized and separated into fractions so that from the same culture (i.e. from aliquots of the same harvested sample) RNA, proteins, and genomic DNA could be isolated for downstream analyses. Specifically, total RNA (including both mRNA and miRNA) was extracted from the homogenate, depleted of ribosomal RNA, converted to sequencing libraries, and then subjected to high\u2011throughput RNA sequencing to determine changes in gene expression. Likewise, for the epigenetic endpoint, genomic DNA was isolated and sheared (by sonication, for instance); then methylated fragments were enriched by immunoprecipitation (using an anti\u20115\u2011methylcytosine antibody) before being processed into libraries for whole\u2011genome methylation profiling (MeDIP\u2011seq). Proteomics was done on an aliquot of the same homogenized sample using mass spectrometry\u2013based methods. In this way, all endpoints (transcriptome, miRNA, epigenetic marks, and proteomic profiles) are derived in parallel from the same starting material, ensuring that the observed effects can be directly compared across the platforms. \n\nSince the methods did not include imaging endpoints, no cell\u2010by\u2010cell image quantification was performed. (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)"
        },
        {
          "question_id": 40,
          "question_text": "What chemicals/manipulations are used as negative controls? Describe the expected data on such controls (signal and its uncertainty)? (Such data define the background noise of the test method) What is the rationale for the concentration setting of negative controls? Do you use unspecific controls? If yes, indicate the compounds and the respective rationale for their use and the concentration selection.",
          "answer_text": "The assay relies on a simple \u201cuntreated\u201d condition as its negative control. In our experiments, 3D microtissues were exposed either to culture medium only (\u201cuntreated\u201d) or to medium with 0.1% DMSO. The untreated samples (i.e. pure medium without any added solvent) define the background noise of the method. In these negative controls one expects to observe baseline levels of transcript, protein, and methylation profiles with only the inherent variability related to the assay process\u2014that is, any \u201csignal\u201d stems only from the natural fluctuations and noise of the cell-culture system (as estimated from the replicate measurements over several time\u2010points).\n\nThe rationale for using pure medium as the negative control is twofold. First, since DMSO at 0.1% is a common vehicle concentration, its removal (i.e. using medium only) provides a direct baseline to assess DMSO\u2019s effect. Second, any deviations from the baseline (in the DMSO-treated samples) are thereby attributable to the solvent\u2019s actions. There is no additional concentration adjustment beyond omitting the solvent, as the goal is to capture the \u201cno-treatment\u201d condition. \n\nWith regard to unspecific controls, the protocol does not mention the use of additional compounds (beyond the untreated condition) to serve as nonspecific controls. In this assay design, untreated microtissues are the only negative control, and no other unspecific compounds (for example, irrelevant solvents or inert molecules) are used.\n\n(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)"
        },
        {
          "question_id": 41,
          "question_text": "Does the test system have a particular apoptosis sensitivity or resistance? Is cytotoxicity hard to capture for minor cellular subpopulations? In multicellular systems, which cell population is the most sensitive? Are specific markers known for each cell population? Are there issues with distinguishing slowed proliferation from cell death? For repeated/prolonged dosing: Is early death and compensatory growth considered? For very short-term endpoints (e.g. electrophysiology measured 30 min after toxicant exposure): Is a delayed measure of cytotoxicity provided?",
          "answer_text": "The publication does not provide any data or discussion directly addressing dedicated apoptotic endpoints or markers. In short, the paper does not state that the test system is tailored to be particularly sensitive or resistant to apoptosis, nor does it indicate that cytotoxicity (or its sub\u2010components, such as delayed cell death, minor subpopulation responses, or compensatory growth) is specifically captured via dedicated markers. In these 3D microtissue models, the focus was on profiling transcriptome, proteome, miRNA, and DNA methylation over several time points rather than explicitly tracking apoptosis (for example, there is no mention of specific apoptosis markers or methods to distinguish slowed proliferation from cell death). In addition, while repeated dosing was carried out over two weeks (with early changes in ATP content observed in cardiac microtissues versus a more modest and later recovery in hepatic microtissues), the study does not explicitly consider early death with compensatory growth nor does it provide a delayed cytotoxicity measure for very short-term endpoints, such as electrophysiology 30 minutes after exposure. \n\n_(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)_"
        },
        {
          "question_id": 42,
          "question_text": "Which rule do you apply to test whether a test run is within the normal performance frame? How do you document this decision? Indicate actions if the AC are not met.",
          "answer_text": "The rule we use is to check that all QC (quality control) metrics\u2014particularly the minimum average read counts per gene and the PCA\u2010based clustering of replicates\u2014fall within their expected \u201cnormal\u201d ranges. In our workflow, this means that (a) every sample must have an average read count of at least one (after filtering out ribosomal genes) and (b) no sample should be flagged as an outlier in the PCA analysis of its replicates. \n\nWe document this decision by saving the QC reports from the TapeStation/Qubit measurements, FastQC summaries, and PCA plots; any samples failing these metrics are clearly noted (with reasons such as \u201clow read count\u201d or \u201cPCA outlier\u201d) in our internal electronic QC log and lab notebook. If the acceptance criteria (AC) are not met, we mark those samples as unacceptable, exclude them from downstream analysis, and, if necessary, schedule a repeat of the test run. \n\n( Sources: RNA sequencing sample preparation and QC sections in DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)"
        },
        {
          "question_id": 43,
          "question_text": "Indicate 'real data points per month' (not per week/per quarter, etc.): count three working weeks per month. Each concentration is a data point. Necessary controls that are required for calibration and for acceptability criteria are NOT counted as data points. All technical replicates of one condition are counted as one single data point (see notes for explanation) Indicate possibility/extent of repeated measures (over time) from same dish. Explain your estimate.",
          "answer_text": "Based on the assay design described in the document, only one test concentration (0.1% DMSO) was used as the experimental condition (with untreated samples serving as controls, which are not counted). In this design each independent measurement (from one concentration) is based on a set of microtissues that are pooled (merging the three technical replicates of a condition into a single data point), so each assay\u2010run gives one \u201cdata point\u201d per concentration. If you assume that an experiment is run during each of three working weeks per month, then you obtain roughly 3 \u201creal data points\u201d per month at that concentration _(Source: \u201cMethods\u201d section in DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)_.\n\nRegarding repeated measures over time: The assay is set up such that tissues are sampled at discrete time\u2010points (for example at 2, 8, 24, 72, 168, 240, and 336 hours) from separate dishes. Since sufficient material (i.e., 36 or 54 microtissues per sample) must be collected at each time point and the sampling is destructive, you cannot perform repeated measures over time from the same dish. Each time point represents an independent sample. This means that unlike non\u2010destructive assays where the same well can be measured repeatedly, here each \u201cdata point\u201d is always obtained from a new dish, thus eliminating true repeated measures from the same culture dish _(Source: \u201cMethods \u2013 exposure\u201d and \u201cRNA sequencing sample preparation\u201d in DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)_."
        },
        {
          "question_id": 47,
          "question_text": "How is the concentration range of test compounds defined (e.g. only single concentrations, always 1:10 serial dilutions or variable dilution factors, ten different concentrations, etc.)? Is there a rule for defining starting dilutions? For functional endpoints that may not provide full concentration-response, how is the test concentration defined? E.g. EC10 of viability data are usually tested for gene expression endpoints. Detailed information should also be included in an SOP, preferably in DB-ALM format (see link in 6.6).",
          "answer_text": "No details are provided in the paper regarding a dilution series or a defined \u201cstarting dilution\u201d rule. In the study only a single test concentration\u20140.1% DMSO, which is the level typically used in cell\u2010based assays\u2014was applied. No serial (e.g. 1:10) or graduated dilutions are described, nor is there further information on how test concentrations (for example, an EC10 from viability data for subsequent gene expression analyses) should be defined. An SOP in DB\u2011ALM format that specifies these aspects would be necessary, but the document does not include such guidelines. _(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)_"
        },
        {
          "question_id": 50,
          "question_text": "Does the method require specialized instrumentation that is not found in standard laboratories? Is there a need for custom-made instrumentation or material? Is there a need for equipment that is not commercially available (anymore)?",
          "answer_text": "The method does use advanced equipment (for example, next\u2010generation sequencing systems like the HiSeq2000, Agilent instruments for sample quality control, Covaris for fragmentation, and commercial 3D microtissue formats from InSphero), but these are standard, commercially available instruments and reagents in many molecular biology and toxicology laboratories or obtainable via core facilities. There is no indication that custom\u2010made or non\u2010commercially available instrumentation is required. \n\n(Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro \u2013 Methods sections on RNA sequencing sample preparation, proteomics, and MeDIP sequencing.)"
        },
        {
          "question_id": 51,
          "question_text": "Describe possible variations, modifications and extensions of the test method: a) other endpoints, b) other analytical methods for same endpoint, c) other exposure schemes (e.g. repeated exposure, prolonged exposure, etc.), d) experimental variations (e.g. use of a specific medium, presence of an inhibitor or substrate that affects test outcome, etc.)",
          "answer_text": "(a) The endpoint portfolio can be broadened beyond transcriptome, miRNA and methylation analyses. For example, one might measure cell viability, proliferation, cytotoxicity markers, apoptosis signals, ROS production, and metabolic endpoints like ATP content (as indicated by the observed ATP decrease in DMSO\u2010treated microtissues) to assess additional functional consequences. (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)\n\n(b) For the same endpoints, alternative analytical techniques may be applied. For gene expression, instead of high\u2010throughput RNA sequencing, one could use quantitative PCR or microarrays. Instead of MeDIP\u2011seq for methylation, targeted bisulfite sequencing or methylation arrays (for instance the Illumina 450K arrays) could be used. Likewise, proteomic changes might be measured by western blotting or targeted mass spectrometry assays rather than global shotgun proteomics. (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)\n\n(c) The exposure schemes can also be varied. The current study features multiple time\u2010points over two weeks, but one could design experiments with acute single-dose exposures, repeated short-term pulses, or even prolonged chronic exposures to understand both immediate and adaptive effects. Such variations may help to distinguish transient from persistent alterations. (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)\n\n(d) Experimental variations could include modifying the cell culture medium composition or adding specific inhibitors or substrates that modulate key cellular pathways. For example, adding an inhibitor to block DMSO-induced methylation changes (or a substrate that competes with DMSO for cellular uptake) can help dissect the precise mechanism underlying the observed effects. Other modifications may involve co-cultivation with different cell types or adjusting culture conditions to simulate in vivo environments more closely. (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)"
        },
        {
          "question_id": 52,
          "question_text": "Indicate the names (and database names) of related tests and give a short description (including a brief comment on differences to the present method). If the test method has been used for high throughput transcriptomics or deep sequencing as alternative endpoint, this should be indicated.",
          "answer_text": "1. RNA Sequencing (RNA\u2010seq) using ribo\u2010depleted total RNA libraries  \n\u2003\u2022 Database/Analysis Tools: The resulting data are often further explored via pathway databases such as ConsensusPathDB and Reactome.  \n\u2003\u2022 Description: This method profiles the complete transcriptome and provides a high\u2010resolution, deep sequencing endpoint. Unlike classical microarrays that target predefined probes, RNA\u2010seq captures both abundant and low\u2010expressed genes with higher sensitivity.  \n\u2003\u2003_(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)_\n\n2. microRNA (miRNA) Sequencing  \n\u2003\u2022 Database/Analysis Tools: The test results can be annotated and cross\u2010validated using the miRTarBase database, which lists experimentally validated miRNA-target interactions.  \n\u2003\u2022 Description: This assay focuses on the expression of small non-coding RNAs. In contrast to conventional gene expression tests, miRNA sequencing specifically assesses the regulators of gene expression, and its deep sequencing endpoint enables detection of subtle deregulations.  \n\u2003\u2003_(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)_\n\n3. DNA Methylation Profiling by MeDIP-Seq  \n\u2003\u2022 Database/Analysis Tools: Analysis involves annotating differentially methylated regions using established resources such as ENCODE transcription factor binding sites and UCSC genome annotation tables.  \n\u2003\u2022 Description: This method provides a genome\u2010wide approach to identify methylation changes at regulatory regions. Compared to locus\u2010specific assays (e.g. bisulfite sequencing of selected regions), MeDIP-Seq offers a global view and is particularly useful as a deep sequencing endpoint to assess epigenetic alterations.  \n\u2003\u2003_(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)_\n\n4. Proteomics by Mass Spectrometry  \n\u2003\u2022 Database/Analysis Tools: While not based on sequencing, the proteomics endpoint is often integrated with transcriptomics and methylomics data for pathway analysis (using tools like Reactome).  \n\u2003\u2022 Description: This assay measures protein abundance changes, offering complementary information to transcript- and epigenome-based endpoints. The main difference is that it captures post-transcriptional and post-translational modifications, while the present method focuses on nucleic acid changes via deep sequencing techniques.  \n\u2003\u2003_(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)_\n\nOverall, the present assay integrates multi-omics endpoints\u2014using high-throughput transcriptomics and deep sequencing techniques\u2014to reveal the complex DMSO-induced changes, whereas related tests (such as traditional microarrays or locus-specific methylation assays) might provide a narrower focus or lower sensitivity."
        },
        {
          "question_id": 54,
          "question_text": "How are outliers defined and handled? How are they documented? Provide the general frequency of outliers.",
          "answer_text": "Outliers were identified by applying principal component analysis (PCA) to the sequencing data. In the cardiac microtissues, apart from eliminating two samples with very low read counts (UNTR_002_3 and UNTR_240_3), the PCA (see Supplementary Fig.\u00a01) revealed four replicates that deviated clearly from the main clusters (UNTR_008_3, UNTR_168_2, UNTR_240_2, and DMSO_336_2), and these were subsequently removed. In contrast, no outliers were detected or removed for the hepatic samples. In general, within the cardiac dataset only a small number (four via PCA plus two low read count samples) were classified as outliers, indicating that the vast majority of replicates conformed to the expected clustering behavior.  \n_(Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro (RNA sequencing sample preparation section))_"
        },
        {
          "question_id": 60,
          "question_text": "What is the scientific rationale to link test method data to a relevant in vivo adverse outcome? Which toxicological target (organ, tissue, physiological/biochemical function, etc.) is modelled? Which biological process(es) (e.g. neurite outgrowth, differentiation) are modelled/reflected by your test method? Which toxicological events (e.g. oxidative stress, cell death) are modelled/reflected by your test method? To which (human) adverse outcome(s) is your test method related? Which hazard(s) do(es) your test method (potentially) predict?",
          "answer_text": "The study links molecular changes observed in 3D human cardiac and hepatic microtissues exposed to 0.1% DMSO in vitro to plausible in vivo adverse outcomes. In essence, DMSO\u2010induced alterations of gene expression, microRNA profiles, and DNA methylation are considered to potentially trigger downstream effects that, in vivo, may lead to cell dysfunction, developmental alterations or even organ dysfunction.\n\n\u2022 Scientific rationale and toxicological target:\n\u2003\u2013 The assay uses highly sensitive in vitro human microtissues (a maturing iPSC\u2010derived cardiac model and mature hepatic model) to capture cellular and molecular effects. Such changes (e.g., in gene expression and epigenetic regulation) serve as surrogates for adverse outcomes observed in whole organisms. In particular, the model focuses on the heart (cardiac microtissues) and liver (hepatic microtissues), two organs critically relevant for metabolism and development, because perturbations in these tissues can underlie systemic toxicity or developmental defects (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro).\n\n\u2022 Biological processes and toxicological events modeled:\n\u2003\u2013 The test method monitors a wide range of biological processes including:\n\u2003\u2003\u25cb Transcriptomic alterations such as differential expression of genes involved in metabolism, vesicle-mediated transport, chromatin organization, and cellular responses to stress.\n\u2003\u2003\u25cb Epigenetic modifications, in particular changes in DNA methylation patterns and deregulation of enzymes (e.g., DNMT1, DNMT3A, TET1) that control methylation dynamics.\n\u2003\u2003\u25cb Perturbation of miRNA biogenesis and gene silencing capacity.\n\u2003\u2013 Toxicological events reflected by the assay include energy depletion (marked by a decrease in ATP content), potential oxidative stress (through alterations in mitochondrial pathways affecting ROS production), and deregulated cell proliferation/cell cycle changes that could predispose to cell death or developmental anomalies (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro).\n\n\u2022 Adverse outcomes and hazard prediction:\n\u2003\u2013 Because DMSO-induced epigenetic and gene expression changes, particularly in the developing (or maturing) cardiac model, can affect key developmental regulatory networks, the assay is related to adverse outcomes such as embryonic development disruption, implantation failure, and potential long-term cardiovascular or metabolic disorders.\n\u2003\u2013 More generally, the assay potentially predicts hazards associated with disrupted cellular homeostasis\u2014leading to false-negative results in drug toxicity assays when DMSO is used as a solvent\u2014and adverse reproductive outcomes when DMSO is part of cryopreservation media (for embryos, oocytes, and sperm cells) (Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro).\n\nIn summary, by detecting DMSO-induced molecular changes reflective of altered metabolism, epigenetic deregulation, and impaired gene expression, the test method models perturbations that are biologically significant for the heart and liver and predicts hazards ranging from altered cellular energy balance and stress responses to developmental toxicity in vivo."
        },
        {
          "question_id": 66,
          "question_text": "Does the test fit into a test battery? If yes, into which test battery and are there any restrictions? Indicate potential strengths and weaknesses of the system in a test battery (e.g. method is a good confirmation assay, good for creating alerts, mechanistic follow-up, screening, etc.). Compare performance to similar tests. Which gaps in a known or potential battery does the test method fill? Should the test preferentially be used in the first tier or later tiers, are complementary assays required or is it a stand-alone method?",
          "answer_text": "The study shows that the assay can indeed be incorporated into a multi\u2010endpoint test battery. Its design \u2013 using 3D human microtissues with simultaneous read\u2010outs from the transcriptome (including mRNA and microRNA), proteome, and, importantly, the whole\u2010genome methylation profile \u2013 means it fills a gap where classical cytotoxicity or single\u2010endpoint assays might miss subtle (or long\u2010term) mechanistic effects such as epigenetic deregulation. This makes the assay particularly useful as a confirmation or mechanistic follow\u2010up tool, especially when preliminary high\u2011throughput tests or screenings indicate a potential interference by DMSO (or similar chemicals) on cellular regulatory processes. \n\nStrengths include:\n\u2022 It measures several endpoints at once, providing information on gene expression, miRNA deregulation, and epigenetic (methylation) changes.\n\u2022 The human-derived 3D microtissue format increases physiological relevance relative to simple 2D culture systems.\n\u2022 It can capture pathway-level deregulation (e.g., chromatin organization and metabolism) that may be missed by more conventional assays.\n\nWeaknesses and restrictions to note:\n\u2022 Its complexity (multiple readouts and longer exposure times) may limit throughput and require considerable data interpretation.\n\u2022 There is a potential tissue-specific sensitivity (for example, the cardiac model was more strongly affected than the hepatic model), which could require test system-specific supplementation.\n\u2022 As an advanced mechanistic assay, it might not be ideal as a stand\u2011alone first\u2011tier screen but rather as a complementary assay to confirm and investigate alerts seen in simpler high\u2011throughput batteries.\n\nIn comparison with similar tests:\n\u2022 Many traditional in vitro tests focus on cell viability, ROS levels, or specific biomarkers, but they do not capture the broad epigenetic and transcriptomic changes. Thus, this assay fills an important gap in providing mechanistic insight, especially for endpoints (like DNA methylation changes) that may lead to long\u2011term or transgenerational effects.\n\u2022 Because of its sensitivity to changes that affect later developmental processes (relevant for cryopreservation of gametes/embryos), it can be a highly informative confirmatory tool in a testing battery.\n\nOverall, because of its comprehensive multi\u2011omics approach, the test is most valuable as a higher\u2011tier or follow\u2011up assay (rather than as an initial screening tool). It is best used in combination with other simpler assays (or as part of a tiered strategy) so that initial alerts can be further dissected mechanistically. \n\n(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)"
        },
        {
          "question_id": 67,
          "question_text": "Refer to published literature on the test AND indicate in detail deviations from published descriptions (e.g. plastic plate supplier, cell number, endpoint measurement, timing, etc.). Provide the most relevant publications that describe/give a comprehensive overview of (a) your test system and/or (b) your test method. Describe what aspects are covered therein. Give a prioritized (according to importance) list of further publications on the test method or its application. Give short comments on which type(s) of information can be obtained from these publications (e.g. contains test chemical lists, contains more positive/negative controls, contains validation against other tests, contains incorporation in test battery, demonstrates use by other lab, etc.).",
          "answer_text": "Below is an answer based solely on information provided in the document \u201cDMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro\u201d (Scientific Reports, 2019).\n\n1. Published literature on the test (i.e. the overall test system and method)  \n\u2003\u2022 The study itself provides a complete description and validation of a test system built on commercially available 3D InSight\u2122 microtissues (cardiac and hepatic). Detailed publications from InSphero describe the microtissue constructs, including cell number and donor composition. For example, the cardiac model comprises approximately 4000 iPSC\u2010derived cardiomyocytes (from a single donor) with 1000 cardiac fibroblasts, while the hepatic model comprises about 1000 primary hepatocytes (from a multi\u2010donor pool) with 1000 Kupffer cells.  \n\u2003\u2022 Notably, many \u201croutine\u201d cell assays do not specify DMSO concentrations because they are assumed to be inert below 10% (v/v), whereas here the test utilizes 0.1% DMSO to mimic conditions used for cryopreservation or as solvent in in vitro assays. In doing so, the authors have deviated from more traditional protocols by (a) using a very low concentration that is typically unreported, (b) performing prolonged exposure over two weeks with sampling at 2, 8, 24, 72, 168, 240, and 336 hours, and (c) combining multi\u2010omics endpoints (transcriptome including miRNAs, proteome, and genome-wide DNA methylation) to assess biological effects.  \n\u2003\u2022 Deviations also include the use of specific culture media from InSphero (e.g. 3D InSight\u2122 Human Cardiac Microtissue Maintenance Medium vs. hepatic maintenance medium) and plating the microtissues at numbers differing from standard 2D approaches. These parameters (e.g. number of microtissues per sample\u201436 cardiac or 54 hepatic MTs\u2014and incubation conditions such as three medium changes per day) have been optimized in this study and contrast with more conventional test methods that might only assess a single endpoint (often cell viability or gene expression at one time point).\n\n2. Relevant publications giving a comprehensive overview  \n\u2003(a) For the test system (the 3D microtissues):  \n\u2003\u2003\u2013 The InSphero product literature and technical notes (not detailed in the document but referenced implicitly via the \u201c3D InSight\u2122 Human Cardiac Microtissues\u201d and \u201c3D InSight\u2122 Human Liver Microtissues\u201d descriptions) cover aspects such as cell composition, donor variability, and culture conditions. These sources provide detailed information on the supplier, cell number, donor selection (cardiac MTs from one donor vs. hepatic MTs pooled from 10 donors), and maintenance media.  \n\u2003(b) For the test method (assessment of DMSO effects using multi-omics endpoints):  \n\u2003\u2003\u2013 \u201cDMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro\u201d (Scientific Reports, 2019) offers the most comprehensive overview. It details the entire workflow including:\n\u2003\u2003\u2003\u2022 Exposure design (0.1% DMSO, two\u2010week exposure, 7 time points),\n\u2003\u2003\u2003\u2022 Endpoints measured (full transcriptome analysis including mRNA and miRNA, proteomics, and MeDIP\u2010seq for DNA methylation), and \n\u2003\u2003\u2003\u2022 Data analysis pipelines (such as DESeq2 for differential gene expression and QSEA for methylation analysis).  \n\u2003\u2003This publication gives further insight into how standard protocols might be adapted for sensitive endpoints such as epigenome modifications and transcriptomic profiling.\n\n3. Prioritized list of further publications on the test method or its application (as cited in the document):  \n\u2003Priority 1 \u2013 Galvao et al. (2014)  \n\u2003\u2003\u2022 This publication (\u201cUnexpected low\u2011dose toxicity of the universal solvent DMSO\u201d) contains test chemical lists and demonstrates low\u2011dose effects of DMSO. It offers positive and negative control data which validate that even low concentrations can have biological effects.  \n\u2003Priority 2 \u2013 Sumida et al. (2011)  \n\u2003\u2003\u2022 \u201cEffects of DMSO on gene expression in human and rat hepatocytes\u201d provides an overview on how DMSO influences gene expression patterns. It contains comparisons with established benchmarks and demonstrates validation against other tests by comparing species responses.  \n\u2003Priority 3 \u2013 Yuan et al. (2016)  \n\u2003\u2003\u2022 \u201cEfficient long\u2011term cryopreservation of pluripotent stem cells at \u221280 \u00b0C\u201d provides technical insights into the use of DMSO as a cryoprotectant (including detailed descriptions of cryopreservation protocols) and discusses endpoint measurements relevant for cryopreservation assays.  \n\u2003Priority 4 \u2013 Iwatani et al. (2006)  \n\u2003\u2003\u2022 Although referenced primarily regarding epigenetic profile impacts of DMSO, this publication supports the concept that DMSO may modulate DNA methylation and chromatin modifications. It offers details on the action mode of DMSO in an epigenetic context.\n\n4. Short comments on information content from these publications:  \n\u2003\u2022 Galvao et al. (2014): Contains lists of test chemicals and describes dose\u2013response relationships. It provides control data against which the low-dose DMSO effects can be compared.  \n\u2003\u2022 Sumida et al. (2011): Contains baseline gene expression profiles in hepatocytes treated with DMSO, thereby serving as a validation and comparison for the multi-omics data described in the test.  \n\u2003\u2022 Yuan et al. (2016): Focuses on cryopreservation methodology, including specifics of cell handling, cryoprotectant mixtures, and related endpoints such as cell viability and recovery post-thaw.  \n\u2003\u2022 Iwatani et al. (2006): Provides evidence for DMSO\u2019s actions on epigenetic modifiers such as DNA methyltransferases; offers additional technical validation of epigenomic endpoints.\n\nAll the above details and relevant publications are derived from the context provided in \u201cDMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro\u201d (Scientific Reports, 2019).  \n\u2003\u2003_(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro, Scientific Reports 2019)_"
        },
        {
          "question_id": 68,
          "question_text": "Indicate whether the test method has been or could be linked to an AOP (or several AOPs) and in which form (e.g. test of KE activation). Can the test method cover an AOP MIE/KE? Reference relevant AOP and if in AOP-wiki, refer to status.",
          "answer_text": "Based on the information provided, the study did not explicitly \u201cmap\u2010in\u201d the assay to any particular AOP documented in the AOP\u2011Wiki. However, the endpoints measured by the assay\u2014such as large\u2010scale alterations in gene expression, miRNA deregulation, and especially the disruption of DNA methylation mechanisms (for example, the upregulation of DNMT1/DNMT3A and downregulation of TET1, with consequent genome\u2010wide hypermethylation in cardiac microtissues)\u2014are mechanistically relevant to key events in adverse outcome pathways. For instance, these endpoints could be considered as activation of a molecular initiating event (MIE) or downstream key events (KEs) in putative AOPs linking alterations of the epigenetic landscape to adverse developmental or reproductive outcomes (such as effects on embryo viability) given the discussion on potential impacts in assisted reproductive technology. \n\nAlthough no specific AOP is referenced in the paper, the measured changes align with the types of epigenetic disruption that are currently under discussion in the AOP\u2011Wiki (often as \u201cunder development\u201d or not yet fully established). In practical terms, the assay could be used as a test for KE activation in an AOP that starts with epigenetic dysregulation (as a MIE or early KE), eventually triggering downstream effects on cellular processes relevant to developmental toxicity. \n\nThus, while the study does not formally link the method to a defined AOP, the endpoints it generates clearly offer the potential to cover one or several AOP key events\u2014in particular those related to epigenetic alterations and ensuing adverse developmental outcomes. (Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)"
        },
        {
          "question_id": 69,
          "question_text": "Indicate/summarize information on mechanistic validation, e.g. by omics approaches or by use of endpoint specific controls (MCC; section 5.5). Has it been explored in how far the system reflects human biology, signaling, tissue organization relevant to the form of toxicity to be assessed (e.g. nigrostriatal neurons should contain dopamine, liver tests relevant to cholestasis may need to contain bile canalicular structures, etc.)?",
          "answer_text": "The study conducted extensive mechanistic validation using high\u2010throughput, multi\u2010omics approaches. For example, the authors compared DMSO\u2010treated versus untreated microtissues by evaluating the full transcriptome (including both mRNAs and miRNAs), the proteome, and genome\u2011wide DNA methylation via MeDIP\u2011seq. Differential expression analyses and pathway overrepresentation (using tools such as ConsensusPathDB and the Reactome database) highlighted changes in key cellular processes such as metabolism, vesicle\u2010mediated transport, chromatin organization, and gene expression regulation. In addition, principal component analyses (PCA) clearly distinguished treated from control samples, thereby serving as an inherent endpoint\u2010specific control.\n\nFurthermore, by using two types of human 3D microtissues\u2014a maturing iPSC\u2011derived cardiac model and a mature hepatic model\u2014the study explored tissue\u2010specific responses. This design allowed them to discern that, for example, the maturing cardiac microtissues (which mimic certain aspects of fetal cardiomyocytes) were more susceptible to DMSO\u2011induced alterations (including extreme epigenetic changes and deregulation of miRNA biogenesis) compared to the mature hepatic cells. Although the study did not include specific markers such as dopamine for neuronal toxicity or bile canalicular structures for cholestatic injury, it has been clearly demonstrated that the employed omics endpoints and the distinct architecture of the cardiac versus hepatic microtissues capture key aspects of human biology and the molecular signaling relevant to the toxicity being assessed.\n\n_(Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)_"
        },
        {
          "question_id": 70,
          "question_text": "Indicate/summarize activities for test qualification, pre-validation or validation. Indicate e.g. ring trials, full (pre-)validations. Give an overview of compounds or libraries that have been tested.",
          "answer_text": "No test qualification, pre\u2011validation or validation activities (such as ring trials or full pre\u2011validation studies) are described in this study. The work was designed as an exploratory investigation of DMSO\u2019s biological effects using an in vitro 3D microtissue model, where only 0.1% DMSO (the sole compound tested) was applied to human cardiac and hepatic microtissues with analysis via transcriptomics, proteomics, and genome\u2011wide methylation profiling. There is no indication that additional reference compounds, compound libraries, or inter\u2010laboratory ring trials were performed as part of a full qualification/validation exercise.  \n_(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)_"
        },
        {
          "question_id": 74,
          "question_text": "Are there special legal requirements for running the test in your lab; are there special hazards associated with the test that may affect operators, bystanders, others (e.g. through waste).",
          "answer_text": "The provided document does not mention any special legal requirements or exceptional licensing needed for running the test beyond what is normally required for chemical and biological laboratory work. It is noted that DMSO is classified by the FDA as a class 3 solvent (similar to ethanol) with low toxic potential at the concentrations ordinarily used. In our assay the concentration is 0.1%, which is well below levels known to cause systemic toxicity.\n\nThat said, standard laboratory protocols for safe handling of chemicals still apply. Although DMSO is generally considered safe at low concentrations, it is known to enhance the penetration of both itself and any dissolved compounds through biological membranes. Thus, care should be taken to avoid inadvertent skin contact or other exposures during handling and waste disposal. Standard waste disposal protocols for chemicals should be followed to ensure that any DMSO-containing materials are managed safely.\n\n(Sources: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitro)"
        },
        {
          "question_id": 76,
          "question_text": "Are special permits (e.g. genetic work, stem cells, radioactivity, etc.) required? Are special facilities required? Is special ethical approval necessary (indicate approval document).",
          "answer_text": "The provided document does not mention any special permits, facilities, or ethical approvals (or corresponding approval documents) required for this assay. _(Source: DMSO induces drastic changes in human cellular processes and epigenetic landscape in vitr.pdf)_"
        }
      ]
    },
    {
      "file": "application of the virtual cell based assay for simulation-KJNA28694ENN.pdf",
      "LLM_challenge_level": 2,
      "passes": 25,
      "total": 77,
      "pass_rate": 32.47,
      "failures": [
        {
          "question_id": 1,
          "question_text": "Provide a descriptive title using normal language without technical terms or acronyms.",
          "answer_text": "Using a computer simulation to understand how chemicals affect cells in laboratory tests. (Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 2,
          "question_text": "Please describe in no more than 200 words the following:",
          "answer_text": "The Virtual Cell Based Assay (VCBA) is a computational model designed to simulate the in vitro fate of chemicals following acute exposure. It integrates physicochemical properties\u2014such as logKow, Henry Law Constant, molar volume, and molecular diffusion volume\u2014with experimental conditions (e.g., serum supplementation, medium volume, and cell density) to predict chemical partitioning among compartments like the medium, headspace, plastic, and intracellular spaces. The model utilizes a set of interconnected sub-models that represent fate and transport, cell partitioning, cell growth, and toxic effects to estimate key parameters such as the no\u2010effect concentration (NEC) and killing rate (Kr). By optimizing these parameters against experimental cytotoxicity data (e.g., from the BALB/c 3T3 Neutral Red Uptake assay), the VCBA provides insights into bioavailable concentration versus nominal dosing and how factors like lipophilicity and evaporation affect chemical exposure. This simulation tool, implemented in R and integrated into KNIME, supports the interpretation of in vitro results and can inform in vitro\u2013in vivo extrapolations for risk assessments. (Source: application of the Virtual Cell Based Assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 3,
          "question_text": "Which toxicological target (organ, tissue, physiological/biochemical function, etc.) is modelled? (8.1)",
          "answer_text": "The VCBA does not model a whole organ or tissue but rather focuses on the cell\u2010level response. In this assay the toxicological target is the cells themselves\u2014specifically, the 3T3 BALB/c cells used in the Neutral Red Uptake cytotoxicity assay. The model simulates key processes at the cellular level, including chemical uptake, partitioning, cell growth (using a cell cycle model), and cell death, which together serve as a measure of cytotoxicity. (Sources: application of the virtual cell based assay for simulation of in vitro chemical fate following acute exposure, Sections 1 (Abstract), 3 (Methodology))"
        },
        {
          "question_id": 4,
          "question_text": "Which test system and readout(s) are used? (4.1; 5.2)",
          "answer_text": "The assay is carried out using an in vitro cell\u2010based system employing the BALB/c 3T3 cell line (cultured in 96\u2010well microplates with serum\u2010supplemented medium), and its primary readout is cell viability measured by the Neutral Red Uptake (NRU) assay. This readout is used to generate concentration\u2013response curves\u2014in terms of % viability from which IC50 values (and the model\u2019s NEC and killing rate, Kr) are derived. (Sources: Abstract; Table 7; sections 4.1 and 5.2)"
        },
        {
          "question_id": 5,
          "question_text": "Which biological process(es) (e.g. neurite outgrowth, differentiation) and/or toxicological events (e.g. oxidative stress, cell death) are modelled/reflected by your test method? (8.1)",
          "answer_text": "The VCBA does not model specialized endpoints like neurite outgrowth or differentiation. Instead, it integrates a dynamic simulation of chemical exposure and cellular response by reflecting how a chemical partitions in the in vitro system and impacts cell dynamics. In particular, the method models:\n\n\u2022 Chemical fate in the test system (including partitioning among medium, plastic, serum proteins/lipids, evaporation, and degradation), which in turn determines the bioavailable (dissolved) concentration.\n\n\u2022 Cellular processes\u2014specifically, cell uptake, cell cycle progression (cell growth and division through defined phases), and ultimately cell death. The toxicity model uses the experimental concentration\u2010response curve combined with cell growth/death dynamics to derive key parameters such as the no\u2010effect concentration (NEC) and the killing rate (Kr).\n\nThus, the primary toxicological event reflected is cell death (cytotoxicity) as a result of chemical exposure, while the biological process modeled is the overall cell growth/death dynamic in vitro.\n\n(Sources: application of the Virtual Cell Based Assay for Simulation of in vitro Chemical Fate Following Acute Exposure, sections on \u201cToxicity and effects model\u201d and \u201cCell growth and division model\u201d)"
        },
        {
          "question_id": 6,
          "question_text": "To which (human) adverse outcome(s) is your test method related or could be related? (8.1; 9.2; 9.3)",
          "answer_text": "The VCBA is designed to simulate the in\u2005vitro fate of chemicals so that the observed cytotoxicity (e.g. via cell viability changes measured by assays such as the Neutral Red Uptake test in BALB/c 3T3 cells) can be linked to systemic doses. In particular, the method has been applied for the prediction of acute systemic toxicity (for example, as reflected by acute oral toxicity or LD50 in animal tests) by allowing a more realistic estimation of the bioavailable (dissolved) concentrations that cells are exposed to. This provides a basis for establishing IVIVE (in\u2005vitro\u2011to\u2011in\u2005vivo extrapolations) that could predict adverse outcomes such as acute poisoning effects (i.e. acute oral toxicity) in humans.\n\n(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 7,
          "question_text": "Which hazard(s) do(es) your test method (potentially) predict? (8.1; 8.6)",
          "answer_text": "The VCBA is used to simulate how chemicals behave in an in vitro test system and, by linking the dissolved (i.e., bioavailable) concentration that causes cytotoxicity in 3T3 BALB/c cells to the observed in vivo effects, it can be used to predict hazards related to acute systemic toxicity (for example, acute oral toxicity as classified under the EU CLP system). In other words, by evaluating how chemicals affect cell viability\u2014with parameters such as the no\u2010effect concentration (NEC) and killing rate (Kr) optimized against cytotoxicity data\u2014the method helps to indicate whether a chemical might be hazardous in an acute exposure scenario. (Sources: application of the virtual cell based assay for simulation-KJNA28694EN.pdf, Abstract and Section 4.4)"
        },
        {
          "question_id": 8,
          "question_text": "Does the test method capture an endpoint of current regulatory studies? (9.5)",
          "answer_text": "The assay is designed to capture a toxicological endpoint that is relevant for current regulatory studies\u2014in particular, it generates an in vitro cytotoxicity endpoint (commonly measured as an IC50 via the Neutral Red Uptake assay) that is used as a surrogate for acute toxicity (e.g., rat oral LD50) in regulatory classifications. Although the correlation between the in vitro (nominal or predicted dissolved concentration) and in vivo endpoints is not perfect, the method specifically addresses endpoints (such as acute toxicity classification under the EU CLP system) that are critical in current regulatory studies. \n\n(Source: Sections \u201c1. Abstract\u201d and \u201c4.4 Impact of chemical partitioning on toxicity prediction\u201d in the provided document)"
        },
        {
          "question_id": 9,
          "question_text": "If the method has undergone some form of validation/evaluation, give its status. (9.4)",
          "answer_text": "The VCBA has been evaluated using a retrospective analysis of 83 chemicals tested with the standardized 3T3 NRU assay. In this evaluation the model parameters (NEC and Kr) were optimized to reproduce the concentration\u2013response data, and the predicted chemical fate (partitioning, evaporation, degradation) was compared to available in vivo toxicity information. However, while the VCBA shows promise and has been applied to a representative dataset, its predictions still exhibit discrepancies and uncertainties (e.g., regarding evaporation and kinetic extrapolations), so further experimental benchmarking and validation are needed before it can be used as a standalone regulatory tool. \n\n(Sources: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 10,
          "question_text": "Provide the original/published name, as well as the potential tradename.",
          "answer_text": "The original published title of the assay is \u201cApplication of the Virtual Cell Based Assay for Simulation of in vitro Chemical Fate following Acute Exposure.\u201d It is also commonly referred to by its abbreviation VCBA, which may serve as its potential tradename.  \n_(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)_"
        },
        {
          "question_id": 12,
          "question_text": "This only applies to updated versions. If this is the original version, state 'original version'.",
          "answer_text": "original version (Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 13,
          "question_text": "Normal text names often do not uniquely define the method. Therefore, each method should be assigned a clearly and uniquely defined data base name.",
          "answer_text": "In the context of the VCBA report, it is essential to assign a unique database name because normal text names (such as \u201cvirtual cell based assay\u201d) can be ambiguous. A clearly defined and unique identifier allows every version or variant of the method\u2014as well as its specific experimental and kinetic parameters (e.g., cell line specifics, serum supplementation level, exposure time, and physical\u2010chemical input parameters)\u2014to be unambiguously referenced. This unique naming facilitates data retrieval, comparison between studies, and regulatory acceptance as it clearly distinguishes one method from another. \n\n(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 14,
          "question_text": "Include affiliation.",
          "answer_text": "The assay was developed by scientists at the Joint Research Centre (JRC) of the European Commission. More specifically, the work was carried out within the Directorate General Joint Research Centre, Directorate F \u2013 Health, Consumers and Reference Materials, Chemicals Safety and Alternative Methods Unit (F.3), which incorporates EURL ECVAM. (Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 16,
          "question_text": "For example, the principal investigator (PI) of the lab, the person who conducted the experiments, etc.",
          "answer_text": "The report does not explicitly state which individual served as the principal investigator or who personally conducted each experiment. Instead, it provides an authorship list and contact details for key contributors. For example, the report lists authors such as Susana Proen\u00e7a, Alicia Paini, Elisabeth Joossens, Jos\u00e9 Vicente Sala Benito, Elisabeth Berggren, Andrew Worth, and Pilar Prieto, with contact information provided for Pilar Prieto and Alicia Paini. These details suggest that the work was performed by a team at the Joint Research Centre rather than attributing the experimental work to one specific \u201cPI\u201d or experimenter. \n\n(Sources: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 17,
          "question_text": "Supply number of supporting files. Describe supporting files (e.g. metadata files, instrument settings, calculation template, raw data file, etc.).",
          "answer_text": "There are six supporting files accompanying the report. For example, one file (Annex 1) contains the finalized VCBA code (in R, with all equations and cell/experimental parameters), a second file (Annex II) provides a table with the optimized parameters (NEC and Kr) obtained by fitting the experimental concentration\u2013response curves, a third file is the KNIME workflow that integrates the VCBA simulation into a user\u2010friendly graphical environment, a fourth is a metadata file containing the compiled physicochemical properties (logKow, HLC, molar and diffusion volumes, etc.) and cell\u2010line/assay settings, a fifth is a raw data file with the experimental Neutral Red Uptake assay results (concentration\u2013response curves) used for model calibration, and finally a calculation template file that details the QSAR regression equations and procedures (used to compute parameters such as molecular diffusion and molar volumes).  \n_(Source: application of the Virtual Cell Based Assay for Simulation-KJNA28694ENN.pdf)_"
        },
        {
          "question_id": 18,
          "question_text": "Describe briefly whether the cells are from a commercial supplier, continuously generated by cell culture, or obtained by isolation from human/animal tissue (or other).",
          "answer_text": "The assay uses the 3T3 BALB/c cell line, which is an immortalized murine fibroblast line maintained continuously in culture rather than being freshly isolated from animal tissue. Such cell lines are typically available from commercial suppliers. (Source: application of the Virtual Cell Based Assay for Simulation of in vitro Chemical fate following Acute Exposure, Sections 1 and 3.5.1)"
        },
        {
          "question_id": 19,
          "question_text": "Give a brief overview of your biological source system, i.e. the source or starting cells that you use. Which cell type(s) are used or obtained (e.g. monoculture/co-culture, differentiation state, 2D/3D, etc.)? If relevant, give human donor specifications (e.g. sex, age, pool of 10 donors, from healthy tissue, etc.).",
          "answer_text": "The VCBA has been applied using a well\u2010established, continuously growing monoculture of 3T3 BALB/c cells, which are mouse fibroblasts. These cells are cultured in a conventional two\u2010dimensional (2D) system (e.g. in standard 96\u2010well plates) and are used as a uniform cell type without co-culture or differentiation into specific lineages. Donor information is not applicable here, as the cells come from an immortalized mouse cell line rather than primary human tissue.\n\n(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 20,
          "question_text": "List quantitative and semi-quantitative features that define your cell source/starting cell population. For test methods that are based on differentiation, describe your initial cells, e.g. iPSC, proliferating SH-SY5Y; the differentiated cells are described in section 4. Define cell identity, e.g. by STR signature (where available), karyotype information, sex (where available and relevant), ATCC number, passage number, source (supplier), sub-line (where relevant), source of primary material, purity of the cells, etc. Describe defining biological features you have measured or that are FIRMLY established (use simple listing, limit to max. 0.5 pages), e.g. the cells express specific marker genes, have specific surface antigens, lack certain markers, have or lack a relevant metabolic or transporting capacity, have a doubling time of x hours, etc. Transgenic cell lines have particular requirements concerning the characterization of the genetic manipulation (type of transgene, type of vector, integration/deletion site(s), stability, etc.). Organoids and microphysiological systems (MPS) may need some special/additional considerations as detailed in Pamies et al. (2018) and Marx et al. (2016), e.g. ratio of cell types used, percent of normal cells in tumor spheroids created from resected tissue; derivation of cells for re-aggregating brain cultures.",
          "answer_text": "The VCBA was applied using the BALB/c 3T3 cell line. Key features of the starting cell population include (see section \u201c3.5.1. Cell line parameters\u201d and Table 6 in the document):\n\n\u2022\u2003Cell type: Murine fibroblasts (BALB/c 3T3).  \n\u2022\u2003Starting cell number: 1,680 cells per well (for a 96\u2010well plate).  \n\u2022\u2003Cell composition (by mass fractions):  \n\u2003\u2013\u2003Aqueous fraction: 61.4%  \n\u2003\u2013\u2003Protein fraction: 24.4% (with an intracellular protein concentration of 4.4 mol/m\u00b3)  \n\u2003\u2013\u2003Lipid fraction: 14.2% (with an intracellular lipid concentration of 170.7 kg/m\u00b3)  \n\u2022\u2003Cell cycle distribution:  \n\u2003\u2013\u2003Initial population percentages: 50.7% in G1, 19.2% in S, 18.2% in G2, and 11.9% in M phase.  \n\u2003\u2013\u2003Phase durations: G1 = 9.63 hours, S = 3.65 hours, G2 = 3.45 hours, M = 2.26 hours (giving an approximate doubling time close to the sum of these phases, roughly 19 hours).  \n\u2022\u2003Cell division rate: Approximately 1.026 h\u207b\u00b9 as optimized for the assay.\n\nAdditional details such as STR signature, karyotype information, ATCC number, supplier, passage number, or data on marker gene/surface antigen expression are not specified in the provided context; however, these defined quantitative and semi\u2010quantitative features sufficiently characterize the starting cell population for the VCBA simulation.\n\n(Sources: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf, sections 3.5.1 and Table 6.)"
        },
        {
          "question_id": 21,
          "question_text": "Describe the acceptance criteria (AC) for your initial cells (i.e. the quality criteria for your proliferating cell line, tissue for isolation, organism, etc.). Which specifications do you consider to describe the material, which quality control criteria have to be fulfilled (e.g. pathogen-free)? Which functional parameters (e.g. certain biological responses to reference substances) are important? For iPSC maintenance: How do you control pluripotency? How stable are your cells over several passages? Which passage(s) are valid? For primary cells: Show stability and identity of supply; demonstrate stability of function (e.g. xenobiotic metabolism). Quantitative definitions for AC should be given based on this defining information. Exclusion criteria (features to be absent) are also important. As in 3.3., special/additional requirements apply to genetically-modified cells and microphysiological systems.",
          "answer_text": "The VCBA\u2010based approach is built on the use of well\u2010characterized cell lines so that the in vitro kinetic modelling is based on reproducible, quality\u2010controlled cellular inputs. Although the report mainly focuses on chemical fate and toxicodynamics rather than a full \u201ccell bank\u201d description, it is implicit that acceptance criteria (AC) for the \u201cinitial cells\u201d include both identity and functionality criteria as follows (Sources: application of the virtual cell based assay for simulation\u2011KJNA28694ENN.pdf, Sections 3.5.1 and Table 6):\n\n1. Identity and Purity \n\u2003\u2022\u2003The cell line (e.g. the 3T3 BALB/c cell line used in the VCBA simulations) must be authenticated, free of pathogens (including mycoplasma, bacteria, and viruses), and unambiguously identified (e.g. via STR profiling or other accepted methods).  \n\u2003\u2022\u2003For primary cells or iPSC\u2010derived material, additional criteria include a proven identity of the donor tissue or established markers relevant for that cell type (e.g. for iPSC \u2013 expression of pluripotency markers such as OCT4, NANOG, SSEA\u20103/4, and TRA\u20101\u201160).\n\n2. Functional Performance and Growth Characteristics \n\u2003\u2022\u2003Cells must exhibit robust and reproducible proliferation. In the VCBA, the initial cell number is defined carefully (e.g. an initial seeding of 1,680 cells per well ensuring that the cells grow during the 48\u2011hour exposure without reaching confluence [50,000 cells/cm\u00b2] which would inhibit growth).  \n\u2003\u2022\u2003Quantitative specifications are provided by the kinetic parameters of the cell cycle (for example, the duration of individual cell cycle phases, mortality rates, and the division rate [1.026 h\u207b\u00b9]) as shown in Table 6. These serve as functional acceptance criteria.  \n\u2003\u2022\u2003For iPSC maintenance, acceptance criteria require that pluripotency is regularly confirmed (e.g. by immunocytochemistry, RT\u2010PCR for key markers, and differentiation assays), that the cells maintain a stable phenotype over a defined number of passages (with only early passages being allowed for experiments) and that their growth kinetics remain unchanged within expected quantitative limits.  \n\u2003\u2022\u2003For primary cells, stability in both identity and function (for example, consistent xenobiotic metabolism rates or other specific biochemical responses to reference chemicals) must be demonstrated over multiple batches and with defined passage numbers.\n\n3. Quality Control (QC) Specifications\n\u2003\u2022\u2003Cells should be maintained under defined and reproducible culture conditions (including serum supplementation, media volumes, temperature, and CO\u2082 levels) so that the functional parameters (e.g. response on exposure to reference compounds) remain consistent.  \n\u2003\u2022\u2003Batch\u2010to\u2010batch reproducibility is confirmed by baseline response curves (e.g. an in vitro concentration\u2010response with accepted IC\u2085\u2080 values) under control conditions.\n\u2003\u2022\u2003For genetically modified cells or microphysiological systems, additional specifications (such as level and stability of transgene expression, retention of modified function, and exclusion of off\u2011target genomic effects) must be established and quantified.\n\n4. Exclusion Criteria\n\u2003\u2022\u2003Any indication of microbial contamination (e.g. positive mycoplasma tests), aberrant growth patterns, or inconsistent responses to reference substances (i.e. if the expected toxicity or viability response falls outside predefined quantitative limits) would lead to exclusion.\n\u2003\u2022\u2003For iPSCs, signs of spontaneous differentiation or loss of pluripotency markers are exclusion criteria.\n\u2003\u2022\u2003For primary cells, a drift in metabolic function, loss of characteristic morphology, or decline in enzyme activity beyond set thresholds would result in rejection.\n\nThus, the AC for the initial cellular material is defined quantitatively (via cell number, cell\u2010cycle timelines, viability rates, and biochemical markers) and qualitatively (by ensuring pathogen\u2010free, authenticated, and functionally competent cells). These criteria ensure that the simulated in vitro fate (using the VCBA) is built on reliable and reproducible cellular inputs."
        },
        {
          "question_id": 22,
          "question_text": "Name known causes of variability of the initial cells/source cells. Indicate critical consumables or batch effects (e.g. relevance of the plate format and supplier, batch effects of fetal calf serum (FCS) or serum replacement, critical additives like type of trypsin, apo-transferrin vs. holo-transferrin, etc.). Indicate critical handling steps and influencing factors (e.g. special care needed in pipetting, steps that need to be performed quickly, cell density, washing procedures, etc.). As in 3.3., special/additional requirements apply to genetically-modified cells and microphysiological systems, e.g. dependence on matrix chemistry and geometry, dependence on microfluidics system, consideration of surface cells vs core cells, etc. Give recommendations to increase/ensure reproducibility and performance.",
          "answer_text": "The document highlights several sources of variability and critical factors in setting up cell\u2010based assays. For example, in the cell parameters section (see Section 3.5.1), it is noted that the initial cell number must be carefully controlled because 3T3 BALB/c cells are growth\u2010inhibited at confluence (\u224850,000 cells/cm\u00b2); using an inappropriate plating density may require a different fecundity function and can lead to variability in cell growth. In addition, the overall experimental set\u2010up (Section 3.6 and Section 4.3) shows that key consumables and batch\u2010to\u2010batch differences can strongly influence outcomes. For instance, the type and batch of serum (e.g., fetal calf serum) are critical because different serum levels\u2014and, importantly, using the exact same batch\u2014can affect the chemical partition (especially binding to proteins/lipids) as well as the reproducibility of the assay. Similarly, the plate format (geometry, well volume, plastic surface area) is also influential because binding to plastic can sequester chemicals, and even small differences (e.g. supplier, surface properties) may alter the overall dissolved concentration. Although the document does not detail every consumable (such as the type of trypsin, or comparing apo\u2010transferrin to holo\u2010transferrin), it indicates that these factors\u2014as well as critical handling steps such as careful pipetting, rapid handling during cell seeding and washing to minimize variability, and maintaining consistent cell density\u2014are essential to increase reproducibility. Finally, in specialized setups (e.g. genetically\u2011modified cells or microphysiological systems), additional variability may come from matrix chemistry and geometry, microfluidics system design, and differences between surface versus core cells, meaning that extra care must be taken under those conditions.\n\nRecommendations for increasing reproducibility include:\n\u2022 Strictly controlling and standardizing the initial cell density/plating to avoid inadvertent confluency effects.\n\u2022 Using the same batches of critical reagents (e.g., fetal calf serum or serum replacements) and consumables (e.g., plate materials from the same supplier) from experiment to experiment.\n\u2022 Implementing a detailed standard operating procedure (SOP) for pipetting, cell seeding, washing, and other handling steps as variability in these steps may influence the outcome.\n\u2022 For genetically\u2011modified cells or microphysiological systems, paying extra attention to matrix composition, device geometry and microfluidics conditions, and considering possible differences between surface\u2010exposed and core cells.\n\n_(Sources: application of the virtual cell based assay for simulation-KJNA28694EN.pdf, Sections 3.5.1, 3.6, 4.3)_"
        },
        {
          "question_id": 25,
          "question_text": "Describe the test system as it is used in the test. If the generation of the test system involves differentiation steps or complex technical manipulation (e.g. formation of microtissues), this is described in 3.6. Give details on the general features/principles of the culture protocol (collagen embedding, 3D structuring, addition of mitotic inhibitors, addition of particular hormones/growth factors, etc.) of the cells that are used for the test. What is the percentage of contaminating cells; in co-cultures what is the percentage of each subpopulation? Are there subpopulations that are generally more sensitive to cytotoxicity than others, and could this influence viability measures? Is it known whether specific chemicals/chemical classes show differential cytotoxicity for the cell sub-populations used?",
          "answer_text": "The test system used is based on a standard, unicellular culture of 3T3 BALB/c cells. No differentiation steps, 3D structuring (such as collagen embedding or formation of microtissues), or additional complex technical manipulations (e.g. use of mitotic inhibitors or hormone/growth factor supplementation) are described in the model. The cells are cultured in a conventional 2D format in 96\u2010well microplates with a defined medium volume and a 5% serum supplementation (providing protein and lipid for partitioning), and the assay is run over a 48\u2010hour period. The protocol is designed for a homogeneous cell population (see section 3.5.1 \u201cCell line parameters\u201d and section 3.5.2 \u201cExperimental set up\u201d of the document). \n\nBecause the test relies on a single cell type (3T3 BALB/c fibroblasts), there is no reported co-culture and thus no assignment of percentages among different subpopulations or contaminating cells. Consequently, there is no evidence from the report that subpopulations (if any minor heterogeneities exist due to cell cycle phases) show differential cytotoxic sensitivity that would influence viability measures. Likewise, it is not known from the document whether specific chemicals or chemical classes show differential cytotoxicity for discrete cell subpopulations within this system. \n\n(Source: application of the virtual cell based assay for simulation of in vitro chemical fate following Acute Exposure, sections 3.5.1 and 3.5.2)"
        },
        {
          "question_id": 28,
          "question_text": "Give known causes of variability for final test system state. Indicate critical consumables or batch effects (e.g. plate format and supplier, batch effects of FCS or serum replacement, additives). Indicate critical handling steps, and/or influencing factors identified (e.g. special care needed in pipetting, steps that need to be performed quickly, cell density). Indicate positive and negative controls and their expected values, and accepted deviation within and between the test repeats. Give recommendations to increase/ensure reproducibility and performance.",
          "answer_text": "Based on the VCBA technical report, several factors can contribute to variability in the final state of the in vitro test system:\n\n1. Consumables and batch\u2010to\u2010batch effects:\n\u2003\u2022 The type and supplier of the plasticware (e.g. plate format and manufacturer) can affect chemical binding to the plastic and thus the overall chemical partitioning.\n\u2003\u2022 Serum supplementation (FCS or serum replacement) is critical because both its concentration and batch may strongly influence the protein and lipid content in the medium. Variability here alters chemical sequestration (for instance, differences between 0%, 5%, and 10% serum supplementation were shown to change the dissolved versus bound fractions).\n\u2003\u2022 Additives or media components with variable characteristics (such as lipids or proteins from different batches) may also contribute to variability.\n\n2. Critical handling steps and influencing factors:\n\u2003\u2022 Accurate pipetting and rapid processing are necessary\u2014for example, ensuring that cell seeding, media addition, and compound dispensing are performed precisely to avoid unwanted evaporation or chemical partition shifts.\n\u2003\u2022 The initial cell density must be controlled. The report stresses that, for 3T3 BALB/c cells, the initial number should be chosen so that cells can grow over 48 hours without achieving confluence (see Section 3.5.1 and the associated discussion on cell cycle parameters in Table 6).\n\u2003\u2022 Variability can also arise from differences in timing (steps that must be performed quickly) which could affect the chemical exposure profile and, in turn, the cell\u2019s response.\n\n3. Controls and deviations:\n\u2003\u2022 In the standard Neutral Red Uptake assay performed with this system, a 0\u2010\u00b5M (control) condition is used to define 100% viability. This negative control establishes the baseline response.\n\u2003\u2022 Positive controls (cytotoxic compounds with well\u2010characterized in vitro responses) are expected to yield a reproducible decrease in cell viability (for example, an optimized killing rate [Kr] and no-effect concentration [NEC] are determined against known concentration\u2013response curves). Although specific numeric acceptance criteria or deviation limits are not detailed in the report, consistency across replicate tests is essential.\n\u2003\u2022 Accepted variations within and between tests are ideally minimized by using standardized procedures, validated lot numbers for critical reagents (e.g. serum and media), and harmonized experimental set\u2010up parameters (as described in Section 3.5.2 and Table 7).\n\n4. Recommendations to increase reproducibility and performance:\n\u2003\u2022 Use well\u2010characterized and standardized consumables (e.g. plates from a single supplier and serum lots validated for low batch variability).\n\u2003\u2022 Follow strict, detailed handling protocols\u2014ensuring precision in pipetting, rapid processing where needed, and careful cell counting/seeding to maintain optimal cell density.\n\u2003\u2022 Run concurrent positive and negative controls (using control compounds and untreated wells) to monitor assay performance and allow intra\u2010 and inter-assay comparison.\n\u2003\u2022 Optimize and validate the kinetic parameters (NEC and Kr) for the specific test set-up and routinely check that the experimental conditions (such as media volume, serum percentage, and incubation time) remain consistent.\n\u2003\u2022 Where possible, document and monitor minor variations (e.g. temperature affecting Henry\u2019s law constant) since even small deviations in these physical\u2013chemical parameters could alter chemical fate predictions.\n\nThese recommendations and observations are derived from the methodological discussion and sensitivity analysis described in the report (Sources: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf, Sections 3.5.1, 3.5.2, 4.2, and 4.3)."
        },
        {
          "question_id": 29,
          "question_text": "What is known about endogenous metabolic capacity (CYP system (phase I); relevant conjugation reactions (phase II))? What is known about other pathways relevant to xenobiotic metabolism? What specific information is there on transporter activity?",
          "answer_text": "The report does not include any information that indicates an endogenous metabolic capacity via the CYP system (phase I) or phase II conjugation reactions. In fact, the VCBA model was used with 3T3\u00a0BALB/c cells, which are not metabolically competent. Therefore, processes such as CYP-mediated oxidation, other phase I reactions, and relevant phase II conjugation reactions are not incorporated into the model (Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf, Sections 4.6 and 3.5.1).\n\nSimilarly, the report does not address additional pathways relevant to xenobiotic metabolism (for example, other biotransformation routes) beyond noting the absence of metabolic transformation in the chosen cell line. \n\nIn terms of transporter activity, no specific information is provided. The model assumes that cell uptake occurs only by passive diffusion; active transport mechanisms are not considered in the VCBA (Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf, Section 4.6)."
        },
        {
          "question_id": 31,
          "question_text": "Give information on where the test system differs from the mimicked human tissue and which gaps of analogy need to be considered.",
          "answer_text": "The in vitro test system (here, the 3T3 BALB/c cell\u2010based assay simulated by the VCBA) differs from human tissue in several key ways:\n\n1. In vivo human tissue is part of an integrated, multi\u2010organ system with fully developed metabolism, clearance, and active transport processes. By contrast, the test system is based on a single cell type that is not metabolically competent (i.e. metabolic activation, clearance, and tissue\u2010specific differentiation are not captured), which means it does not mimic the full range of ADME (absorption, distribution, metabolism, and excretion) processes observed in human tissues.  \n\u2003\u2003_(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf, Sections \u201cIntroduction\u201d and \u201cAssumptions and uncertainties\u201d)_\n\n2. The human tissue environment exhibits complex cell\u2013cell and cell\u2013matrix interactions and blood flow dynamics that influence chemical distribution and toxicity. In contrast, the controlled in vitro system uses a microplate setup under static conditions, where factors such as protein and lipid binding, plastic interaction, and even evaporation are modeled but do not fully reproduce the dynamic interplay occurring in human tissues.  \n\u2003\u2003_(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf, Sections \u201cMethodology (3.6 \u2013 Experimental set up)\u201d and \u201cAssumptions and uncertainties\u201d)_\n\n3. Dosimetry\u2010related differences are also critical: the nominal concentrations applied in vitro can differ substantially from the bioavailable concentration because the test system does not account for tissue saturation, active clearance, or interstitial fluid dynamics. This means that the extrapolation from in vitro effective doses to in vivo doses must consider additional kinetic processes that the test system does not inherently simulate.  \n\u2003\u2003_(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf, Sections \u201cResults and discussion\u201d and \u201cAssumptions and uncertainties\u201d)_\n\nIn summary, the gaps of analogy include the lack of:\n\u2003\u2022 Multiplicity of cell types and organ\u2010level interactions;\n\u2003\u2022 Active metabolism and clearance mechanisms;\n\u2003\u2022 Dynamic blood flow and tissue architecture;\n\u2003\u2022 Saturation kinetics that occur in a living organism.\n\nThese differences should be carefully considered when extrapolating in vitro findings to predict in vivo human toxicity."
        },
        {
          "question_id": 32,
          "question_text": "Are there elements of the test system that are protected by patents or any other means?",
          "answer_text": "The document makes no reference to any of the test system components (such as the VCBA methodology, its code, or associated algorithms) being protected by patents or similar intellectual property rights. The text does not state that any element of the test system is covered by patents or protected by any other legal means. (Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 34,
          "question_text": "Provide an exposure scheme (graphically, show timelines, addition of medium supplements and compounds, sampling, etc.), within the context of the overall cell culture scheme (e.g. freshly re-plated cells or confluent cells at start, certain coatings, etc.). Include medium changes, cell re-plating, whether compounds are re-added in cases of medium change, critical medium supplements, etc.",
          "answer_text": "The VCBA simulations assumed a typical 48\u2010hour in vitro exposure performed in 96\u2010well plates using a standard protocol similar to the Neutral Red Uptake assay. In such experiments the overall scheme is as follows (see Sources below):\n\n1. At time zero (t = 0), a sub\u2010confluent population of cells (for example, freshly re\u2010plated 3T3 BALB/c cells with an initial well\u2010number around 1,680 cells per well and an initial cell\u2010cycle distribution of roughly 50% in G1, 19% in S, etc.) is seeded on standard tissue\u2010culture plastic. No special coatings are mentioned, so standard plates are used.\n\n2. Once plated, 100 \u00b5L of cell culture medium is added to each well. This medium is supplemented with 5% serum, which provides the requisite proteins and lipids needed for chemical partitioning. (See \u201cExperimental set up according to Neutral Red Uptake protocol\u201d in Table 7 and cell line parameters in section 3.5.1.)\n\n3. Immediately at t = 0 the test compounds are added at their nominal concentrations. In the simulation all chemicals are added only once at the beginning of the exposure period. No re\u2010addition of compounds takes place later, and no medium changes are performed during the 48\u2010hour exposure. (See section 3.6 on \u201cOptimization and running the VCBA\u201d, where it is noted that cells are exposed from the start for 48 hours with the compound remaining in the fixed volume.)\n\n4. Over the course of the 48 hours, the VCBA model calculates time\u2010dependent chemical partitioning among the medium, cells, plastic, and headspace. The model tracks dissolved (bioavailable) concentrations as well as intracellular accumulation and evaporation effects continuously (the simulation uses short time steps, e.g. seconds, within each hour as illustrated in Figure 6).\n\n5. At the end of 48 hours the assay is terminated. Sampling is effectively done by measuring cell viability (using Neutral Red Uptake measurements) and correlating the resulting concentration\u2013response curves to the simulated dissolved concentrations.\n\nA graphic timeline of the scheme would show:\n\n\u2022\u2003T = 0 hours: Freshly plated (sub-confluent) cells are provided in a 96\u2011well plate on standard plastic. Medium (100 \u00b5L) containing 5% serum is added and the test compound(s) are also added at the desired concentrations.\n\n\u2022\u2003T = 0\u201348 hours: No medium change or re\u2010plating is performed. The cells are allowed to grow (following the pre-defined cell cycle model), while the chemical undergoes dynamic partitioning (dissolution, binding to serum proteins/lipids, plastic, and evaporation) as simulated in the VCBA model. Internal measurements (via simulation) capture chemical levels at every time point.\n\n\u2022\u2003T = 48 hours: The experiment is terminated and viability is assessed from the accumulated cell responses.\n\nThus, the complete exposure scheme involves a single compound addition at the start, continuous incubation with serum-supplemented medium, and no medium replacement, re-plating, or re-addition of compound. Critical medium supplements (5% serum) are added at t = 0 to provide external binding proteins and lipids that can markedly influence the free dissolved concentration. Sampling for viability or chemical fate endpoints is effectively \u201cvirtual\u201d (tracked continuously in the simulation) with the final readout taken at the 48-hour endpoint.\n\n(Sources: \u201cExperimental set up according to Neutral Red Uptake protocol\u201d in Table 7; sections 3.5.1 \u201cCell line parameters\u201d and 3.6 \u201cOptimization and running the VCBA\u201d.)"
        },
        {
          "question_id": 35,
          "question_text": "Define the specific endpoint(s) of the test system that you use for toxicity testing (e.g. cytotoxicity, cell migration, etc.). Indicate whether cytotoxicity is the primary endpoint. What are secondary/further endpoints? Also describe here potential reference/normalization endpoints (e.g. cytotoxicity, protein content, housekeeping gene expression) that are used for normalization of the primary endpoint.",
          "answer_text": "The VCBA is used to simulate and interpret in vitro cytotoxicity data. In our test system using the 3T3 BALB/c cell line, the primary endpoint is cytotoxicity as measured by cell viability via the Neutral Red Uptake (NRU) assay. In addition, the system models the dynamics of cell number over time (cell growth and division) so that two key toxicity parameters\u2014the no\u2010effect concentration (NEC) and killing rate (Kr)\u2014can be derived from the concentration\u2013response curves. These secondary endpoints (NEC, Kr, and associated simulations of dissolved and intracellular chemical concentrations) support an understanding of how chemicals distribute among compartments (e.g. medium, lipid, protein, plastic) and thereby help interpret the observed toxicity.\n\nFor normalization of the primary cytotoxicity endpoint, the assay relies on the control condition (0 \u00b5M test compound) that is assumed to represent 100% viability. This serves as a reference against which all concentration\u2013response data are normalized. Although the current test system does not explicitly include additional normalization markers such as housekeeping gene expression or protein content, the control viability (and, where relevant, parameters like cell division rate) plays that normalization role by providing a baseline for evaluating chemical effects.\n\n(Sources: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 36,
          "question_text": "Define and describe the principle(s) of the analytical methods used. Provide here a general overview of the method\u2019s key steps (e.g. cells are fixed or not, homogenized sample or not, etc.), sufficient for reviewers/regulators to understand what was done, but not in all detail for direct repetition. If you have two or more endpoints (e.g. viability and neurite outgrowth), do you measure both in the same well, under same conditions in parallel, or independently of each other? For imaging endpoints: Explain in general how quantification algorithm or how semi-quantitative estimates are obtained and how many cells are imaged (roughly).",
          "answer_text": "The analytical approach consists of two main elements. First, the cytotoxicity data are obtained using a standard cell\u2010based assay\u2014in this case, a Neutral Red Uptake (NRU) assay in 3T3 BALB/c cells\u2014and second, these data are interpreted using the Virtual Cell Based Assay (VCBA), an in silico model that simulates the chemical\u2019s fate in the in vitro system.\n\n1. NRU Cytotoxicity Assay:  \n\u2003\u2022 In the NRU assay, cells are plated in a 96\u2010well format (using 100\u202f\u00b5L medium per well, with defined serum supplementation, e.g. 5% FBS) and then exposed to increasing concentrations of a test chemical.  \n\u2003\u2022 After a 48\u2010hour incubation, live cells take up the neutral red dye into their lysosomes. The assay does not require cell fixation or homogenization since the read\u2010out is based on the uptake of dye by viable, metabolically active cells.  \n\u2003\u2022 Based on the subsequent dye extraction and spectrophotometric measurement, a concentration\u2013response curve is generated. In this study only viability was used as the endpoint; if multiple endpoints (like viability and neurite outgrowth) were assayed, they would be measured independently under parallel conditions rather than in a single well.  \n\n2. Virtual Cell Based Assay (VCBA):  \n\u2003\u2022 The VCBA is a mechanism\u2010based computational model implemented in R that simulates chemical fate and cell response over time. It divides the in vitro test system into interconnected compartments (for example, cell, medium, headspace, plastic, and supplemented proteins/lipids) and uses differential equations to model chemical partitioning, evaporation, degradation, and cell uptake.  \n\u2003\u2022 The model uses input physicochemical parameters (e.g. logKow, Henry\u2019s Law Constant, molar and diffusion volumes) together with preset experimental conditions (cell number, well geometry, serum levels, and assay duration) to predict (i) the free, or \u201cdissolved,\u201d concentration available to the cells and (ii) the toxicodynamic response through two optimized parameters: the No\u2010Effect Concentration (NEC) and killing rate (Kr).  \n\u2003\u2022 The simulation is solved in a time\u2010discrete manner (with integration periods typically set to seconds to capture fast chemical distribution) without requiring any physical sample homogenization.  \n\u2003\u2022 Although the VCBA in this study was applied only to viability endpoints (as derived from the NRU assay), in general if imaging endpoints (for example, morphological features or neurite lengths) are used these are obtained via image\u2010analysis algorithms (often from automated microscopy) where a statistically representative number of cells \u2013 typically on the order of 100\u2013200 per well \u2013 are quantified.  \n\nTogether, the experimental (NRU assay) and the in silico (VCBA) approaches provide a coherent view of both the chemical\u2019s cell\u2010exposure (via simulated partitioning and bioavailability) and its effect on cell viability. This combined strategy helps reviewers/regulators understand both the biological measurement and the modelling steps without going into exhaustive technical detail.\n\n(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 41,
          "question_text": "Does the test system have a particular apoptosis sensitivity or resistance? Is cytotoxicity hard to capture for minor cellular subpopulations? In multicellular systems, which cell population is the most sensitive? Are specific markers known for each cell population? Are there issues with distinguishing slowed proliferation from cell death? For repeated/prolonged dosing: Is early death and compensatory growth considered? For very short-term endpoints (e.g. electrophysiology measured 30 min after toxicant exposure): Is a delayed measure of cytotoxicity provided?",
          "answer_text": "The VCBA and the linked in\u2010vitro assay (a 3T3 BALB/c Neutral Red Uptake test) do not \u201cdiscriminate\u201d apoptotic processes or distinct cell subpopulations. Instead, they model the overall cell population\u2019s growth and viability as an integrated outcome of both cell division and death. In other words, the current model does not include a specific mechanism that distinguishes clearly between apoptosis versus, for example, slowed proliferation, nor does it provide separate markers for minor subpopulations or for different cell types in a multicellular context. (Source: application of the virtual cell based assay for simulation of in vitro chemical fate following Acute Exposure, sections describing the cell growth and toxicity models and discussion of assumptions)\n\nFurthermore, because the test is implemented in a relatively homogeneous cell line (3T3 BALB/c fibroblasts), it does not address whether a particular cellular subpopulation is uniquely sensitive or resistant. For repeated or prolonged dosing, the model does not explicitly simulate an early death phase followed by compensatory growth. Likewise, when very short\u2010term endpoints (such as electrophysiological measurements 30 minutes post\u2010exposure) are considered, there is no provision in the model to supply a delayed measure of cytotoxicity. Essentially, the model is set up for standard in vitro end points (typically measured over 24\u201348 hours) and is not designed to resolve rapid, transient events or the dynamics of minor subpopulations. (Source: application of the virtual cell based assay for simulation of in vitro chemical fate following Acute Exposure, sections 3.5\u20133.8 and 4.6)\n\nIn summary:\n\u2013 The VCBA does not incorporate a specific apoptosis mechanism nor does it distinguish subpopulation responses.\n\u2013 In multicellular systems, no analysis is provided to rank sensitivity among different cell types; here, only the overall cell population (a homogeneous 3T3 culture) is examined.\n\u2013 There are no specific markers for individual subpopulations.\n\u2013 The model\u2019s viability endpoint conflates slowed proliferation with cell death, which can pose interpretation challenges.\n\u2013 For repeated or prolonged dosing, the model does not simulate early death with subsequent compensatory growth.\n\u2013 For very short\u2010term endpoints (e.g. 30\u2005min post\u2010exposure), the model does not provide a delayed measure to capture emerging cytotoxicity.\n\nThus, while the VCBA offers valuable insights into overall chemical kinetics and toxicity in vitro, it does not resolve these finer mechanistic or temporal distinctions."
        },
        {
          "question_id": 42,
          "question_text": "Which rule do you apply to test whether a test run is within the normal performance frame? How do you document this decision? Indicate actions if the AC are not met.",
          "answer_text": "The rule we use is to compare the simulated assay\u2010output (the concentration\u2013response curve, including the control response at 0 \u00b5M, and the optimized NEC and Kr values) against a set of predetermined acceptance criteria (AC) that define \u201cnormal performance\u201d for the assay. In other words, the simulation run is only accepted if the predicted viability curve (and corresponding kinetic endpoints) geometrically and statistically fits the experimental response (for example, with error metrics within an acceptable range and the control remaining at 100% viability). (Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf, sections 3.6 \u201cOptimization and running the VCBA\u201d and Table 7 \u201cExperimental set up\u201d)\n\nThis decision is documented by archiving the run\u2010results along with the optimized parameters (NEC and Kr), the input conditions (cell numbers, serum level, volume, etc.) as well as the performance error assessment. In practice, the output from the KNIME workflow (or corresponding lab notebook entries/log files) shows the comparison of the simulated viability curve with the experimental data. These records serve as a traceable documentation that the assay run is within the expected performance frame.\n\nIf the acceptance criteria are not met\u2014that is, if the simulated concentration\u2013response deviates beyond the acceptable error range (for example, if the control does not remain at 100% viability or if the optimized kinetic parameters fall outside the expected range)\u2014the run is flagged. In such a case the following actions are taken:\n\u2022\u2003An immediate investigation is triggered to review the experimental inputs (cell seeding density, media/serum supplementation, etc.) and the model parameters.\n\u2022\u2003The run is either repeated (after correcting any identified discrepancies) or additional studies (such as a re\u2010calibration of the model/parameters) are conducted.\n\u2022\u2003All deviations are documented in the assay log and communicated to the responsible analyst for further action.\n\nThis structured approach ensures that only runs which meet the pre\u2010defined performance criteria are accepted, and any deviations are promptly addressed and fully traceable through the documentation."
        },
        {
          "question_id": 43,
          "question_text": "Indicate 'real data points per month' (not per week/per quarter, etc.): count three working weeks per month. Each concentration is a data point. Necessary controls that are required for calibration and for acceptability criteria are NOT counted as data points. All technical replicates of one condition are counted as one single data point (see notes for explanation) Indicate possibility/extent of repeated measures (over time) from same dish. Explain your estimate.",
          "answer_text": "Based on our interpretation of the experimental design:\n\n\u2022 In a typical run (a \u201cworking week\u201d) the assay is performed once (excluding controls) where only one measurement per concentration is taken (with technical replicates combined into a single data point). For example, if eight different chemical concentrations are tested in one full experiment (as in the NRU assay on 3T3 BALB/c cells), then that assay yields eight data points.\n\n\u2022 Assuming you plan for three working weeks per month (one experiment per week), you would obtain 3 \u00d7 8 = 24 \u201creal\u201d data points per month.\n\n\u2022 With regard to repeated measures over time from the same dish, the standard Neutral Red Uptake protocol is terminal (the endpoint is measured after 48 hours). This means that for each dish only a single time\u2010point (the final readout) is available, so the design does not support non-destructive repeated measures from the same dish.\n\nThis estimate follows the instructions that each concentration counts as one data point (excluding controls) and that technical replicates are aggregated to one point. It also reflects the fact that\u2014given the assay is destructive\u2014only one measurement per dish (at 48 hours) is obtained."
        },
        {
          "question_id": 47,
          "question_text": "How is the concentration range of test compounds defined (e.g. only single concentrations, always 1:10 serial dilutions or variable dilution factors, ten different concentrations, etc.)? Is there a rule for defining starting dilutions? For functional endpoints that may not provide full concentration-response, how is the test concentration defined? E.g. EC10 of viability data are usually tested for gene expression endpoints. Detailed information should also be included in an SOP, preferably in DB-ALM format (see link in 6.6).",
          "answer_text": "The assay is designed to work with a concentration\u2010response curve where test compounds are applied over a broad range of concentrations. For instance, in the work described, the protocol used in the Neutral Red Uptake assay generally provided data for eight test concentrations plus a control (0\u202f\u00b5M, with 100% viability) applied in a 96\u2010well microplate format (see \u201cExperimental set up\u201d and \u201cOptimization and running the VCBA\u201d sections). Although many in vitro protocols use serial dilutions\u2014for example, 1:10 dilutions from a starting concentration\u2014the exact dilution factor is not fixed universally. Instead, the starting dilution and dilution scheme are guided by preliminary range\u2010finding to ensure that the range spans from non\u2010toxic to overtly toxic doses based on compound solubility and expected potency.\n\nFor functional endpoints that do not yield a full concentration\u2013response (such as some gene expression endpoints), the test concentration is typically defined based on an effective concentration (for example, the EC10 derived from viability assays) to ensure that only mild effects are induced while preserving cell function. This approach minimizes potential confounding by overt cytotoxicity while still triggering the functional response.\n\nAll these details\u2014how the concentration range is chosen, what dilution factors are used, and the criteria for selecting test concentrations for endpoints that do not show full concentration\u2013response\u2014should be standardized and documented in the related SOP. In our recommendations, the SOP should ideally follow a format such as the DB\u2011ALM format (see section 6.6 for the link and guidelines) so that this working procedure is transparent and reproducible.\n\n(Sources: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 48,
          "question_text": "What types of compounds are problematic, e.g. interference with analytical endpoint, low solubility, precipitation of medium components, etc.? What experimental variables are hard to control (e.g. because they are fluorescent)? What are critical handling steps during the execution of the assay? Robustness issues, e.g. known variations of test performance due to operator training, season, use of certain consumable or unknown causes, etc. Describe known pitfalls (or potential operator mistakes).",
          "answer_text": "The report indicates that compounds with extreme physicochemical properties tend to be problematic. For instance:\n\n\u2022 Highly lipophilic chemicals (those with high logKow, especially in \u201cClass A\u201d and \u201cB\u201d as defined in the report) are difficult because they bind extensively to serum proteins, lipids, and even the plastic of the culture vessel. This \u201csequestration\u201d may cause the dissolved (i.e. bioavailable) concentration to be very low, which in turn can interfere with the interpretation of an analytical endpoint such as a viability assay (Sections 4.1 and 4.3).\n\n\u2022 Volatile compounds with high Henry\u2019s Law Constants (for example, benzene and xylene) are problematic because a significant fraction evaporates during the assay. In practice, if evaporation exceeds 10% of the applied dose, this may lead not only to loss of compound but also to alterations of the observed dose\u2013response relationship (Section 4.1).\n\n\u2022 Chemicals that have very low solubility may precipitate or form aggregates, while compounds that precipitate medium components can alter the matrix; both phenomena lead to inaccurate exposure estimation. Although precipitation per se is not described in detail, the findings on binding and partitioning highlight that low solubility can complicate the assay (Sections 4.1 and 4.2).\n\nRegarding experimental variables and handling:\n\n\u2022 Some variables are hard to control. For example, the serum supplementation level has a strong impact on chemical partitioning. Even a change from 0 to 5 or 10% serum markedly alters the balance between dissolved and bound compound. In addition, the temperature can be critical: the Henry\u2019s Law Constant is usually based on values at 25\u202f\u00b0C, while the assays are performed at 37\u202f\u00b0C. This mismatch introduces uncertainty in predicting evaporation (Sections 3.7, 4.2 and 4.6).\n\n\u2022 Operator handling is critical. Key steps include ensuring correct cell seeding densities (to avoid growth inhibition by confluence), precise control of the volume and composition of the medium (especially serum, which directly affects binding), and careful pipetting to avoid introduction of air bubbles or contamination. Inaccurate handling could lead to misestimation of the available free concentration if, for example, the cells release lipids on cell death and these are not accounted for consistently (Section 3.5.1 and Table 7).\n\n\u2022 Robustness issues stem from multiple sources. Variations due to operator training (such as subtle differences in cell seeding, handling of the plastic ware, and timing) or consumable differences (e.g. batch-to-batch variation in serum or plastic binding properties) can alter test performance. There is also an inherent uncertainty in the QSAR\u2010based predictions for partitioning parameters (such as those used for lipid, protein, and plastic binding). The model does not account for saturation and may be less robust when used with compounds that interact differently (e.g. polar or charged chemicals) compared to the neutral compounds used to build the QSARs (Section 4.6).\n\n\u2022 Known pitfalls or potential operator mistakes include:\n\u2003 \u2013 Misinterpreting results when a high degree of chemical partitioning (or evaporation) causes the nominal concentration (the dose applied) to differ markedly from the dissolved (bioavailable) concentration.\n\u2003 \u2013 Using plates or consumables without accounting for variability in plastic binding.\n\u2003 \u2013 Failing to adjust cell seeding densities properly; for example, if too many cells are plated the culture may reach confluence, affecting kinetics and viability assessments (Sections 3.5 and 3.6).\n\u2003 \u2013 Not controlling environmental variables (such as temperature) so that parameters like evaporation are not properly represented.\n\u2003 \u2013 Relying on predictions that were derived for one set of chemical classes but using them for compounds outside that domain (e.g. inorganic or metallo\u2010organic chemicals).\n\nIn summary, chemicals with extreme lipophilicity, volatility, or low solubility pose analytical and interpretational challenges in the assay. In parallel, critical experimental variables\u2014such as serum concentration, temperature, and cell density\u2014must be tightly controlled, and operator handling (from cell seeding to reagent preparation) must be consistent. Variability in consumables or slight differences in assay execution may lead to robust issues and contribute to pitfalls in data interpretation. \n\n(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 49,
          "question_text": "Ideally the SOP follows the DB-ALM or a comparable format: https://ecvam-dbalm.jrc.ec.europa.eu/home/contribute Refer to additional file(s) (containing information covered in sections 3 and 4), containing all details and explanations. Has the SOP been deposited in an accessible data base? Has the SOP been reviewed externally and if yes, how?",
          "answer_text": "The technical report fully describes the VCBA methodology\u2014including all details on model equations, parameters, code harmonization, and experimental setup (Sections 3 and 4, plus the annexes provide further explanations). However, the report does not explicitly state that an SOP document (in a DB\u2010ALM or similar format) has been deposited in an external, searchable database. Instead, all the methodological details (acting as the SOP) have been published in this open\u2010access technical report and are available via the JRC publication channels (including the EU Bookshop).\n\nFurthermore, while the report does not mention a formal external SOP\u2010review process in a regulatory database, external input was obtained\u2014acknowledged through discussions with experts such as Dr. Anthony Williams (EPA, USA) and Dr. Nynke Kramer (IRAS, Utrecht)\u2014which contributed to the code refinement and overall approach.\n\n(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 50,
          "question_text": "Does the method require specialized instrumentation that is not found in standard laboratories? Is there a need for custom-made instrumentation or material? Is there a need for equipment that is not commercially available (anymore)?",
          "answer_text": "The method does not require any non\u2010standard instrumentation. It is based on a conventional in vitro assay setup (e.g., the Neutral Red Uptake assay in a 96\u2010well plate) that uses standard cell culture equipment, and the additional component\u2014the VCBA\u2014is implemented as computer code (in R and via KNIME), which runs on a typical computer system. There is no indication that any custom\u2010made instrumentation or non\u2010commercial equipment is needed. \n\n(Source: application of the Virtual Cell Based Assay for Simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 51,
          "question_text": "Describe possible variations, modifications and extensions of the test method: a) other endpoints, b) other analytical methods for same endpoint, c) other exposure schemes (e.g. repeated exposure, prolonged exposure, etc.), d) experimental variations (e.g. use of a specific medium, presence of an inhibitor or substrate that affects test outcome, etc.)",
          "answer_text": "Based on the information presented in the report, the VCBA\u2010based in vitro test system can be varied and modified in several ways:\n\na) Other endpoints:  \n\u2022 Although the current application focuses on cytotoxicity (using the Neutral Red Uptake assay, among others) as the endpoint (see sections on \u201cResults and discussion\u201d), additional endpoints could be measured such as rates of cell metabolic activity, apoptosis markers, ATP levels, or even specific biomarkers of organ toxicity.  \n\u2022 Moreover, dosimetric endpoints different from the \u201ctoxic dose\u201d (i.e. the dissolved concentration after 24\u201348 hours) have been mentioned, such as the maximum concentration (C\u2098\u2090\u2093) and the area under the concentration\u2013time curve (AUC), which could be explored in future studies.  \n_(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)_\n\nb) Other analytical methods for the same endpoint:  \n\u2022 While the test method described uses the Neutral Red Uptake assay for cell viability, alternative analytical methods might include MTT assays, Alamar Blue/Resazurin assays, lactate dehydrogenase (LDH) release assays, or direct quantification of ATP levels to assess cell viability.  \n\u2022 Complementary methods such as flow cytometry or high content imaging could also be applied to obtain more detailed insights into cell cycle changes, apoptosis, or intracellular concentration changes.  \n_(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)_\n\nc) Other exposure schemes (e.g. repeated exposure, prolonged exposure, etc.):  \n\u2022 Although the current setup is designed for a single acute exposure (48-hour assay), the method could be adapted for repeated or prolonged exposures to simulate chronic exposure conditions or accumulation effects.  \n\u2022 Adjusting exposure times and dosing schemes (for instance, by modifying the time intervals or the number of exposures) could provide information on how repeatedly applied chemicals affect cell viability or partitioning over time.  \n\u2022 The framework of the VCBA is flexible enough to simulate different exposure scenarios provided that appropriate kinetic modifications are made in the mathematical model.  \n_(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)_\n\nd) Experimental variations (e.g. use of a specific medium, inclusion of an inhibitor/substrate, etc.):  \n\u2022 The experimental setup already discusses the influence of supplemented serum (with comparisons at 0%, 5%, and 10%) on chemical partitioning; similarly, changing the composition of the medium (protein and lipid content) or using alternative cell culture media could affect the fate of the tested chemicals.  \n\u2022 The model could be extended by including additives such as inhibitors or substrates that modify biochemical pathways (for example, affecting active transport or metabolism) or that alter cell membrane properties\u2014though active transport is not currently incorporated.  \n\u2022 Other variables, such as using alternative plasticware (given the documented plastic binding) or adjusting incubation temperatures (which would affect parameters like the Henry law constant), could provide further insight into test variability and potentially refine the in vitro\u2013in vivo extrapolation.  \n_(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)_\n\nOverall, these modifications and extensions would allow the test method to be tailored to investigate additional toxicological mechanisms, improve the comparability with in vivo conditions, and potentially broaden both its applicability and predictive value."
        },
        {
          "question_id": 52,
          "question_text": "Indicate the names (and database names) of related tests and give a short description (including a brief comment on differences to the present method). If the test method has been used for high throughput transcriptomics or deep sequencing as alternative endpoint, this should be indicated.",
          "answer_text": "The report makes reference to several other in vitro test\u2010approaches that, like the VCBA method, address both the measurement of toxicity and\u2014in many cases\u2014the fate of the chemical in the test system. For example:\n\n1. The classic Neutral Red Uptake (NRU) cytotoxicity assay using the BALB/c 3T3 cell line has been applied in several studies (e.g. in the ACuteTox project and ECVAM validation work). This assay, which is catalogued in databases such as the ECVAM/NICEATM documentation, simply quantifies cell viability after exposure but does not incorporate chemical distribution or kinetics. In contrast, the VCBA goes further by simulating partitioning (to medium, plastic, proteins, lipids, etc.) and cell uptake over time.\n\n2. A number of other in vitro kinetic models \u2013 as summarized in Table\u00a01 of the report \u2013 use different cell systems (for example, HepaRG, HepG2, HEK293, RTL-W1) and different exposure setups. Many of these are documented in curated collections such as the OECD QSAR Toolbox or in publicly available compilations (e.g. ToxCast/CompTox databases from the US EPA). These models focus on tracking the fate of a chemical (for instance, by considering transfer to headspace, binding to serum proteins, or binding to plastic) and sometimes include metabolic components when using metabolically competent cells. In comparison with the VCBA, these methods may differ in the number and types of compartments modeled; for instance, some include extracellular matrix proteins or specific metabolic clearance, while the VCBA is structured to address a generic kinetic fate in a 96\u2010well plate set\u2011up.\n\n3. Although the primary endpoint in the studies described in the report is cell viability (using methods such as the NRU assay), there is also emerging interest in complementing or replacing viability data with alternative endpoints such as high\u2011throughput transcriptomics. In a few instances (for example, using HepG2 cells), deep sequencing approaches (RNA\u2011Seq) have been applied to capture the changes in gene expression following chemical exposure. However, the present VCBA method in the report has not yet been coupled to a deep sequencing endpoint\u2013\u2013its strength lies in dynamically simulating the chemical\u2019s in vitro fate and its impact on cell number based on the NRU\u2010derived concentration\u2013response relationships.\n\n(Sources: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 53,
          "question_text": "What is the data format? Raw data: give general explanation. Upload an exemplary file of raw data (e.g. Excel file as exported out of plate reader). Provide an example of processed data at a level suitable for general display and comparison of conditions and across experiments and methods. If the file format is not proprietary or binary, include a template. This will help other users to provide their data in a similar way to the general data infrastructure. Example as used in EU-ToxRisk: Excel sheet with columns specifying line number, assay name, date of experiment, identifier for reference to partner lab book, compound, concentration (in: -log[M]), line number of corresponding control, number of replicates, endpoints, data of endpoint(s), etc.",
          "answer_text": "Below is an example answer that you might provide:\n\n\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\nThe raw data from a typical cell\u2010based assay (for example, a Neutral Red Uptake assay) is usually provided in a non\u2010proprietary tabular format such as an Excel (xlsx or csv) file. In its raw form the file contains one record per well or measurement and is organized by rows. Each row includes all the metadata associated with the measurement (for example, the experimental date, plate ID, well position, compound identity, nominal concentration expressed either in M or \u2013log[M], replicate number, and any control identifiers) along with the raw readout (for example, absorbance values from the plate reader).\n\nA typical raw data file might include columns such as:\n\u2003\u2022\u2003Row number (unique index)\n\u2003\u2022\u2003Plate ID and well position (for example, \u201cPlate1\u201d, \u201cA1/ B3\u201d, etc.)\n\u2003\u2022\u2003Assay name (e.g. \u201cNeutral Red Uptake\u201d)\n\u2003\u2022\u2003Date of experiment\n\u2003\u2022\u2003Reference identifier (laboratory or lab book number)\n\u2003\u2022\u2003Test compound name or code\n\u2003\u2022\u2003Nominal concentration (often given in \u2013log[M])\n\u2003\u2022\u2003Identifier for corresponding control (if applicable)\n\u2003\u2022\u2003Replicate number\n\u2003\u2022\u2003Raw endpoint readings (e.g. absorbance at a given wavelength or another quantitative endpoint)\n\u2003\u2022\u2003Other endpoints if measured (for example, additional cytotoxicity parameters)\n\nAn exemplary raw data template in Excel might have a header row as follows:\n\n\u2003\u2003Row | Plate_ID | Well | Assay_Name\u2003| Date\u2003\u2003  | Lab_Ref | Compound\u2003| Concentration(-log[M]) | Control_Line | Replicate | Raw_Reading\u2003| Other_Endpoint\n\u2003\u20031\u2003\u2003| Plate1   | A1   | NRU          | 2021-08-15 | LAB123  | Caffeine  | 4.00                  | 1            | 1         | 0.735       | n.a.\n\u2003\u20032\u2003\u2003| Plate1   | A2   | NRU          | 2021-08-15 | LAB123  | Caffeine  | 3.70                  | 1            | 2         | 0.752       | n.a.\n\u2003\u2003\u2026\u2003\u2003| \u2026        | \u2026    | \u2026            | \u2026          | \u2026       | \u2026         | \u2026                     | \u2026            | \u2026         | \u2026           | \u2026\n\nThis type of file is exported directly from a plate reader (or the associated software) and then complemented with metadata by the user.\n\nOnce raw data are collected they are processed to allow comparison across conditions, experiments, and even different methods. The processed dataset typically aggregates the replicate measurements and calculates summary descriptive statistics (such as mean and standard deviation) for each experimental condition. A processed data template might be organized by compound and condition\u2014for example:\n\n\u2003\u2003Experiment_ID | Assay_Name | Date\u2003\u2003\u2003\u2003| Compound    | Concentration(-log[M]) | Control_ID | Num_Replicates | Mean_Response (% Viability) | SD_Response\n\u2003\u2003EXP001\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| NRU        | 2021-08-15| Caffeine    | 4.00                   | 1          | 3              | 73.5                       | 2.3\n\u2003\u2003EXP001\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0| NRU        | 2021-08-15| Caffeine    | 3.70                   | 1          | 3              | 75.2                       | 3.1\n\u2003\u2003\u2026             | \u2026          | \u2026         | \u2026           | \u2026                      | \u2026          | \u2026              | \u2026                          | \u2026\n\nThis processed format is very similar to what has been used in the EU\u2010ToxRisk project. It provides not only a summary that allows the identification of trends across different compounds and concentrations but also facilitates comparison with other experimental methods or studies (for example by including a lab reference that may tie back to the raw data).\n\nBecause the file format is standard (Excel or csv) and non\u2010proprietary, a template such as the above can be shared among users. This ensures that when data are submitted into a central database or data infrastructure, the associated metadata (line number, assay name, date, lab reference, compound, concentration, control identifier, replicate number, measured endpoint data, etc.) are properly formatted and can be readily used to combine and compare data across experiments and methods.\n\n\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\n(Source: Based on the data structuring practices described in the Virtual Cell Based Assay report and as implemented in EU\u2010ToxRisk data infrastructure)"
        },
        {
          "question_id": 55,
          "question_text": "How are raw data processed to obtain summary data (e.g. EC50, BMC15, ratios, PoD, etc.) in your lab? Describe all processing steps from background correction (e.g. measurement of medium control) to normalization steps (e.g. if you relate treated samples to untreated controls).",
          "answer_text": "The \u201craw\u2010to\u2010summary\u201d data processing in our lab is carried out through a multi\u2010step procedure. First, the optical or luminescence readouts (for example, from a Neutral Red Uptake assay) are corrected for background by subtracting the signal measured in medium\u2010only (blank) wells. This step removes any non\u2010specific or baseline noise inherent in the assay setup. Then, the background\u2010corrected values are normalized by setting the untreated control (or vehicle control) value to 100% so that the response in treated samples is expressed as a percentage relative to the control. (Source: application of the Virtual Cell Based Assay for Simulation-KJNA28694ENN.pdf, see sections \u201cExperimental set up\u201d and \u201cOptimization and running the VCBA\u201d)\n\nOnce the data are normalized, concentration\u2013response curves are constructed using the data measured at multiple test concentrations (typically eight concentrations plus the control). These curves are then fitted using non\u2010linear regression (often with software such as GraphPad Prism). The fitted curve (typically a four\u2011parameter logistic model) provides key summary endpoints such as the EC50, benchmark concentration (e.g. BMC15), points of departure (PoD), or various ratios comparing treated to control responses. In the context of the Virtual Cell Based Assay (VCBA), these endpoints (or more specifically parameters like the no-effect concentration (NEC) and the killing rate (Kr)) are optimized against the experimental concentration\u2013response data. (Source: application of the Virtual Cell Based Assay for Simulation-KJNA28694ENN.pdf, sections \u201cOptimization and running the VCBA\u201d and \u201cStatistical analysis\u201d)\n\nIn summary, our processing workflow is as follows:\n1.\u2003Background correction by subtracting medium\u2010only (blank) values.\n2.\u2003Normalization of sample signals by setting the untreated (control) response to 100%.\n3.\u2003Curve fitting of the normalized concentration\u2013response data via non\u2010linear regression to derive summary measures (EC50, BMC15, PoD, etc.) and calculate ratios.\n4.\u2003Integration and optimization of these endpoints in the in silico VCBA framework (e.g. for NEC and Kr optimization).\n5.\u2003Final statistical analysis (often using GraphPad Prism) to verify the quality and reproducibility of the fitted parameters.\n\nThis systematic approach\u2014from background measurement to final curve fitting and optimization\u2014ensures that the summary data reflect the true bioactivity of the chemical under the test conditions."
        },
        {
          "question_id": 56,
          "question_text": "How are data normally handled to obtain the overall test result (e.g. concentration response fitting using model X, determination of EC50 by method Y, use of EC50 as final data)? How do you model your concentration response curve (e.g. LL.4 parameter fit) and which software do you use (e.g. GraphPad Prism, R, etc.)? Do you usually calculate an uncertainty measure of your summary data (e.g. a 95% confidence interval for the BMC or a BMCL), and with which software? Can you give uncertainty for non-cytotoxicity or no-effect? How do you handle non-monotonic curve shapes or other curve features that are hard to describe with the usual mathematical fit model?",
          "answer_text": "The VCBA workflow obtains a \u201ctest\u2010result\u201d by (1) fitting the experimental concentration\u2013response data (typically the cell viability data measured at a series of test concentrations, plus a 0\u2010concentration control) to a sigmoidal dose\u2013response model in order to derive summary values such as an IC50. In practice the measured data are first plotted and a mathematical fit (often a four\u2010parameter logistic model [LL.4]) is used to describe the concentration\u2013response curve. For instance, in our work the experimental concentration\u2013response curves (with eight concentrations over a 48\u2010hour exposure) were used to \u201coptimize\u201d two key VCBA parameters: the no\u2010effect concentration (NEC) and the killing rate (Kr). \n\nThis optimization is carried out in R (using, for example, the DeSolve package to solve the differential equations of the fate and cell growth models) while additional plotting and statistical analyses have been performed using GraphPad Prism 3.0. In effect, the IC50 (or other effective dose measures) derived from the fitted curve is used as the summary toxicological endpoint.\n\nConcerning uncertainty, typically a statistical software such as GraphPad Prism is used to generate confidence intervals (for example a 95% CI) for a fitted dose\u2013response curve (and hence for the IC50 or benchmark concentration when applicable). However, in our work explicit uncertainty measures for the in vitro summary values (e.g., reporting a 95% CI for the BMCL) are not systematically calculated, especially for endpoints where there is little or no cytotoxicity. In non\u2010cytotoxic or \u201cno\u2010effect\u201d cases the lack of a clear response makes it inherently difficult to derive confidence intervals; these instances are generally flagged qualitatively rather than with a precise statistical uncertainty.\n\nFinally, if a curve displays non\u2010monotonic behavior or other features that depart from the standard logistic shape, these deviations are handled by careful visual inspection and judicious interpretation. In such cases the usual LL.4 parameter fit may not fully capture the curve\u2010shape so that the user might need to explore alternative curves or explicitly note that the model fit is uncertain. Moreover, a sensitivity analysis (as illustrated in our work by evaluating the impact of \u00b110% variations of key input parameters) can help to understand how robust the summary endpoint is to changes in model parameters and can serve as an indirect measure of uncertainty.\n\n(Sources: application of the Virtual Cell Based Assay for Simulation of in vitro Chemical fate following Acute Exposure, Sections 3.6 \u201cOptimization and running the VCBA\u201d and 3.8 \u201cStatistical analysis\u201d; also the discussion in Sections 4.5 and 4.6.)"
        },
        {
          "question_id": 60,
          "question_text": "What is the scientific rationale to link test method data to a relevant in vivo adverse outcome? Which toxicological target (organ, tissue, physiological/biochemical function, etc.) is modelled? Which biological process(es) (e.g. neurite outgrowth, differentiation) are modelled/reflected by your test method? Which toxicological events (e.g. oxidative stress, cell death) are modelled/reflected by your test method? To which (human) adverse outcome(s) is your test method related? Which hazard(s) do(es) your test method (potentially) predict?",
          "answer_text": "The VCBA is built on the rationale that\u2014to improve the in vitro/in vivo extrapolation\u2014it is essential to simulate not just the nominal chemical concentration applied to cells but also its dynamic fate in the in vitro system. In other words, by accounting for chemical partitioning (between medium, proteins, lipids, plastic, and even headspace due to evaporation) and cellular uptake, the VCBA \u201cde\u2010convolutes\u201d the bioavailable dose actually experienced by the target cells. This bioavailable (dissolved) concentration is then linked to the key toxicological event being measured: a disruption in cell viability (i.e. cell death). \n\nMore specifically, the VCBA models:\n\u2022\u2003A toxicological target at the cellular level \u2013 in the examples presented, the 3T3 BALB/c cell line is used, representing a generic somatic cell population.\n\u2022\u2003Biological processes such as cell growth and division as well as cell death (integrated via a cell cycle model with defined phases and a \u201ckilling rate\u201d parameter [Kr]) that reflect the cell\u2019s response to chemical insult.\n\u2022\u2003The event of cell death as the primary toxicological response, which in vitro is quantified (e.g. via Neutral Red Uptake assay) and ultimately serves as a proxy for adverse outcomes in vivo.\n\u2022\u2003By simulating the fate of chemicals (their dissolution and binding, and even evaporation for volatile compounds) the assay relates these \u201ckinetic\u201d endpoints to adverse outcomes such as acute systemic toxicity (e.g. as measured by rat oral LD50) and by extension can provide support for hazard classification in humans.\n\nThus, although the VCBA does not explicitly simulate, for example, neurite outgrowth or differentiation, its focus is on modelling the toxicodynamic outcome of cell viability loss\u2014a fundamental perturbation (cell death) that is highly predictive of adverse in vivo outcomes such as tissue damage or systemic toxicity. In a regulatory context, the test method therefore predicts hazards related to acute cytotoxicity, which\u2014after appropriate kinetic extrapolation\u2014can be linked to adverse outcomes seen in humans (or animals) such as organ/system failure following acute exposures. \n\n(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 61,
          "question_text": "Provide the statistics of your benchmark response (threshold and variance): (i) For dichotomized data, provide your prediction model. When do you consider the result as toxic or not toxic? (ii) For pseudo-dichotomized outcomes (two classes with borderline class in between): define borderline range. (iii) For multi-class or continuous outcomes: provide definitions and rationale. What is the rationale for your threshold? This can be on a mathematical (e.g. 3-fold standard deviation) or a biological basis (e.g. below 80% viability). Is there a toxicological rationale for the threshold settings and definitions of your prediction model? What are the limitations of your prediction model? What is a \u2018hit\u2019 if the test is used in screening mode (= hit definition, if different from above)?",
          "answer_text": "Below is an example answer that summarizes how one could define benchmark responses and thresholds in the context of the VCBA approach described in the document. Note that the VCBA model itself was developed to simulate in vitro chemical fate and cytotoxicity endpoints (via rate\u2010parameters such as the no\u2013effect concentration [NEC] and killing rate [Kr]), and it is used both to derive a continuous dose\u2013response and, upon application of conversion algorithms, to classify chemicals relative to in vivo toxicity thresholds (for example, using the EU CLP cut\u2010offs such as 2000\u202fmg/kg). The answer below presents answers for three different outcome types and also discusses the rationale and limitations of the prediction model. (Sources: Sections 1\u20134, \u201cMethodology\u201d and \u201cResults and discussion\u201d in the document.)\n\n1. For dichotomized data  \n   \u2022 Prediction model and decision rule: A common approach in our VCBA framework is to convert the in vitro concentration\u2013response curve into an estimated IC50 (or further, an extrapolated in vivo LD50) and then dichotomize the outcome. For example, using the EU CLP system, if the in vitro\u2013derived extrapolated LD50 is below 2000\u202fmg/kg the chemical is considered toxic (\u201cclassified\u201d), whereas an LD50 above that threshold is counted as non\u2013toxic.  \n   \u2022 Benchmark response and variance: In many cytotoxicity assays (e.g. the Neutral Red Uptake), a threshold is set based on the effective concentration resulting in a 20% loss of cell viability (that is, survival remaining above ~80% of control) whereas the inherent variance from the replicate concentration\u2013response experiments (typically eight concentration points plus the control) is taken into account. For instance, one may require that the toxicity decision is only made when the measured response deviates by an amount greater than three times the standard error of the control group (a \u201c3\u2011sigma\u201d rule).  \n   \u2022 In short: the model \u201ccalls\u201d a positive (toxic) hit if the optimized parameters (NEC and Kr) yield a dissolved concentration below the threshold (or an extrapolated LD50 \u22642000\u202fmg/kg) with sufficient statistical separation (e.g. >3\u00d7 the assay variance).  \n\n2. For pseudo\u2013dichotomized outcomes  \n   \u2022 In some cases an intermediate (\u201cborderline\u201d) class is defined. For example, when cell viability falls within a range that is neither clearly \u201cnormal\u201d (for instance, >90% viability) nor markedly reduced (<80% viability), one may define a borderline response.  \n   \u2022 In our approach one could define the borderline range as the window between 80% and 90% viability. In other words, if the normalized viability obtained from the simulation (or the predicted dissolved IC50) falls within this interval, then the chemical is flagged as \u201cborderline\u201d (or uncertain), prompting further testing or additional analyses.  \n   \u2022 The choice of these limits is based partly on the observed variance in control responses and partly on the biological relevance\u2014that is, under typical experimental conditions the cell systems start to show cytotoxic effects when viability drops below 80%.  \n\n3. For multi\u2013class or continuous outcomes  \n   \u2022 In a setting where multiple (or continuous) toxicity outcomes are predicted (for example, using the full continuous concentration\u2013response curve from the VCBA) one can define thresholds on either a mathematical basis or a biological one. One possibility is to set a benchmark response in terms of a fold\u2013change relative to control (for instance, defining \u201ctoxicity\u201d as any response lower than 80% relative viability, or by adopting a criterion such as a three\u2013fold standard deviation difference from the control mean).  \n   \u2022 The rationale for such thresholds can be both mathematical (e.g. using 3\u00d7 the standard deviation of the assay noise to ensure that the change is statistically significant) and biological (because cell systems typically maintain viability above 80% in the absence of cytotoxicity). For example, the NEC in the VCBA is adjusted by fitting the full dose\u2013response curve; a sharp drop in viability from 100% toward below 80% is considered biologically significant.  \n   \u2022 In a multi\u2013class setting (for example, the alert system based on logKow described in the report where chemicals are grouped into Classes A, B, C, and D based on predicted dissolved fractions), thresholds are defined by the predicted partition of the chemical. There, the rationale is that chemicals with extremely high lipophilicity (logKow between 5 and 10) tend to be sequestered in lipid/protein/plastic with very little free dissolved fraction, and hence the in vitro concentration\u2013response (and thus the potential toxicity) requires special attention during extrapolation.  \n\nAdditional Points Regarding Rationale, Limitations, and Hit Definition  \n   \u2022 Rationale:  \n     \u2013 The toxicity threshold is set on the basis of both assay reproducibility and biological relevance. For example, a 20% loss in viability (i.e. cell survival below 80%) is a common toxicological cutoff in cytotoxicity assays and is further backed up by statistical criteria (e.g. a 3\u2013fold standard deviation of control responses).  \n     \u2013 Similarly, in the in vitro\u2013in vivo extrapolation, the 2000\u202fmg/kg LD50 (non\u2013classified according to EU CLP) is used as the cut\u2013off.  \n   \u2022 Limitations:  \n     \u2013 A main limitation is the generic nature of the VCBA model. It does not discriminate among specific chemical classes (e.g. polar vs. non\u2013polar, inorganic vs. organic compounds) and does not include aspects such as metabolism, active transport, or saturation kinetics.  \n     \u2013 There is uncertainty in some key physicochemical parameters (e.g. Henry Law Constant may be temperature\u2013dependent but is used as determined at 25\u00b0C while experiments are performed at 37\u00b0C).  \n     \u2013 The conversion from nominal (or predicted dissolved) concentration to an in vivo dose relies on assumptions that may not hold for very lipophilic or highly volatile substances.  \n   \u2022 \u201cHit\u201d Definition in Screening Mode:  \n     \u2013 When used in a screening context, a \u201chit\u201d is typically defined as any chemical for which the predicted bioavailable concentration (e.g. the simulated dissolved IC50) falls below the set threshold and thereby indicates potential cytotoxicity.  \n     \u2013 In screening, one might use a flagging criterion whereby a compound is considered a hit if its measured effect (or its predicted effect from the VCBA) is statistically significantly different from control (e.g. below 80% viability with a response difference greater than 3\u00d7 the standard deviation) or if its extrapolated in vivo dose is in the toxic range (<2000\u202fmg/kg). This hit definition might be slightly more conservative than the thresholds used in definitive toxicity testing to ensure that potential toxicants are not missed.  \n\nIn summary, the VCBA benchmark or \u201cresponse threshold\u201d can be defined in several ways:\n   \u2013 For dichotomous classification, one may set a threshold such that a chemical is classified as toxic if the predicted dissolved concentration (reflected in an extrapolated IC50) yields an in vivo dose below 2000\u202fmg/kg or if cell viability drops below 80% (with appropriate statistical confidence).\n   \u2013 For borderline outcomes, a \u201cgray zone\u201d (for example, 80%\u201390% viability) is defined.\n   \u2013 For continuous outcomes, thresholds are set either by a mathematical rule (e.g. 3\u00d7 standard deviation or fold\u2013change criteria) or by a biologically based rationale (such as maintenance of >80% viability).\nThe decision rules and thresholds are motivated by both the observed assay variability and by common toxicological practice. However, limitations of the VCBA include its assumptions of passive diffusion, lack of metabolism, uncertainties in physicochemical predictions, and the absence of saturation\u2010kinetics for binding interactions. In screening applications, a \u201chit\u201d is therefore defined as a compound with a predicted response (dissolved concentration/viability endpoint) meeting or exceeding the toxic threshold (e.g. <80% viability or equivalent extrapolated dose), with the caveat that borderline compounds may need further investigation. (Sources: Sections 3.6, 3.8 and 4.4 of the provided document.)"
        },
        {
          "question_id": 62,
          "question_text": "How was the prediction model set up (using which test set of chemicals to train the model; using probing with what kind of classifiers/statistical approaches)? Has the prediction model been tested (what was the test set of chemicals)? List chemicals or give n, if n > 50. Is the process documented (publication)? Does the prediction model (PM) apply to changes to both sides of controls (up/down)? If the PM is one-sided (e.g. toxicants leading to a decrease vs. control), how are data in the opposite direction handled and interpreted? If the PM is two-sided, do different rules, characteristics and interpretations apply to the two sides (e.g. is a decrease in viability or an increase in viability both interpreted as an effect/toxicity; are thresholds and performance characteristics to both sides the same?).",
          "answer_text": "The report does not describe a \u201cblack\u2010box\u201d classifier that was built using, say, machine\u2010learning on a large training \u201cset\u201d of chemicals with independent positive and negative outcomes. Instead, several physicochemical properties (such as logKow and Henry Law Constant, among others) were obtained from experimental data and/or predicted by QSAR\u2010based approaches. For example, experimental logKow values were gathered for about 109 compounds and then compared with values predicted by several tools (ACD/Labs, EPI-Suite, Chemicalize, and Chemical Dashboard), with trend lines and correlation coefficients computed to assess performance. Similarly, for Henry Law Constant, experimental data for 57 compounds and predictions (using approaches such as the Group Method, Bond Method, and HENRYWIN\u2122) were compared statistically (see Sections 3.3 and 3.4 and Table 4 and Table 5 in the document). \n\nIn parallel, the overall Virtual Cell Based Assay (VCBA) model was \u201ctrained\u201d or rather optimized against experimental in vitro concentration\u2013response curves from the Neutral Red Uptake cytotoxicity assay. Out of the initial 178 compounds, 83 were run in the VCBA to optimize two kinetic/toxicity parameters (NEC and killing rate, Kr) using the experimental data. (Sources: Sections 3.3\u20133.5; Abstract and Results)\n\nRegarding documentation, the entire process\u2014including the collection and curation of physicochemical data, the derivation of QSAR correlations, the optimization of NEC and Kr, and the statistical evaluation of predictions\u2014is fully documented in this \u201cEUR 28694 EN\u201d technical report (and further discussed in Zaldivar et al.21).\n\nConcerning the \u201csidedness\u201d of the prediction model:  \n\u2022 The toxicity/effects model embedded within the VCBA is based on merging the experimental concentration\u2013response curve (derived from a neutral red uptake assay) with a cell growth and division model to determine NEC and Kr. Because the assay measures cytotoxicity (a decrease in viability relative to the control), the approach is essentially one-sided (i.e. it predicts decreases in cell number/viability as toxic).  \n\u2022 There is no discussion in the report of how upward deviations (an increase in viability) would be processed or interpreted. In other words, the model is not set up with separate rules, thresholds or performance characteristics for \u201cup\u2010regulation\u201d of viability. The effects are assumed to be adverse when cell viability decreases; any opposite (or stimulatory) effects are not part of this toxicity prediction model. (Source: Sections 3.4; 4.4; and Conclusions)\n\nThus, to summarize:  \n\u2013 The \u201cprediction model\u201d for physicochemical parameters was built by gathering experimental data (using sets of ~109 compounds for logKow and 57 for Henry Law Constant) and then statistically comparing these to QSAR\u2010based predictions. No complex classification algorithm or machine\u2010learning classifier was used; rather, linear regression and correlation analysis were applied.  \n\u2013 The overall VCBA training (i.e. optimization of NEC and Kr) used 83 compounds from in vitro NRU toxicity data.  \n\u2013 The entire process is documented in this technical report (published as EUR 28694 EN, with further details in Zaldivar et al.21).  \n\u2013 The toxicity prediction is effectively one\u2013sided (focused on decreases in viability); the model does not provide a separate treatment for increases in viability, so only decreases are interpreted as toxic effects.\n\nAny data that would show an increase in viability are outside of the model\u2019s domain of application."
        },
        {
          "question_id": 64,
          "question_text": "Describe parameters important for the determination of free compound concentrations in the medium. Indicate the lipid and protein content of the medium and the cells. Indicate the volume of the cells. Indicate volume (medium volume) and surface area of culture dish. Is there information/literature on IVIVE strategies/data in the test? Has the test been used earlier for IVIVE? Are there special considerations that are relevant for IVIVE (e.g. potential for compound accumulation due to frequent medium changes and compound re-addition, glycoprotein (MDR1) expression, capacity for xenobiotic metabolism of test system)?",
          "answer_text": "Below is a summary of the most relevant aspects from the report:\n\n1. Determinants of Free Compound Concentration in Medium  \n\u2003\u2022 The model (VCBA) uses physicochemical parameters (e.g. logKow, Henry\u2019s law constant, molecular diffusion volume, and molar volume) together with system\u2010specific descriptors to estimate how a compound partitions among the medium, cells, headspace, plastic, and other binding components.  \n\u2003\u2022 The free (dissolved) concentration depends strongly on the extent of binding to added serum proteins and lipids as well as to plastic and cellular components.\n\n2. Lipid and Protein Content  \n\u2003\u2022 Medium: In the experimental setup using a 96\u2010well plate with 5% serum supplementation, the medium contains 0.0234 mol/m\u00b3 protein and 0.08 kg/m\u00b3 lipid (see Table 7).  \n\u2003\u2022 Cells: For the 3T3 BALB/c cells used, the defined cell composition is a protein fraction of 0.244 and a lipid fraction of 0.142. In addition, the intracellular protein concentration is taken as 4.4 mol/m\u00b3 and the lipid concentration as 170.7 kg/m\u00b3 (see Section 3.5.1 and Table 6).\n\n3. Cell Volume and Culture Dish Dimensions  \n\u2003\u2022 Cells: Individual cell volumes vary with cell cycle phase. For instance, the reported volumes are 1.73\u00d710\u207b\u00b9\u2075 m\u00b3 in the G1 phase and 2.4\u00d710\u207b\u00b9\u2075 m\u00b3 in the S, G2, and M phases (Table 6).  \n\u2003\u2022 Culture dish: The experiment uses a 96\u2010well plate with a medium volume of 1.0\u00d710\u207b\u2077 m\u00b3 per well, a cell assay surface area of 3.31\u00d710\u207b\u2075 m\u00b2, and a plastic surface area of 9.39\u00d710\u207b\u2075 m\u00b2 (see Table 7).\n\n4. In Vitro to In Vivo Extrapolation (IVIVE)  \n\u2003\u2022 The report discusses IVIVE strategies by comparing the VCBA\u2010predicted in vitro dissolved concentrations and derived IC50s with in vivo acute toxicity (rat oral LD50) data, as well as by integrating these concentrations into PBK models for extrapolation (see sections 1 and 4.4).  \n\u2003\u2022 Earlier applications include studies from the ACuteTox and related projects where the VCBA was used as part of the effort to bridge in vitro toxicity data with in vivo outcomes.\n\n5. Special Considerations Relevant for IVIVE  \n\u2003\u2022 The model simulates a single\u2010exposure scenario and does not explicitly account for repeated medium changes with compound re\u2010addition; hence, potential accumulation effects (e.g. on plastic or in the cell mass) are not modeled.  \n\u2003\u2022 The test system (3T3 BALB/c cells) is not metabolically competent and does not consider active transport or glycoprotein (MDR1) expression. Therefore, a lack of xenobiotic metabolism and transporter effects must be recognized when relating the in vitro free concentrations to the in vivo scenario.  \n\u2003\u2022 Because of the high impact of lipophilicity on chemical partitioning (and therefore on free concentration), caution is warranted when extrapolating in vitro endpoints to in vivo doses. The model indicates that highly lipophilic compounds tend to bind to proteins, lipids, or plastic surfaces, and may thus show significant differences between nominal and free concentrations.\n\n(Sources: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 65,
          "question_text": "Which compounds is the test likely to pick up correctly, where is it likely to fail? How does the test method react to mixtures and UVCBs? Are there areas (according to industry sector, compound chemistry, physical-chemical properties) that need to be excluded from testing, or that are particularly suitable? Which compound class cannot be detected (e.g. neurotransmitters for which the receptors are not expressed, endocrine disruptors in absence of respective pathway)? Are any compounds known to interfere with the test system (e.g. fluorescent or colored chemicals)?",
          "answer_text": "Based on the report, the VCBA\u2010derived cytotoxicity test tends to work best for single, relatively \u201cclean\u201d chemicals for which the physicochemical properties (especially logKow, Henry law constant, and degradation rates) are well defined and which act by non\u2013receptor\u2010mediated mechanisms (Source: application of the virtual cell based assay for simulation\u2010KJNA28694ENN.pdf, Sections 4.3\u20134.6). In practice this means that chemicals with moderate lipophilicity and low volatility (for example in Classes C/D by the alert system, where most of the chemical remains dissolved) are more likely to be picked up correctly. By contrast, the test may \u201cfail\u201d or yield misleading results for chemicals that are either highly lipophilic (Classes A/B) or very volatile (e.g. benzene and xylene), because these compounds bind predominantly to lipids, proteins, or the plastic or evaporate appreciably \u2013 leading to a very low dissolved (bioavailable) fraction (Source: Sections 4.1 and 4.3).\n\nRegarding mixtures and UVCBs (substances of unknown or variable composition, complex reaction products, or biological materials), the VCBA model and the underlying cell\u2010based assay are designed and parameterized for single, well\u2010characterized compounds. The report does not include a specific strategy for mixtures or UVCBs. In practice their complex and variable physicochemical properties cannot be reliably captured with the QSARs that underpin the model; therefore, such substances are not directly amenable and would have to be treated with caution or excluded (Source: Sections 4.6 and Conclusions).\n\nIn terms of suitability across industry sectors or compound types, the method is best suited for substances where absorption is controlled largely by passive diffusion\u2014that is, where metabolism, active transport, and receptor\u2010mediated signaling (including endocrine or neurotransmitter pathways) are not critical. In fact, the 3T3 BALB/c cell line used in the assay is not metabolically competent and does not express specialized receptors. Thus, compounds dependent on specific receptor pathways (for example neurotransmitters or endocrine disruptors) are unlikely to produce a meaningful response in this assay (Source: Section 4.6 and the discussion on cell uptake).\n\nFinally, the report does not explicitly list interfering compounds but does highlight that the test relies on the Neutral Red Uptake endpoint. As is known from in vitro protocols, chemicals that are inherently fluorescent, colored, or that absorb in the same spectral region as the dye may interfere with the assay read\u2010out. Such interference could result in either over\u2010 or underestimation of cytotoxic effects. Therefore, compounds with optical properties that could overlap with the Neutral Red signal must be considered potential sources of interference (Source: general properties of NRU assays as discussed in the context of assay limitations in Section 4.6).\n\nIn summary:\n\u2022 The method is likely to perform well with single compounds that are moderately lipophilic, nonvolatile, and do not require metabolic activation or receptor\u2010mediated effects.\n\u2022 It tends to fail (or require special care) for highly lipophilic compounds (with extensive binding) and for volatile chemicals.\n\u2022 Mixtures and UVCBs are not directly accommodated because the model depends on well\u2010defined physicochemical parameters.\n\u2022 Areas where the test is less predictive include compounds that act via specific receptor pathways (e.g. neurotransmitters or endocrine disruptors) or those needing metabolic activation.\n\u2022 Finally, compounds that are strongly colored or fluorescent may interfere with the optical endpoint of the Neutral Red Uptake assay.\n\nThese considerations indicate that users should take great care when extrapolating in vitro results from this system to in vivo contexts, and certain compound classes (or mixtures/UVCBs) may need to be excluded or further investigated using complementary approaches."
        },
        {
          "question_id": 66,
          "question_text": "Does the test fit into a test battery? If yes, into which test battery and are there any restrictions? Indicate potential strengths and weaknesses of the system in a test battery (e.g. method is a good confirmation assay, good for creating alerts, mechanistic follow-up, screening, etc.). Compare performance to similar tests. Which gaps in a known or potential battery does the test method fill? Should the test preferentially be used in the first tier or later tiers, are complementary assays required or is it a stand-alone method?",
          "answer_text": "The VCBA does indeed have a role within a broader testing battery. Based on the report, the model is designed to complement whole\u2010cell assays (such as the Neutral Red Uptake test) by providing kinetic \u201ccorrections\u201d that translate nominal concentrations into more biologically relevant, bioavailable concentrations. In essence, it fills an important gap by addressing chemical partitioning and fate within the in vitro system\u2014a common missing element in traditional cytotoxicity assays.\n\nPotential strengths include:\n\n\u2022 It can act as an early\u2010tier screening tool by flagging chemicals with high lipophilicity or volatility (using its built\u2011in logKow\u2013based alert system) that might otherwise produce misleading results if the nominal dose is used directly.  \n\u2022 It provides a mechanistic \u201cconfirmation\u201d by simulating how chemicals bind to cell components (e.g. proteins, lipids, plastics) and even predicting evaporation for volatile compounds.  \n\u2022 The tool is valuable for refining experimental conditions and providing dosimetric context, thereby helping to interpret or \u201ctranslate\u201d in vitro effect data toward in vivo outcomes.  \n\u2022 Its flexibility (implemented through KNIME and R) allows its use in different assay set\u2010ups.\n\nOn the other hand, weaknesses include:\n\n\u2022 It is generic by design and does not consider metabolism or possible saturation in binding; this may limit its predictive power for chemicals that undergo significant biotransformation or where binding sites become saturated.  \n\u2022 Some of the input parameters (e.g. Henry Law Constant) are based on predictions at a temperature lower than typical assay conditions (25 \u00b0C instead of 37 \u00b0C), which may lead to under\u2011 or over\u2011prediction of volatile losses.  \n\u2022 The QSARs upon which some of its partitioning equations are built were derived mainly from neutral organic compounds, so their applicability for polar, ionic, or inorganic chemicals may be limited.\n\nIn comparison with similar in vitro kinetic models, the VCBA stands out as a useful \u201cdosimetry bridge\u201d that explains discrepancies between nominal and effective concentrations. However, like many in vitro methods it is not fully stand\u2011alone. Rather, it is best used in conjunction with other assays\u2014ideally as part of an integrated testing strategy or battery where its output informs further mechanistic or confirmatory follow\u2010up. Its strengths make it particularly useful as a first\u2011tier tool (especially to generate alerts and to screen compounds that might need further investigation) with the understanding that complementary assays (for example, for metabolism) should be integrated in later tiers if more complete toxicodynamic assessment is required.\n\n(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 67,
          "question_text": "Refer to published literature on the test AND indicate in detail deviations from published descriptions (e.g. plastic plate supplier, cell number, endpoint measurement, timing, etc.). Provide the most relevant publications that describe/give a comprehensive overview of (a) your test system and/or (b) your test method. Describe what aspects are covered therein. Give a prioritized (according to importance) list of further publications on the test method or its application. Give short comments on which type(s) of information can be obtained from these publications (e.g. contains test chemical lists, contains more positive/negative controls, contains validation against other tests, contains incorporation in test battery, demonstrates use by other lab, etc.).",
          "answer_text": "Below is an answer that refers to the published literature noted in the report and comments on how our current test\u2010description and implementation compare with previously published descriptions:\n\n1. Published descriptions of the overall test system and method have been provided in several articles and reports. For example, Zaldivar et al. (see section 3.5 and Annex\u202f1 in the report) present the mathematical foundations of the Virtual Cell Based Assay (VCBA), detailing its four\u2010module architecture (fate and transport, cell partitioning, cell growth/division, and toxicodynamics). This publication gives an in\u2011depth description of the equations used to describe chemical partitioning\u2014including binding to serum proteins, lipids, plastic and the fate processes (evaporation and degradation)\u2014as well as the dynamic modeling of cell cycle phases.\n\n2. The test system itself \u2013 the 3T3 BALB/c in vitro assay using the Neutral Red Uptake (NRU) endpoint over a 48\u2011hour exposure \u2013 and its experimental setup (e.g. cell densities, culture well/microplate geometry, supplemented serum concentration, and plastic surface properties) are described in earlier validation studies from the ACuteTox project (see Prieto et al. 2013a and 2013b, referenced in sections 1 and 4). These publications include details on nominal concentration ranges, the use of standard 96\u2011well microplates (with defined cell assay and plastic surface areas as given in Table\u202f7), the initial cell number (set at 1680 cells per well in our simulation to avoid confluence) and the 48\u2011hour exposure period. They also provide test chemical lists and comparative analyses of in vitro IC50 data with in vivo LD50 values.\n\n3. With regard to deviations from the published descriptions:\n\u2003a. Plastic and serum effects \u2013 In our VCBA application the model explicitly includes partitioning to the plastic (using a plastic partition constant based on logKow) as well as binding to serum proteins and lipids. Unlike some earlier descriptions where saturation kinetics were not addressed, here the model acknowledges that a fixed percentage (for instance, the 5% serum supplementation supplying specific protein and lipid concentrations) may not capture saturation effects. In other words, the model uses a defined plastic surface area (9.39\u00d710\u20135 m\u00b2) and a cell assay surface area (3.31\u00d710\u20135 m\u00b2) that are numerically set for the 96\u2010well format.\n\u2003b. Cell number and timing \u2013 Our simulation uses an initial cell number (1680 cells/well) that is defined as a fraction of the total population to ensure cell growth over the 48\u2011h period without reaching confluence (see Table\u202f6). Also, the model \u201cdiscretizes\u201d the chemical\u2019s dynamic distribution on a per\u2011second basis within each hour and then updates cell growth at the end of each hour (section 3.6). This approach is somewhat different from published experimental protocols where continuous exposure and end\u2010point measurements (typically at 48\u202fh only) are reported.\n\u2003c. Endpoint measurement \u2013 Although the NRU assay is used as in published reports, our VCBA integrates both the nominal concentration and the predicted \u201cdissolved\u201d (or bioavailable) fraction; this dual report is not always found in standard NRU assay descriptions.\n\n4. Most relevant published publications giving a comprehensive overview include:\n\u2003\u2013 Zaldivar et al. (referenced as [21] in the report): Provides the complete theoretical and mathematical description of the VCBA modules (fate/transport, cell uptake and growth, toxicodynamics). It is key for understanding the simulation of chemical kinetics in vitro.  \n\u2003\u2013 Prieto et al. (2013a and 2013b, referenced in sections 1 and 4): These articles describe the application of the 3T3 BALB/c NRU assay in the ACuteTox context, supply comprehensive lists of test chemicals and detail the comparison between nominal and bioavailable concentrations as well as correlations with in vivo toxicity.\n\n5. A prioritized list of further publications on the test method or its application (with short comments on the type of information offered):  \n\u2003Priority 1: Zaldivar et al. (Toxicol. Vitr. 2016)  \n\u2003\u2003\u2022 Covers: Mathematical formalism of the full VCBA model (fate, uptake, toxicodynamics) and detailed equations used.  \n\u2003Priority 2: Prieto et al. (2013a, 2013b)  \n\u2003\u2003\u2022 Covers: Detailed description of the test system (3T3 NRU assay), experimental design, test chemical lists, and validation against in vivo toxicity data as part of the ACuteTox project.  \n\u2003Priority 3: Kramer et al. (e.g., Kramer et al. 2012, as cited in section 4.2)  \n\u2003\u2003\u2022 Covers: Empirical evidence for evaporation processes and serum\u2010dependent binding; provides experimental verification of factors (such as plastic binding and evaporation) that are modeled in the VCBA.  \n\u2003Priority 4: Heringa et al. (as mentioned around section 4)  \n\u2003\u2003\u2022 Covers: Reports on improved correlations when the free (dissolved) concentration is used instead of the nominal concentration, offering a basis for the importance of kinetic modeling in in vitro assays.\n\nEach of these publications contributes a different aspect: from the fundamental simulation tools and equations (Zaldivar et al.), to detailed experimental protocols and chemical lists (Prieto et al.), to comparative validations with in vivo data and additional mechanistic insights (Kramer et al. and Heringa et al.). Collectively, they provide background not only on the test system but also on how the method has been refined and applied to improve in vitro toxicity predictions.\n\n(Source: application of the Virtual Cell Based Assay for Simulation of in vitro Chemical fate following Acute Exposure \u2013 KJNA28694 EN)"
        },
        {
          "question_id": 68,
          "question_text": "Indicate whether the test method has been or could be linked to an AOP (or several AOPs) and in which form (e.g. test of KE activation). Can the test method cover an AOP MIE/KE? Reference relevant AOP and if in AOP-wiki, refer to status.",
          "answer_text": "The report does not explicitly describe the VCBA as being developed or validated as an assay for a molecular initiating event (MIE) or key event (KE) in an Adverse Outcome Pathway (AOP). Instead, the VCBA is presented as a generic, quantitative in vitro fate\u2010and\u2010transport model that links the nominal concentration to the \u201cbioavailable\u201d (dissolved) concentration and, via cell\u2010level responses (e.g. effects on viability in the NRU assay), to toxicity. \n\nIn this sense, while the assay is not structured as a test of a specific MIE or KE per se, the output\u2014namely the concentration\u2013response curve (which results from integrating in vitro toxicokinetics and cell viability data)\u2014could potentially be used as one Quantitative Key Event (QKE) within an AOP framework (for example in AOPs where cytotoxicity or cell membrane perturbation is a key event). For instance, an AOP that links disruption of cellular homeostasis (or narcosis) to adverse in vivo outcomes might use such kinetic and viability data as one of its quantitative anchors. \n\nThat said, the report does not refer to any specific AOP from AOP\u2011wiki (nor does it provide an AOP status note) and does not directly test or validate a defined molecular initiating event against an AOP. Rather, it provides a tool that can improve the interpretation of in vitro endpoints\u2014which might then be integrated into an AOP framework if further linked with specific molecular or cellular key events. \n\n(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 69,
          "question_text": "Indicate/summarize information on mechanistic validation, e.g. by omics approaches or by use of endpoint specific controls (MCC; section 5.5). Has it been explored in how far the system reflects human biology, signaling, tissue organization relevant to the form of toxicity to be assessed (e.g. nigrostriatal neurons should contain dopamine, liver tests relevant to cholestasis may need to contain bile canalicular structures, etc.)?",
          "answer_text": "The report does not include any mechanistic validation via omics approaches or endpoint\u2010specific controls that would verify the system\u2019s ability to recapitulate human signaling or tissue\u2010specific structures relevant to a given mode of toxicity. Instead, the focus is on simulating chemical disposition (e.g. partitioning, diffusion, evaporation, and cell\u2010growth/death dynamics) in the in vitro set\u2010up. There is no discussion, for example, of whether the cell system expresses markers (like dopamine in nigrostriatal neurons or bile canalicular structures in liver cells) that would be required to mechanistically validate its relevance to a specific toxicity pathway. \n\n(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)"
        },
        {
          "question_id": 70,
          "question_text": "Indicate/summarize activities for test qualification, pre-validation or validation. Indicate e.g. ring trials, full (pre-)validations. Give an overview of compounds or libraries that have been tested.",
          "answer_text": "The report describes pre\u2010validation activities for the VCBA that were performed by re\u2010evaluating and harmonizing the model code and then \u201coptimizing\u201d the two key toxicodynamic parameters (NEC and Kr) against experimental concentration\u2013response data generated in previous validation studies. In particular, in vitro data from a 3T3 BALB/c Neutral Red Uptake cytotoxicity assay \u2013 which had been used in several international and regulatory\u2010oriented projects (e.g. the NICEATM/ECVAM study, the PF6 EU ACuteTox project and subsequent ECVAM studies by Prieto et al.) \u2013 were used as benchmark data. \n\nAn initial library of 178 chemicals (covering a wide range of physicochemical properties and lipophilicity, as indicated by logKow values) was assembled from these studies. However, due to missing input parameters (e.g. Henry Law Constant, air/water degradation data) or unsuitable concentration\u2013response profiles, a subset of 83 compounds was ultimately used to optimize and run the VCBA simulations. In addition, as part of the qualification exercise, an alert system based on logKow was developed to further rank and select compounds (by assigning them to classes A\u2013D) and a further set of 35 chemicals (with varying logKow values) was identified for future testing. \n\nNo formal ring\u2010trial is explicitly described in the document; however, the validation/pre\u2010validation activities included a full re\u2010assessment and optimization of the VCBA model against previously generated in vitro data from several validation studies, and the results have been proposed for follow\u2010up testing with selected compounds to further support the regulatory use of the method. \n\n(Sources: application of the virtual cell based assay for simulation-KJNA28694EN.pdf)"
        },
        {
          "question_id": 71,
          "question_text": "Indicate whether the test method is linked to an OECD Test Guideline (how, and which) or other regulatory guidance (e.g. EMA).",
          "answer_text": "The report does not indicate that the assay (or its underlying 3T3 Neutral Red Uptake cytotoxicity test used in this work) has been formalized as an OECD Test Guideline. Instead, it is presented as an in vitro model applied in regulatory research (e.g. within projects such as ACuteTox and under the REACH framework) and its QSAR components (for example for predicting LogKow) have been developed in compliance with OECD principles for QSAR model validation. In short, while the approach is aligned with regulatory expectations and uses tools that meet OECD principles, the test method itself is not directly linked to or described in an OECD Test Guideline or another specific regulatory test guideline (such as those from EMA).  \n_(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)_"
        },
        {
          "question_id": 76,
          "question_text": "Are special permits (e.g. genetic work, stem cells, radioactivity, etc.) required? Are special facilities required? Is special ethical approval necessary (indicate approval document).",
          "answer_text": "There is no indication in the document that special permits (such as for genetic modifications, stem cell work, or radioactivity) are required, nor that special or high\u2010security facilities are needed or that any additional ethical approval (with a corresponding approval document) must be obtained. The VCBA is described as a computational tool that simulates chemical fate in a standard in vitro cell culture setup (for example, using the 3T3 BALB/c cell line), and no section of the document refers to any requirements for permits, specialized facilities, or extra ethical clearance beyond standard laboratory practices.  \n_(Source: application of the virtual cell based assay for simulation-KJNA28694ENN.pdf)_"
        }
      ]
    },
    {
      "file": "ESOTC2023.pdf",
      "LLM_challenge_level": 0,
      "passes": 69,
      "total": 77,
      "pass_rate": 89.61,
      "failures": [
        {
          "question_id": 1,
          "question_text": "Provide a descriptive title using normal language without technical terms or acronyms.",
          "answer_text": "Europe in 2023: A Year of Hot Summers, Heavy Rains, and Noticeable Change  \n(Source: ESOTC2023.pdf)"
        },
        {
          "question_id": 2,
          "question_text": "Please describe in no more than 200 words the following:",
          "answer_text": "The ESOTC2023 document is a comprehensive summary of Europe\u2019s climate in 2023. It details a year of contrasting extremes, marked by record warmth in many regions, unprecedented heat stress, and significant increases in both temperature and extreme weather events. Europe experienced its joint warmest year on record, with above-average surface air temperatures, while precipitation anomalies led to both severe droughts in some areas and catastrophic floods in others. The report also highlights the impact on marine environments\u2014in particular, record-high sea surface temperatures and extensive marine heatwaves\u2014as well as significant glacier mass loss and changes in snow cover. Additionally, it provides an analysis of violent wildfires, shifts in air pressure patterns, and increased thermal stress that affected human health. Utilizing a variety of data sources, including in situ measurements, satellite observations, and reanalysis datasets, the report offers vital insights for policymakers and emphasizes the urgent need for climate action and adaptation strategies.  \n_(Source: ESOTC2023.pdf)_"
        },
        {
          "question_id": 6,
          "question_text": "To which (human) adverse outcome(s) is your test method related or could be related? (8.1; 9.2; 9.3)",
          "answer_text": "The report highlights several human adverse outcomes that may be related to the type of extreme environmental conditions monitored by the test method. In particular, the document details adverse outcomes including:\n\n\u2022 Heat\u2010related illnesses and mortality caused by extreme heat stress (with record days of \u201cextreme heat stress\u201d and increased heat\u2010related deaths noted).  \n\u2022 Injuries and fatalities from extreme weather events such as flooding and storms, which can overwhelm health services.  \n\u2022 Secondary adverse health effects including respiratory problems and exacerbated chronic diseases triggered by wildfires (and the associated air pollution) as well as mental and psychosocial health impacts due to these extreme events.\n\nThese human health outcomes are discussed in the sections on climate impacts on public health (see, for example, the discussion on extreme weather and climate events and their effects on health in the report) _(Source: ESOTC2023)_"
        },
        {
          "question_id": 7,
          "question_text": "Which hazard(s) do(es) your test method (potentially) predict? (8.1; 8.6)",
          "answer_text": "The method (i.e. the ESOTC2023 assay) provides predictive insights into a range of climate\u2010related hazards. It enables the identification of hazards such as extreme heat events (including prolonged heat stress and record\u2010breaking heatwaves), drought conditions, flooding from anomalously high precipitation and river flows, wildfires (including record large fire events), as well as marine heatwaves indicated by anomalously high sea surface temperatures. (Sources: ESOTC2023.pdf)"
        },
        {
          "question_id": 10,
          "question_text": "Provide the original/published name, as well as the potential tradename.",
          "answer_text": "The document\u2019s published title is \"European State of the Climate \u2013 Summary 2023\" and it may be referred to by its abbreviated tradename, \"ESOTC 2023.\" (Source: ESOTC2023.pdf)"
        },
        {
          "question_id": 12,
          "question_text": "This only applies to updated versions. If this is the original version, state 'original version'.",
          "answer_text": "original version (Source: ESOTC2023.pdf)"
        },
        {
          "question_id": 14,
          "question_text": "Include affiliation.",
          "answer_text": "Based on the content provided in ESOTC2023.pdf, the assay (i.e. the report) is produced through a joint collaboration. Its affiliation includes the Copernicus Climate Change Service (C3S) and the World Meteorological Organization (WMO). C3S is implemented by the European Centre for Medium\u2010Range Weather Forecasts (ECMWF) with funding from the European Union. In addition, a wide range of institutions and agencies contribute to the report. These include international organizations (for example, EUMETSAT, the European Environment Agency, and the WMO regional networks) as well as various national meteorological and hydrological services (such as DMI in Denmark, DWD in Germany, the National Observatory of Athens in Greece, and others). This extensive network of affiliations ensures that the report reflects a broad and authoritative overview of the state of the climate for 2023. (Source: ESOTC2023.pdf)"
        },
        {
          "question_id": 50,
          "question_text": "Does the method require specialized instrumentation that is not found in standard laboratories? Is there a need for custom-made instrumentation or material? Is there a need for equipment that is not commercially available (anymore)?",
          "answer_text": "There is no indication that the approach described in ESOTC2023 relies on any specialized or custom\u2010made instrumentation that isn\u2019t already part of established monitoring networks. The report is based on data from standard sources including operational satellites, in situ measurements, model\u2010based estimates, and reanalysis, all of which are produced and maintained by organizations such as ECMWF, WMO, and other affiliated institutions. There is no mention of the need for custom-made instruments or equipment that is no longer commercially available. (Source: ESOTC2023.pdf)"
        }
      ]
    },
    {
      "file": "CELEX_52020DC0065_EN_TXT.pdf",
      "LLM_challenge_level": 0,
      "passes": 68,
      "total": 77,
      "pass_rate": 88.31,
      "failures": [
        {
          "question_id": 1,
          "question_text": "Provide a descriptive title using normal language without technical terms or acronyms.",
          "answer_text": "A European plan to build computer programs that everyone can trust. (Source: CELEX_52020DC0065_EN_TXT.pdf)"
        },
        {
          "question_id": 2,
          "question_text": "Please describe in no more than 200 words the following:",
          "answer_text": "The document is a European Commission White Paper on Artificial Intelligence published on February 19, 2020. It presents a comprehensive European strategy to harness AI\u2019s benefits while addressing its risks. The text outlines how AI can improve sectors such as healthcare, manufacturing, transport, and public services by enhancing efficiency and innovation, while also underlining challenges like discrimination, opacity, and safety concerns. It calls for a coordinated approach that combines regulatory innovation with significant investment, emphasizing the creation of an \u201cecosystem of excellence\u201d through stronger research networks, enhanced skills, and SME support. The White Paper details proposals for a risk\u2010based regulatory framework that only imposes additional binding obligations on high-risk AI applications. Key measures include improved transparency, detailed record-keeping, human oversight, and prior conformity assessments to ensure compliance with EU values and fundamental rights. It also highlights the need for international cooperation and a robust governance structure that involves national and European authorities to avoid market fragmentation. Overall, the document aims to shape AI\u2019s development so that it is ethical, trustworthy, and aligned with European standards and societal expectations.  \n_(Source: CELEX_52020DC0065_EN_TXT.pdf)_"
        },
        {
          "question_id": 6,
          "question_text": "To which (human) adverse outcome(s) is your test method related or could be related? (8.1; 9.2; 9.3)",
          "answer_text": "The document does not describe a cell\u2010based toxicological assay but rather discusses a regulatory framework addressing the potential harms of artificial intelligence. However, relating the question to human adverse outcomes as discussed in the document, these include both material harms (such as safety risks that might lead to physical injury or damage) and immaterial harms (for example, infringement of fundamental rights including breaches of privacy, loss of human dignity, and discrimination, as well as issues arising from opaque or biased algorithmic decision\u2010making) _(Source: CELEX_52020DC0065_EN_TXT.pdf)_"
        },
        {
          "question_id": 10,
          "question_text": "Provide the original/published name, as well as the potential tradename.",
          "answer_text": "The document\u2019s published title is \u201cWhite Paper on Artificial Intelligence \u2013 A European approach to excellence and trust\u201d (Source: CELEX_52020DC0065_EN_TXT.pdf). The paper itself does not specify a separate tradename; however, it is sometimes informally referred to by shorthand as the \u201cEU AI White Paper\u201d (Source: CELEX_52020DC0065_EN_TXT.pdf)."
        },
        {
          "question_id": 11,
          "question_text": "Provide the original deposition date of first version and date of current version.",
          "answer_text": "The document header shows \u201cBrussels, 19.2.2020\u201d along with the designation \u201cCOM(2020) 65 final.\u201d No separate deposition date for a prior version is provided, so the document\u2019s first (original) and current (final) version both are dated 19 February 2020.  \n_(Source: CELEX_52020DC0065_EN_TXT.pdf)_"
        },
        {
          "question_id": 12,
          "question_text": "This only applies to updated versions. If this is the original version, state 'original version'.",
          "answer_text": "original version (Source: CELEX_52020DC0065_EN_TXT.pdf)"
        },
        {
          "question_id": 14,
          "question_text": "Include affiliation.",
          "answer_text": "The assay (the White Paper on Artificial Intelligence) is affiliated with the European Commission. This affiliation is indicated on the first page of the document, which shows \u201cEUROPEAN COMMISSION Brussels, 19.2.2020.\u201d _(Source: CELEX_52020DC0065_EN_TXT.pdf)_"
        },
        {
          "question_id": 42,
          "question_text": "Which rule do you apply to test whether a test run is within the normal performance frame? How do you document this decision? Indicate actions if the AC are not met.",
          "answer_text": "The approach is to check that the outcomes of the run fall within the predefined acceptance criteria\u2014that is, the performance \u201cbaseline\u201d defined during the design and validation phases. In practice this means applying the rule defined under the robustness and accuracy requirements (see the \u201cRobustness and Accuracy\u201d section in the document) to compare the test run\u2019s output against the expected\u00a0benchmark. In addition, the decision is documented by recording all relevant data: the test conditions, the measured parameters, a description of the training data and record\u2010keeping elements (as outlined in the \u201cKeeping of Records and Data\u201d section), and by comparing these with the baseline values. If the acceptance criteria (AC) are not met\u2014that is, if the test run falls outside the normal performance frame\u2014the process requires that corrective actions be initiated such as re\u2010running the test (possibly after a review of data and parameters), recalibrating or re\u2011training the system as needed, and formally logging a non\u2011conformity report so that the appropriate follow\u2011up (including, if necessary, notification to the relevant competent authorities) can proceed. \n\n(Sources: CELEX_52020DC0065_EN_TXT.pdf)"
        },
        {
          "question_id": 57,
          "question_text": "How and how long are raw and other related data stored? What backup procedures are used (how frequently)? How are data versions identified?",
          "answer_text": "The document specifies that economic operators must keep accurate records regarding the data sets (including those used for training and testing AI systems) and related documentation (such as details of the algorithms and training processes) for a limited and reasonable period to ensure effective enforcement of the applicable rules. However, it does not provide a specific time period for how long raw or related data should be stored. Similarly, the document does not detail any backup procedures (or their frequency) nor does it explain how different data versions are marked or identified. \n\n_(Source: CELEX_52020DC0065_EN_TXT.pdf)_"
        }
      ]
    },
    {
      "file": "The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf",
      "LLM_challenge_level": 0,
      "passes": 57,
      "total": 77,
      "pass_rate": 74.03,
      "failures": [
        {
          "question_id": 1,
          "question_text": "Provide a descriptive title using normal language without technical terms or acronyms.",
          "answer_text": "A possible descriptive title is: \"The Hidden Toll of Detailed Data Requirements on Researchers.\" (Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)"
        },
        {
          "question_id": 2,
          "question_text": "Please describe in no more than 200 words the following:",
          "answer_text": "The assay investigates the cumulative impact of FAIR data management on biomedical researchers by quantifying metadata-induced fatigue. Over a 36\u2011month period, 47 researchers completed weekly FAIRification tasks\u2014dealing with complex metadata schemes, semantic validation tools, and triple store deposition protocols. The study measured both physiological and behavioral responses, including metrics such as eye twitch frequency, cortisol levels during SPARQL query failures, template rage incidents, and triple hallucinations. It found that continuous exposure to extensive metadata work\u2014quantified by nearly 389 submitted triples per week on average and multiple evolving template versions\u2014correlated with increased levels of fatigue and decreased morale. The study attributes these effects to phenomena like semantic drift syndrome and ontological burnout. Its findings underscore the need for ergonomic and psychologically considerate metadata schema designs, suggesting strategies like FAIR breaks, community-based schema therapy, and metadata detox retreats to mitigate the burden. Overall, the assay highlights the importance of balancing rigorous data stewardship practices with measures that protect researcher well-being.\n\n(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)"
        },
        {
          "question_id": 4,
          "question_text": "Which test system and readout(s) are used? (4.1; 5.2)",
          "answer_text": "The test system for this assay consists of 47 biomedical researchers from four European consortia who performed weekly FAIRification tasks (Methods section). The readouts include both physiological measures (e.g., eye twitch frequency and cortisol levels during SPARQL query failures) and behavioral indicators (e.g., template rage and triple hallucinations) as well as metrics like the average number of triples submitted per week, the SPARQL Rage Incident Metric, and the number of template versions (Methods, Results sections). (Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)"
        },
        {
          "question_id": 5,
          "question_text": "Which biological process(es) (e.g. neurite outgrowth, differentiation) and/or toxicological events (e.g. oxidative stress, cell death) are modelled/reflected by your test method? (8.1)",
          "answer_text": "The assay isn\u2019t set up to model conventional cellular processes such as neurite outgrowth, differentiation, oxidative stress, or cell death. Instead, it is designed to capture and quantify the physiological and psychological \u201ctoxicological\u201d impact of prolonged engagement with complex metadata operations. In this study, endpoints include measures of researcher stress such as eye twitch frequency, cortisol levels during SPARQL query failures, and behavioral patterns such as template rage and triple hallucinations, which serve as proxies for cognitive and physical stress responses. These phenomena are reflections of metadata-induced fatigue rather than traditional toxicological events in cell biology.  \n_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_"
        },
        {
          "question_id": 6,
          "question_text": "To which (human) adverse outcome(s) is your test method related or could be related? (8.1; 9.2; 9.3)",
          "answer_text": "The assay is related to adverse outcomes including psychological fatigue and stress (evidenced by phenomena such as SPARQL Rage and template fatigue), burnout (specifically \"ontological burnout\"), as well as physical symptoms such as increased eye twitch frequency and elevated cortisol levels. These outcomes capture both the mental and physiological toll of prolonged exposure to complex metadata management tasks. (Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)"
        },
        {
          "question_id": 7,
          "question_text": "Which hazard(s) do(es) your test method (potentially) predict? (8.1; 8.6)",
          "answer_text": "The assay predicts hazards stemming from prolonged exposure to complex metadata, including psychological stress (e.g., template fatigue, ontological burnout, glossary anxiety, and JSON\u2010induced insomnia), physical responses (such as increased eye twitch frequency and elevated cortisol levels during SPARQL query failures), and semantic complications (e.g., SPARQL rage, semantic drift syndrome, and compulsive identifier normalization) (Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)."
        },
        {
          "question_id": 9,
          "question_text": "If the method has undergone some form of validation/evaluation, give its status. (9.4)",
          "answer_text": "The method was rigorously evaluated in a 36\u2011month longitudinal study involving 47 researchers across four European consortia. Using both quantitative metrics (e.g., average weekly triples submitted, the SPARQL Rage Incident Metric) and qualitative assessments (e.g., coding of Slack channel outbursts), the study reliably documented the accumulation of metadata-induced fatigue. This confirms that the method has undergone comprehensive evaluation with validated outcomes. (Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)"
        },
        {
          "question_id": 10,
          "question_text": "Provide the original/published name, as well as the potential tradename.",
          "answer_text": "The published title is \u201cThe Burden of FAIRness: A Longitudinal Study of Metadata-Induced Fatigue in Biomedical Researchers\u201d (as seen in the document\u2019s title and abstract). The provided context does not indicate a distinct potential tradename.  \n_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_"
        },
        {
          "question_id": 12,
          "question_text": "This only applies to updated versions. If this is the original version, state 'original version'.",
          "answer_text": "Updated version (Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)"
        },
        {
          "question_id": 14,
          "question_text": "Include affiliation.",
          "answer_text": "The assay was conducted by researchers affiliated with the Institute for MetaResearch Fatigue Studies (IMRFS) (Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)."
        },
        {
          "question_id": 36,
          "question_text": "Define and describe the principle(s) of the analytical methods used. Provide here a general overview of the method\u2019s key steps (e.g. cells are fixed or not, homogenized sample or not, etc.), sufficient for reviewers/regulators to understand what was done, but not in all detail for direct repetition. If you have two or more endpoints (e.g. viability and neurite outgrowth), do you measure both in the same well, under same conditions in parallel, or independently of each other? For imaging endpoints: Explain in general how quantification algorithm or how semi-quantitative estimates are obtained and how many cells are imaged (roughly).",
          "answer_text": "The analytical method in this study uses a longitudinal, mixed\u2010methods design to probe the effects of repetitive and complex metadata tasks on biomedical researchers. In practice, 47 researchers performed weekly FAIRification tasks over 36 months that involved interacting with complex metadata schemes, semantic validation tools, and triple store deposition protocols. During these tasks, two broad types of endpoints were quantified:\n\n1. Physiological endpoints \u2013 for instance, measurements such as eye twitch frequency and cortisol levels during SPARQL query failures were recorded. These provide objective indicators of stress induced by metadata processing tasks.\n\n2. Behavioral endpoints \u2013 subjective phenomena like \u201ctemplate rage\u201d and \u201ctriple hallucinations\u201d were also monitored. These were further supported by qualitative coding of communications (such as Slack channel outbursts) to capture the nuances of metadata-induced fatigue.\n\nBoth sets of endpoints were measured concurrently but via separate modalities (i.e., physiological indicators were logged simultaneously with behavioral observations rather than being derived from the same \u201csample\u201d). Unlike traditional cell-based assays, there was no sample homogenization, cell fixation, or imaging of cells. In this case, \u201cimaging endpoints\u201d were not applicable since the study did not involve cellular imaging or image-based quantifications.\n\nThis overview should give reviewers and regulators a clear, general understanding of the key steps and endpoints without providing exhaustive procedural detail.  \n(Sources: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)"
        },
        {
          "question_id": 42,
          "question_text": "Which rule do you apply to test whether a test run is within the normal performance frame? How do you document this decision? Indicate actions if the AC are not met.",
          "answer_text": "The rule applied is to compare the test run\u2019s key performance metrics\u2014such as the average number of triples submitted, the SPARQL Rage (SPRIM) values, and the number of templates\u2014to the benchmark values documented (as shown in Table 1 of the study). In other words, a test run is considered within the normal performance frame if its metrics fall within the established range (for example, around an average of 389 triples per week, SPRIM values close to those reported, and template counts per consortium as noted).\n\nThis decision is documented by logging the observed metrics in the test run report and comparing them directly with the acceptance criteria derived from the study\u2019s performance benchmarks (see Methods and Table 1). If the acceptance criteria (AC) are not met\u2014for instance, if there is a significant deviation in SPARQL Rage or an abnormal number of triples or templates\u2014then actions are triggered. These include notifying stakeholders, conducting diagnostic reviews to determine sources of the deviation, and implementing remedial measures (such as a FAIR break or protocol adjustments as discussed in the Recommendations for metadata detox retreats).\n\n(Sources: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)"
        },
        {
          "question_id": 43,
          "question_text": "Indicate 'real data points per month' (not per week/per quarter, etc.): count three working weeks per month. Each concentration is a data point. Necessary controls that are required for calibration and for acceptability criteria are NOT counted as data points. All technical replicates of one condition are counted as one single data point (see notes for explanation) Indicate possibility/extent of repeated measures (over time) from same dish. Explain your estimate.",
          "answer_text": "Based on the study design described in The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf, the following can be deduced:\n\n\u2022 For each condition (interpreted here as each concentration), one measurement is taken during each working week. Using three working weeks per month, this translates to three real data points per month per condition (with necessary calibration controls omitted and all technical replicates merged into one).  \n\u2022 Because the FAIRification tasks were performed every week over a 36\u2011month period, the same \u201cdish\u201d (or research subject\u2019s task) is measured repeatedly over time. This provides a repeated\u2010measures design where, for any given condition, the same source is tracked longitudinally across months.\n\n_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_"
        },
        {
          "question_id": 50,
          "question_text": "Does the method require specialized instrumentation that is not found in standard laboratories? Is there a need for custom-made instrumentation or material? Is there a need for equipment that is not commercially available (anymore)?",
          "answer_text": "The document does not indicate that the method requires any specialized, custom\u2010made, or commercially unavailable instrumentation. It only refers to standard procedures like logging physiological data (e.g., eye twitch frequency and cortisol levels) and applying metadata tasks using common protocols and tools. There is no mention of any equipment beyond what is typically available in standard laboratories. _(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_"
        },
        {
          "question_id": 51,
          "question_text": "Describe possible variations, modifications and extensions of the test method: a) other endpoints, b) other analytical methods for same endpoint, c) other exposure schemes (e.g. repeated exposure, prolonged exposure, etc.), d) experimental variations (e.g. use of a specific medium, presence of an inhibitor or substrate that affects test outcome, etc.)",
          "answer_text": "Based on The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf, potential modifications and extensions include:\n\na) Other endpoints:  \n\u2022 Besides the current endpoints (SPARQL Rage, semantic drift syndrome, and triple hallucinations), one could measure additional physiological responses such as variations in cortisol levels and eye twitch frequency, or capture behavioral markers (e.g., glossary anxiety or JSON-induced insomnia).  \n_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_\n\nb) Other analytical methods for the same endpoint:  \n\u2022 While the study uses a mixed\u2010methods approach (quantitative metrics like SPRIM and qualitative coding of Slack outbursts), one could also deploy advanced time\u2010series analysis for fatigue progression, detailed eye tracking analytics, or apply machine learning classification to detect nuanced changes in semantic drift.  \n_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_\n\nc) Other exposure schemes:  \n\u2022 In addition to the weekly FAIRification tasks over 36 months, one might consider testing repeated or intermittent exposures versus continuous prolonged exposure, or even challenge the system with bursts of metadata tasks followed by recovery periods, to assess the dose\u2013response relationship of metadata-induced fatigue.  \n_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_\n\nd) Experimental variations:  \n\u2022 Variations could include using different types of media or deposition protocols (e.g., switching between JSON, YAML, or RDF-based formats), deploying inhibitors (such as enforced \u201cFAIR breaks\u201d or schema therapy groups) that might attenuate fatigue, or introducing substrates (for instance, automated annotation humor modules) that could alter the test outcome.  \n_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_\n\nThese variants provide avenues for understanding and mitigating the multifaceted impacts of prolonged metadata exposure on biomedical researchers."
        },
        {
          "question_id": 52,
          "question_text": "Indicate the names (and database names) of related tests and give a short description (including a brief comment on differences to the present method). If the test method has been used for high throughput transcriptomics or deep sequencing as alternative endpoint, this should be indicated.",
          "answer_text": "The document lists four related test efforts by their project codes, which one may view as both the test names and implicit \u201cdatabase names\u201d for these related assays. These are:\n\n\u2022\u2003FAIR4U  \n\u2022\u2003DATASTORM  \n\u2022\u2003ONTODRIVE  \n\u2022\u2003METANIGHT  \n\nEach of these projects employed similar metadata deposition and validation workflows; however, they differed in scale and impact. For instance, although the overall approach was comparable\u2014with regular submission of RDF triples and use of semantic validation protocols\u2014the projects varied in the average number of triples submitted per week, the intensity of SPARQL Rage (SPRIM), and the number of metadata templates in use. In contrast to methods used in high throughput transcriptomics or deep sequencing (which employ molecular endpoints), these tests focused on the psychological, physiological, and semantic consequences of FAIR enforcement. There is no indication in the document that any of these tests have been used with high throughput transcriptomics or deep sequencing as alternative endpoints.  \n_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_"
        },
        {
          "question_id": 58,
          "question_text": "How are metadata documented and stored (lab book, Excel files, left in machine, etc.)? How are they linked to raw data? What metadata are stored/should be stored?",
          "answer_text": "The document does not specify details such as whether metadata are recorded in lab books, Excel files, or simply left \u201cin machine\u201d memory. Instead, it explains that researchers undertake FAIRification tasks involving complex metadata schemes, use semantic validation tools, and follow triple store deposition protocols to handle their metadata (Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf). Likewise, the study does not detail the mechanism by which metadata are linked to raw data nor does it list the specific kinds of metadata that are or should be stored. The focus is on the challenges introduced by extensive metadata (e.g., RDF triples, SPARQL endpoint data) and the resulting fatigue, rather than on precise documentation or linkage procedures."
        },
        {
          "question_id": 67,
          "question_text": "Refer to published literature on the test AND indicate in detail deviations from published descriptions (e.g. plastic plate supplier, cell number, endpoint measurement, timing, etc.). Provide the most relevant publications that describe/give a comprehensive overview of (a) your test system and/or (b) your test method. Describe what aspects are covered therein. Give a prioritized (according to importance) list of further publications on the test method or its application. Give short comments on which type(s) of information can be obtained from these publications (e.g. contains test chemical lists, contains more positive/negative controls, contains validation against other tests, contains incorporation in test battery, demonstrates use by other lab, etc.).",
          "answer_text": "Below is an answer based solely on the information provided in The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf:\n\n1. Published literature on the test  \n\u2003\u2022 The document cites three key references that provide background and complementary overviews of aspects of the assay. These are:  \n\u2003\u2003\u2013 [1] SPARQL Rage Support Group (2024), \u201cEscaping the Triple Spiral\u201d \u2013 This publication focuses on behavioral endpoints such as SPARQL query\u2010related frustrations (e.g. SPARQL Rage) and offers benchmarks for how query failures impact user physiology and behavior.  \n\u2003\u2003\u2013 [2] FAIR4U Consortium (2023), \u201cTemplate Overload and its Discontents\u201d \u2013 This work describes issues associated with excessive template iterations and offers a perspective on how template proliferation influences user stress and workflow interruptions.  \n\u2003\u2003\u2013 [3] Van Onto (2022), \u201cA Meta-Meta-Analysis of Metadata Metadata\u201d \u2013 This meta\u2010analysis reviews multiple studies on metadata handling, including aspects of semantic drift and annotation overload.\n\n2. Deviations from published descriptions  \n\u2003\u2022 The study described in our document uses a mixed-methods approach that differs from previous literature in that it combines both physiological measures (e.g. eye twitch frequency, cortisol levels during SPARQL query failures) and behavioral indicators (e.g. template rage, triple hallucinations).  \n\u2003\u2022 Unlike traditional cell-based test techniques\u2014which often detail parameters such as plastic plate supplier, cell number, endpoint measurement, and timing\u2014this test system focuses on human cognitive and physiological responses to FAIR-related metadata tasks. In that respect, no deviations such as plate supplier or cell number are reported because the \u201cassay\u201d does not use cell culture elements.  \n\u2003\u2022 Moreover, whereas published literature in technical assay contexts might specify reagent sources and incubation times, our assay adapts a longitudinal trial format (47 researchers over 36 months) to capture the progressive impact of metadata-induced fatigue.  \n\u2003\u2003\u2003_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_\n\n3. Prioritized list of further publications and the type of information they provide  \n\u2003a. First Priority \u2013 SPARQL Rage Support Group (2024), \u201cEscaping the Triple Spiral\u201d  \n\u2003\u2003\u2022 Provides detailed insight into the measurement and characterization of frustration-type endpoints (e.g. SPARQL Rage) that are central to the current test.  \n\u2003\u2003\u2022 Includes benchmarks for query failure impacts, which help put our physiological markers (like cortisol spikes) into context.  \n\u2003\u2003\u2003_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_\n\n\u2003b. Second Priority \u2013 FAIR4U Consortium (2023), \u201cTemplate Overload and its Discontents\u201d  \n\u2003\u2003\u2022 Contains comprehensive descriptions of template proliferation and related behavioral outcomes.  \n\u2003\u2003\u2022 Offers extended data on test parameters such as template version counts and methodological notes on how overload is quantified.  \n\u2003\u2003\u2003_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_\n\n\u2003c. Third Priority \u2013 Van Onto (2022), \u201cA Meta-Meta-Analysis of Metadata Metadata\u201d  \n\u2003\u2003\u2022 Presents a broad review of multiple test systems addressing metadata fatigue, covering aspects such as semantic drift and identifier normalization.  \n\u2003\u2003\u2022 Serves as an overarching reference for validation against other tests and for understanding incorporation in broader test batteries.  \n\u2003\u2003\u2003_(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_\n\nEach of these publications provides complementary information: the first offers details on behavioral endpoints, the second focuses on methodological aspects concerning template workflows, and the third validates and contextualizes the assay within the larger framework of metadata research."
        },
        {
          "question_id": 70,
          "question_text": "Indicate/summarize activities for test qualification, pre-validation or validation. Indicate e.g. ring trials, full (pre-)validations. Give an overview of compounds or libraries that have been tested.",
          "answer_text": "The document does not mention any specific activities such as ring trials, full pre\u2010validation, or formal test qualification procedures. Instead, the study reports that 47 researchers were monitored over 36 months while they performed weekly FAIRification tasks (e.g., working with complex metadata schemes, semantic validation tools, and triple store deposition protocols) to track indicators like SPARQL Rage and template usage _(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_. There is also no overview provided of any compounds or libraries that have been tested."
        },
        {
          "question_id": 73,
          "question_text": "Has the test system been transferred to other labs? Has the test method been used by various operators (over a long time period)? Has the test method been transferred to other labs? Is there data on inter-laboratory variability? What are procedures and how was the performance (experience) of the transfer?",
          "answer_text": "The document does not provide any information on the test system being transferred to other labs. It also does not describe whether various operators used the test method over a long period, nor does it include any data on inter-laboratory variability or procedures related to the transfer and its performance. _(Source: The_Burden_of_FAIRness_Metadata_Fatigue_Expanded.pdf)_"
        }
      ]
    },
    {
      "file": "Optimization of cell viability assays to improve repilicability and reproducibility.pdf",
      "LLM_challenge_level": 2,
      "passes": 25,
      "total": 77,
      "pass_rate": 32.47,
      "failures": [
        {
          "question_id": 1,
          "question_text": "Provide a descriptive title using normal language without technical terms or acronyms.",
          "answer_text": "Improving Laboratory Methods to Reliably Measure the Impact of Cancer Drugs on Cell Health. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 2,
          "question_text": "Please describe in no more than 200 words the following:",
          "answer_text": "The assay is designed to optimize cell viability testing in cancer drug sensitivity screens by systematically identifying and correcting sources of experimental variability. It employs the resazurin reduction assay in a 2D cell culture model to measure viability after drug treatment. Key experimental parameters\u2014including cell seeding density, growth medium composition, drug storage conditions, evaporation (particularly edge effects in 96\u2011well plates), and the use of matched DMSO vehicle controls\u2014are optimized to minimize intra\u2011 and interexperimental variability. Quality control metrics, such as Z-factor, signal window, and coefficient of variation, are used to confirm assay robustness. The optimized conditions yield reproducible dose-response curves and reliable drug potency measurements (e.g., IC50, GR50, GRmax) for different breast cancer cell lines (MCF7, HCC38, MCF\u201110A, and MDA\u2011MB\u2011436). This standardized approach ensures that critical confounders are addressed, facilitating consistent and predictive preclinical cancer drug screening results. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 3,
          "question_text": "Which toxicological target (organ, tissue, physiological/biochemical function, etc.) is modelled? (8.1)",
          "answer_text": "The assay models the cytotoxic response of breast cancer cells by evaluating cell viability. In other words, it uses several breast-derived cell lines (e.g. MCF7, HCC38, MCF-10A, and MDA-MB-436) to assess how anticancer drugs (such as bortezomib, cisplatin, and carboplatin) affect cellular metabolic activity and survival. This approach targets the cell viability endpoint, which serves as a proxy for assessing drug-induced toxicity in cancer cells. (Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens)"
        },
        {
          "question_id": 4,
          "question_text": "Which test system and readout(s) are used? (4.1; 5.2)",
          "answer_text": "The assay uses in vitro 2D cultures of breast cancer cell lines (primarily MCF7 and HCC38 during optimization, with additional validation in MCF\u201110A and MDA\u2011MB\u2011436 cells) as the test system. It determines cell viability by a resazurin reduction assay, where the conversion of resazurin to resorufin is measured either via absorbance (using a 560\u202fnm excitation/615\u202fnm emission setup) or by fluorescence. (Sources: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 5,
          "question_text": "Which biological process(es) (e.g. neurite outgrowth, differentiation) and/or toxicological events (e.g. oxidative stress, cell death) are modelled/reflected by your test method? (8.1)",
          "answer_text": "The assay is designed to measure cell viability and, by extension, cytotoxicity induced by drug treatments. In our test method, the decrease in resazurin conversion is used as a surrogate for cell death resulting from drug-induced cytotoxicity. In addition, under specific drug treatments (for example, bortezomib), the assay\u2014in conjunction with additional analyses such as cell cycle assessment\u2014is used to detect effects like G2/M phase arrest, which is indicative of altered cell cycle progression due to toxicity. \n\n(Sources: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 6,
          "question_text": "To which (human) adverse outcome(s) is your test method related or could be related? (8.1; 9.2; 9.3)",
          "answer_text": "The test method is designed to detect changes in cell viability that arise from exposure to potentially cytotoxic compounds. In this context, the assay is related to human adverse outcomes associated with cytotoxicity \u2013 that is, reduced cell viability, induction of cell death (via mechanisms such as apoptosis or necrosis), and cell cycle disruption (e.g. G2/M phase arrest as seen with bortezomib). In a human setting, these effects can be linked to tissue toxicity, impaired tissue regeneration, and organ dysfunction when exposure to such compounds occurs. \n\n(Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens)"
        },
        {
          "question_id": 7,
          "question_text": "Which hazard(s) do(es) your test method (potentially) predict? (8.1; 8.6)",
          "answer_text": "This assay is designed to predict hazards related to drug\u2010induced cytotoxicity. In other words, it evaluates the potential of pharmaceutical compounds (e.g. bortezomib, cisplatin, carboplatin) to reduce cell viability \u2013 a proxy for cell death, growth inhibition, and other adverse cellular responses. These endpoints help indicate whether a compound poses a cytotoxic hazard in a preclinical setting.  \n_(Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)_"
        },
        {
          "question_id": 8,
          "question_text": "Does the test method capture an endpoint of current regulatory studies? (9.5)",
          "answer_text": "Yes, the assay is designed to capture endpoints\u2014such as the IC50 and growth rate inhibition metrics (GR50, GRmax)\u2014that are commonly used in regulatory studies to assess drug potency and cytotoxicity. These endpoints are well recognized in current regulatory evaluations of drug safety and efficacy. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 9,
          "question_text": "If the method has undergone some form of validation/evaluation, give its status. (9.4)",
          "answer_text": "The optimized assay has been validated. It was evaluated using quality control metrics such as Z-factor, signal window, and coefficient of variation, with the majority of data points showing a Z-factor above 0.75, a signal window greater than 10, and a CV below 5%\u2014all meeting recommended thresholds. In addition, validation experiments (including comparison of IC50, GR50, and GRmax values across four different breast cancer cell lines and comparison with published data such as from pharmacoDB and Hafner et al.) confirmed its robustness, reproducibility, and suitability for drug cytotoxicity testing. (Sources: Optimization of cell viability assays to improve repilicability and reproducibility.pdf)"
        },
        {
          "question_id": 10,
          "question_text": "Provide the original/published name, as well as the potential tradename.",
          "answer_text": "The article\u2019s published title is \u201cOptimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens\u201d (Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens.pdf). In this work the cell viability measurement is based on the reduction of resazurin\u2014a method that is commonly sold under the tradename \u201cAlamar Blue\u201d (Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens.pdf)."
        },
        {
          "question_id": 11,
          "question_text": "Provide the original deposition date of first version and date of current version.",
          "answer_text": "The manuscript was originally deposited (i.e. received) on 19 November 2019 and its current version was accepted on 25 February 2020. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 12,
          "question_text": "This only applies to updated versions. If this is the original version, state 'original version'.",
          "answer_text": "original version (Source: Optimization of cell viability assays to improve repilicability and reproducibility.pdf)"
        },
        {
          "question_id": 14,
          "question_text": "Include affiliation.",
          "answer_text": "The assay\u2019s authors are affiliated with the following institutions:  \n\u2022 Department of Oncology, Institute of Clinical Sciences, Sahlgrenska Cancer Center, Sahlgrenska Academy at University of Gothenburg, Gothenburg, Sweden  \n\u2022 Department of Clinical Pathology, Sahlgrenska University Hospital, Gothenburg, Sweden  \n\u2022 Department of Radiation Physics, Institute of Clinical Sciences, Sahlgrenska Cancer Center, Sahlgrenska Academy at University of Gothenburg, Gothenburg, Sweden  \n_(Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)_"
        },
        {
          "question_id": 15,
          "question_text": "Provide the details of the principal contact person.",
          "answer_text": "The principal contact person is Peter Larsson. He is designated as the corresponding author, and his email is peter.larsson.3@gu.se. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 16,
          "question_text": "For example, the principal investigator (PI) of the lab, the person who conducted the experiments, etc.",
          "answer_text": "The document specifies which individuals played particular roles in the assay development. For instance, Peter Larsson is noted as the person who actually \u201cperformed the experiments\u201d (see the Author contributions section: \u201cP.L.: Performed the experiments\u201d) while the overall study concept, experimental design, and acquisition of funding were led by senior investigators such as Toshima Z. Parris, Khalil Helou, and Per Karlsson (see \u201cAuthor contributions\u201d where it states \u201cT.Z.P., K.H. and J.B.: Study concept and experimental design\u201d and \u201cK.H., P.K. and T.Z.P.: Acquisition of funding\u201d). This indicates that in this work the principal investigator(s) provided guidance, secured resources, and oversaw the project, whereas the day\u2010to\u2010day experimental work was conducted by Peter Larsson.  \n_(Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)_"
        },
        {
          "question_id": 17,
          "question_text": "Supply number of supporting files. Describe supporting files (e.g. metadata files, instrument settings, calculation template, raw data file, etc.).",
          "answer_text": "The paper provides several supporting files that together help ensure full transparency of the assay\u2010optimization process. For example, supplementary information is made available via the provided DOI, and these files include:\n\n\u2022 A metadata file detailing cell culture conditions, reagent details (e.g. FBS concentrations, solvent information), instrument settings (e.g. plate reader filter wavelengths for resazurin detection), and experimental parameters such as cell seeding density and drug storage conditions.  \n\u2022 Calculation templates (in the form of R scripts and/or spreadsheets) used for generating dose\u2013response curves and computing key metrics (IC50, GR50, GRmax), as well as quality control metrics (Z-factor, signal window, coefficient of variation).  \n\u2022 Raw data files that contain the underlying resazurin absorbance measurements and the dose\u2013response data collected under the different experimental conditions.  \n\u2022 Supplementary figures and tables (for instance, a file including Supplementary Table 1 on variance component analysis and Supplementary Table 2 on assay quality control metrics, plus Supplementary Figure 1 that illustrates the edge effect on 96\u2011well plates).\n\nEach of these files supports different aspects of the assay\u2014from instrument settings and metadata required for reproducibility, to the calculation templates and raw data essential for independent validation of the analysis results _(Sources: Optimization of cell viability assays to improve replicability and reproducibility.pdf)_."
        },
        {
          "question_id": 18,
          "question_text": "Describe briefly whether the cells are from a commercial supplier, continuously generated by cell culture, or obtained by isolation from human/animal tissue (or other).",
          "answer_text": "The cells used in the assay are established human breast cancer cell lines that were purchased from a commercial supplier (the American Type Culture Collection, ATCC). This information is specified in the Methods section of the document. (Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens.pdf)"
        },
        {
          "question_id": 19,
          "question_text": "Give a brief overview of your biological source system, i.e. the source or starting cells that you use. Which cell type(s) are used or obtained (e.g. monoculture/co-culture, differentiation state, 2D/3D, etc.)? If relevant, give human donor specifications (e.g. sex, age, pool of 10 donors, from healthy tissue, etc.).",
          "answer_text": "The assay employs human breast cell lines grown as conventional 2D monocultures. Specifically, established breast cancer cell lines such as HCC38, MCF7, and MDA-MB-436 are used, along with the non-tumorigenic mammary cell line MCF\u201010A. No additional donor\u2010specific details (e.g., age or sex) are provided in the source document. \n\n_(Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)_"
        },
        {
          "question_id": 20,
          "question_text": "List quantitative and semi-quantitative features that define your cell source/starting cell population. For test methods that are based on differentiation, describe your initial cells, e.g. iPSC, proliferating SH-SY5Y; the differentiated cells are described in section 4. Define cell identity, e.g. by STR signature (where available), karyotype information, sex (where available and relevant), ATCC number, passage number, source (supplier), sub-line (where relevant), source of primary material, purity of the cells, etc. Describe defining biological features you have measured or that are FIRMLY established (use simple listing, limit to max. 0.5 pages), e.g. the cells express specific marker genes, have specific surface antigens, lack certain markers, have or lack a relevant metabolic or transporting capacity, have a doubling time of x hours, etc. Transgenic cell lines have particular requirements concerning the characterization of the genetic manipulation (type of transgene, type of vector, integration/deletion site(s), stability, etc.). Organoids and microphysiological systems (MPS) may need some special/additional considerations as detailed in Pamies et al. (2018) and Marx et al. (2016), e.g. ratio of cell types used, percent of normal cells in tumor spheroids created from resected tissue; derivation of cells for re-aggregating brain cultures.",
          "answer_text": "\u2022 Four breast cell lines were used: MCF7, HCC38, MCF-10A, and MDA-MB-436 (all obtained from ATCC), with authentication performed using ATCC STR profiling (no further details on karyotype, passage numbers, or sub-lines are provided).  \n\u2022 Quantitative seeding details: cells were plated at a density of 7.5\u00a0\u00d7\u00a010\u00b3 cells per 96-well in 100 \u03bcl culture medium.  \n\u2022 Population doubling times were measured, with approximate values of 21 hours (MCF-10A), 26 hours (MCF7), 35 hours (MDA-MB-436) and 36 hours (HCC38).  \n\u2022 The cells were cultured under typical conditions (RPMI or DMEM supplemented with 10% FBS and other specific additives as appropriate) without further details on specific marker gene expression, surface antigen profile, or purity figures provided in this document.  \n\n( Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf )"
        },
        {
          "question_id": 21,
          "question_text": "Describe the acceptance criteria (AC) for your initial cells (i.e. the quality criteria for your proliferating cell line, tissue for isolation, organism, etc.). Which specifications do you consider to describe the material, which quality control criteria have to be fulfilled (e.g. pathogen-free)? Which functional parameters (e.g. certain biological responses to reference substances) are important? For iPSC maintenance: How do you control pluripotency? How stable are your cells over several passages? Which passage(s) are valid? For primary cells: Show stability and identity of supply; demonstrate stability of function (e.g. xenobiotic metabolism). Quantitative definitions for AC should be given based on this defining information. Exclusion criteria (features to be absent) are also important. As in 3.3., special/additional requirements apply to genetically-modified cells and microphysiological systems.",
          "answer_text": "The study relied on established, authenticated cancer cell lines (e.g. HCC38, MCF7, MCF\u201010A, and MDA\u2010MB\u2010436) purchased from a reputable repository (ATCC) whose cells are pre\u2011qualified for purity and identity via STR profiling. In our assay, the following acceptance criteria for the initial cells (i.e. the quality criteria for our proliferating cell lines) were considered:\n\n1. Identity and purity are ensured by using cells from ATCC (with authentication by STR profiling) and by confirming that cells are pathogen\u2010free (for example, since ATCC supplies pathogen\u2010screened cells, additional mycoplasma tests are implicit).  \n2. The cells must show reproducible growth characteristics, with population doubling times that fall within expected ranges (e.g. about 26 hours for MCF7 and 36 hours for HCC38, as measured by non\u2010linear regression during exponential growth).  \n3. The cells\u2019 functional responses are verified by comparing their viability in response to reference agents (for example, cytotoxic responses to bortezomib and cisplatin yield reproducible IC50, GR50, and GRmax values under optimized conditions).  \n4. Culture conditions (for example, the use of a seeding density of 7.5 \u00d7 10\u00b3 cells per 96\u2011well in 100 \u00b5l growth medium with 10% FBS) are optimized to ensure that cells remain in the exponential phase during treatment, without reaching plateau phase\u2010associated changes.\n5. Quantitative acceptance criteria at the assay level are defined by quality control metrics: a Z-factor close to 1 (most data here had Z > 0.75), a signal window (SW) that exceeds 10 (ideally >2 SD of the largest signal), and a coefficient of variation (CV) of less than 5% (threshold <20%). Although these QC values are measured at the assay level, they reflect that the cells behave consistently under standardized conditions.\n6. Exclusion criteria include the appearance of contamination (such as mycoplasma or other pathogens), unexpected deviations in population doubling time or morphology, or aberrant or non\u2010reproducible responses to the reference compounds.\n\nFor genetically modified cells or systems with additional complexity (e.g. iPSC or primary cells), extra requirements would typically include demonstrating retention of identity, stability of function over serial passages (with defined valid passage numbers), and control of specific functional markers (such as pluripotency markers in iPSC cultures). In our case, since established cancer cells are used, the focus is on cell identity, pathogen\u2010free status, expected growth rates, and reproducible drug sensitivity profiles.\n\n(All details are derived from the context provided in Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens \u2013 Methods and Discussion sections.)"
        },
        {
          "question_id": 22,
          "question_text": "Name known causes of variability of the initial cells/source cells. Indicate critical consumables or batch effects (e.g. relevance of the plate format and supplier, batch effects of fetal calf serum (FCS) or serum replacement, critical additives like type of trypsin, apo-transferrin vs. holo-transferrin, etc.). Indicate critical handling steps and influencing factors (e.g. special care needed in pipetting, steps that need to be performed quickly, cell density, washing procedures, etc.). As in 3.3., special/additional requirements apply to genetically-modified cells and microphysiological systems, e.g. dependence on matrix chemistry and geometry, dependence on microfluidics system, consideration of surface cells vs core cells, etc. Give recommendations to increase/ensure reproducibility and performance.",
          "answer_text": "The study identified several factors that can introduce variability right from the start with the initial/source cells and consumables. For example, variability may arise from differences in cell types (including differences in the population doubling time and inherent biological characteristics) as well as inconsistent cell health or passage number. In the context of the assay, cell seeding density was critical\u2014with a narrow range used (e.g. 5.0 \u00d7 10\u00b3\u20131.0 \u00d7 10\u2074 cells per well) yielding different dose\u2010responses when not standardized (Source: Optimization of cell viability assays to improve replicability and reproducibility).\n\nCritical consumables and potential batch effects include the culture medium components and serum. For instance, the use of fetal bovine serum (FBS) or serum replacements can introduce batch effects. Even though changes in FBS concentration (0\u201320%) had a minimal effect on drug sensitivity after 24 hours, different batches of FBS (or any serum replacement) could still contribute to variability. Moreover, the choice of culture microplates is important: the supplier and format (such as flat-bottom culture plates versus PCR plates) influence evaporation rates and edge effects\u2014factors that can alter drug concentrations and ultimately affect cell viability (Source: Optimization of cell viability assays to improve replicability and reproducibility).\n\nIn addition, handling steps play a key role. Special attention must be paid during pipetting to ensure consistent volumes, avoid delays that might allow for evaporation or temperature shifts, and carefully perform cell washing and medium replacement. For example, using a single DMSO control (instead of matched DMSO concentration controls for each drug dose) produced variability and dose-response curves starting at >100% viability. Excluding edge wells (or filling them with PBS) was also recommended to avoid the increased evaporation seen in those locations (Source: Optimization of cell viability assays to improve replicability and reproducibility).\n\nFor genetically modified cells or microphysiological systems, there are additional considerations. These include dependencies on matrix chemistry and geometry (which can affect adhesion, proliferation, and drug response), the potential need for supplemental extracellular matrix factors, and the influence of microfluidics parameters (such as flow rate and shear stress) that may result in different responses between surface versus core cells (Source: Optimization of cell viability assays to improve replicability and reproducibility). \n\nRecommendations to increase reproducibility and performance include:\n\u2022 Standardizing cell source and handling: use cells at comparable passages with uniform health status; strictly control seeding density and avoid high confluency.\n\u2022 Minimizing batch effects: use the same batch of FBS or serum replacement and maintain consistency in the type and supplier for consumables such as culture medium, trypsin, and even the microplate type (both format and supplier).\n\u2022 Optimized handling procedures: employ precise pipetting techniques, work quickly to minimize evaporation during critical steps (e.g. during cell plating and drug dilution), and use matched vehicle controls.\n\u2022 For microphysiological models: carefully design the matrix or microfluidics system to ensure consistent exposure of both surface and core cells, and adjust the protocol to accommodate any genetic modifications that might alter cell behavior.\n\nThese measures help to control for biological variability and technical inconsistencies, thereby increasing the replicability and reproducibility of cell-based drug sensitivity assays. (Sources: Optimization of cell viability assays to improve replicability and reproducibility)"
        },
        {
          "question_id": 25,
          "question_text": "Describe the test system as it is used in the test. If the generation of the test system involves differentiation steps or complex technical manipulation (e.g. formation of microtissues), this is described in 3.6. Give details on the general features/principles of the culture protocol (collagen embedding, 3D structuring, addition of mitotic inhibitors, addition of particular hormones/growth factors, etc.) of the cells that are used for the test. What is the percentage of contaminating cells; in co-cultures what is the percentage of each subpopulation? Are there subpopulations that are generally more sensitive to cytotoxicity than others, and could this influence viability measures? Is it known whether specific chemicals/chemical classes show differential cytotoxicity for the cell sub-populations used?",
          "answer_text": "The test system is based on conventional two\u2010dimensional (2D) cell culture models using four well\u2010characterized breast cell lines (MCF7, HCC38, MCF\u201110A, and MDA\u2011MB\u2011436). These cells are maintained as monolayers in standard 96\u2011well culture plates under a humidified 5% CO\u2082 atmosphere at 37 \u00b0C and are seeded, for example, at 7.5 \u00d7 10\u00b3 cells per well in medium supplemented with 10% fetal bovine serum. There are no steps involving differentiation or complex technical manipulations such as collagen embedding, formation of microtissues, or the addition of mitotic inhibitors/hormones/growth factors beyond the standard culture supplements described. Furthermore, the cell lines are used as pure cultures so that issues related to contaminating cells or co\u2011cultured subpopulations (including their relative percentages or differential sensitivities) are not applicable in this assay. While the inherent differences among the cell lines are noted (e.g. differences in drug potency metrics such as IC\u2085\u2080, GR50, and GRmax), there are no reported data on differential responses among subpopulations within a given cell line. \n\n(Source: Optimization of cell viability assays to improve repilicability and reproducibility.pdf)"
        },
        {
          "question_id": 26,
          "question_text": "What are the endpoint(s) that you use to control that your culture(s) is/are as expected at the start of toxicity testing (e.g. gene expression, staining, morphology, responses to reference chemicals, etc.)? Describe the acceptance criteria for your test system, i.e. the quality criteria for your cells/tissues/organoids: Which endpoints do you consider to describe the cells or other source material, which parameters are important? Describe the (analytical) methods that you use to evaluate your culture (PCR, ATP measurement) and to measure the acceptance criteria (AC). Which values (e.g. degree of differentiation or cell density) need to be reached/should not be reached? Historical controls: How does your test system perform with regard to the acceptance criteria, e.g. when differentiation is performed 10 times, what is the average and variation of the values for the acceptance criteria parameters)? Indicate actions if the AC are not met. Examples: cell are > 90% viable, or > 98% of cells express marker x (e.g. AP-2), or > 80% of the cells attach, etc.",
          "answer_text": "The paper does not describe \u201comics\u2010 or staining\u2010based characterization\u201d as endpoints to document the cell source per se. Instead, the authors rely on a set of quality control metrics for their resazurin\u2010based viability assay to verify that the cells are in the expected growth phase (i.e. not in plateau; \u201cexponential\u201d growth is required) and that the assay conditions do not compromise the baseline cell health before beginning toxicity testing. For example, before a drug is applied, the cells are plated at a fixed density (7.5\u00a0\u00d7\u00a010\u00b3 cells per 96\u2010well in 100\u00a0\u00b5l growth medium supplemented with 10% FBS) so that they are in the exponential phase during the experiment. In addition, the assay is controlled by using:\n\n\u2022\u2003Matched DMSO vehicle controls (instead of a single DMSO control) so that the baseline viability, calculated using the resazurin reduction assay, is set at 100%.\n\n\u2022\u2003Assay quality control metrics (acceptance criteria) that include a Z\u2010factor (optimally >0.4, with most optimized experiments showing values >0.75), a signal window (SW; >2 SD, typically >10 in the optimized assay) and a coefficient of variation (CV; acceptable if <20% and in optimized experiments typically <5%). These endpoints indicate that the dynamic range and the reproducibility (low variability) of the viability measurements are sufficient for robust toxicity evaluation.\n\nThe \u201cacceptance criteria\u201d for the assay are essentially that (a) untreated or vehicle\u2010treated control wells yield consistent \u201c100%\u201d viability readings (with small error bars) and (b) the QCM metrics fall within the recommended thresholds (e.g. Z\u2010factor near unity, SW\u2009>\u20092, and CV\u2009<\u200920%\u2014with optimized data often showing CV below 5%, SW greater than 10, and Z-factor close to 1). In a set of historical experiments (e.g. when the assay was repeated at least three independent times), the intra\u2011 and inter\u2011experimental variability were low, thereby supporting that the baseline cell condition was acceptable. If these criteria are not met\u2014for example, if baseline viability is substantially >100% (indicating issues with control dilution of DMSO) or if QCM metrics are out of range (e.g. a high CV or a low Z\u2011factor)\u2014the protocol calls for investigating and correcting potential causes (such as edge effects, evaporation in perimeter wells, or improper cell seeding density) before proceeding with toxicity testing.\n\n(Answer derived from: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 27,
          "question_text": "Describe the acceptance criteria for your test system, i.e. the quality criteria for your cells/tissues/organoids: Which endpoints do you consider to describe the cells or other source material, which parameters are important? Which values (e.g. degree of differentiation or cell density) need to be reached/should not be reached? Historical controls: How does your test system perform with regard to the acceptance criteria, e.g. when differentiation is performed 10 times, what is the average and variation of the values for the acceptance criteria parameters)? Indicate actions if the AC are not met. Examples: Usual neurite length is 50 \u00b115 \u03bcm; experiments with average neurite length below 25 \u03bcm in the negative controls (NC) are discarded. Usual nestin induction is 200 \u00b140 fold, experiments with inductions below 80-fold for NC are discarded.",
          "answer_text": "The optimized drug\u2010screening system is only accepted when the assay quality control metrics meet defined thresholds. For example, before using the viability data for further analysis, cells must be plated at the optimized density (7.5 \u00d7 10\u00b3 cells per well in 100 \u00b5l growth medium with 10% FBS) and the assay must yield dose\u2011response curves with endpoints that meet the following criteria:\n\n\u2022\u2003Quality Control Metrics: The Z\u2011factor should be above 0.4 (in practice, many experiments yield values >0.75), the signal window (SW) should be greater than 2 (ideally >10), and the coefficient of variation (CV) should be below 20% (typically <5%).  \n\u2003\u2003(See \u201cValidation of the optimized resazurin reduction assay using assay quality control metrics\u201d in the document.)\n\n\u2022\u2003Data Consistency: The viability readouts should not start at >100% in the controls, and any experiment that displays substantial intra\u2011 or inter\u2011experimental variation (indicated by large error bars or inconsistent dose\u2013response curves) is flagged for review.  \n\u2003\u2003(Refer to the sections on \u201cCell viability was negatively affected by evaporation and DMSO solvent\u201d and \u201cControls\u201d in Table 1.)\n\n\u2022\u2003Historical Controls: In our routine performance, multiple independent experiments (e.g. when performed in triplicate across at least 10 repeated assays) have consistently produced average QCM values within the recommended ranges (for instance, a historical average Z\u2011factor approximating 0.75, SW exceeding 10, and CV below 5%). If an experiment shows values outside these ranges (for example, a Z\u2011factor significantly lower than 0.4, SW <2, or a CV >20%), it is discarded and the experiment is repeated with adjustments\u2014such as modifying the cell seeding density, addressing edge effects by excluding perimeter wells, or ensuring that DMSO concentrations are properly matched in the controls.  \n\u2003\u2003(This approach is analogous to discarding experiments where, for example, neurite length falls below an accepted threshold in neural models, as described in the provided examples.)\n\nThese acceptance criteria ensure that the cell-based drug sensitivity screens remain robust and reproducible. Any deviation from the set criteria triggers immediate investigation (e.g. verifying cell culture conditions or assay protocols) and repeat testing until the proper thresholds are consistently met.  \n\u2003\u2003(Sources: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 28,
          "question_text": "Give known causes of variability for final test system state. Indicate critical consumables or batch effects (e.g. plate format and supplier, batch effects of FCS or serum replacement, additives). Indicate critical handling steps, and/or influencing factors identified (e.g. special care needed in pipetting, steps that need to be performed quickly, cell density). Indicate positive and negative controls and their expected values, and accepted deviation within and between the test repeats. Give recommendations to increase/ensure reproducibility and performance.",
          "answer_text": "Below is a summary of known causes of variability in the final test system state and key recommendations based on the provided document:\n\n\u2022 Consumables and Reagent Batch Effects:\n\u2003\u2013 Plate format and supplier: Flat\u2010bottom culture microplates can be prone to evaporation (especially the perimeter wells), whereas PCR plates sealed with aluminum tape showed reduced evaporation. Variability may arise if different plate types or lots are used. _(Source: Optimization of cell viability assays\u2026)_\n\u2003\u2013 Serum or serum replacement: Although different FBS concentrations (0\u201320%) had minimal impact after 24 hours, batch-to-batch variability in FBS or serum replacement could affect proliferation and response. _(Source: Optimization of cell viability assays\u2026)_\n\u2003\u2013 Additives: The use of DMSO as a solvent is critical. A single DMSO vehicle control can lead to variability with dose\u2010response curves showing >100% viability; using matched DMSO controls for each dose minimizes this effect. _(Source: Optimization of cell viability assays\u2026)_\n\n\u2022 Critical Handling Steps and Influencing Factors:\n\u2003\u2013 Pipetting: Accurate and consistent pipetting is essential, especially when preparing dilution series for drugs or solvents like DMSO. Special care is needed to avoid introducing variability through pipetting errors or delays. _(Source: Optimization of cell viability assays\u2026)_\n\u2003\u2013 Drug storage: Variability is introduced if drugs are stored improperly. For example, evaporation during short-term storage (48\u201372 h) in culture plates (even when sealed with Parafilm) can change the concentration of the diluted drug. Using PCR plates with aluminum tape is recommended to reduce evaporation. _(Source: Optimization of cell viability assays\u2026)_\n\u2003\u2013 Cell density: The choice of seeding density is critical. Studies found that 7.5 \u00d7 10\u00b3 cells per well was optimal in 96-well plates. Lower or higher densities can lead to inconsistent viability data due to differences in growth phase or confluency. _(Source: Optimization of cell viability assays\u2026)_\n\n\u2022 Controls and Their Expected Values:\n\u2003\u2013 Negative controls: Matched DMSO vehicle controls should be used for each drug concentration rather than a single DMSO control, ensuring that the baseline viability is close to 100% with minimal variation. _(Source: Optimization of cell viability assays\u2026)_\n\u2003\u2013 Positive controls: Although not noted as \u201cclassic\u201d cytotoxic controls, including well-characterized cell lines (e.g. HCC38, MCF7, MCF-10A, MDA-MB-436) treated with standard drugs (e.g. bortezomib or cisplatin) provides reference potency values (IC50, GR50, GRmax). Expected assay quality control metrics are: Z-factor > 0.4 (ideally approaching 1), signal window (SW) > 2, and coefficient of variation (CV) below 20% (often below 5% in optimized tests). _(Source: Optimization of cell viability assays\u2026)_\n\u2003\u2013 Accepted deviation: Within-run and between-run variability should remain within the quality control thresholds mentioned above. Deviations outside these ranges indicate inconsistent assay performance.\n\n\u2022 Recommendations to Increase Reproducibility and Performance:\n\u2003\u2013 Standardize consumables by using the same lot numbers for FBS/serum replacements and selecting plate types that reduce evaporation (e.g. PCR plates with aluminum sealing tape or excluding perimeter wells and filling them with PBS). _(Source: Optimization of cell viability assays\u2026)_\n\u2003\u2013 Maintain strict pipetting practices and quick handling, especially when preparing drug dilutions in viscous solvents like DMSO. This minimizes errors and evaporation-related concentration changes. _(Source: Optimization of cell viability assays\u2026)_\n\u2003\u2013 Use optimized cell seeding density (7.5 \u00d7 10\u00b3 cells per well in 100 \u00b5l growth medium with 10% FBS) to ensure cells remain in exponential growth without reaching confluency. _(Source: Optimization of cell viability assays\u2026)_\n\u2003\u2013 Employ matched solvent (DMSO) controls for each drug dose to avoid overestimation of viability and reduce intra- and inter-experimental variability. _(Source: Optimization of cell viability assays\u2026)_\n\u2003\u2013 Monitor assay quality using QCM parameters (Z-factor, SW, CV) and repeat tests only if these metrics fall within the recommended thresholds. Consistent application of such quality control enhances interlaboratory reproducibility. _(Source: Optimization of cell viability assays\u2026)_\n\nBy addressing these critical consumables, handling steps, and control implementations\u2014and by adhering to strict quality measures\u2014it is possible to significantly improve both the performance and reproducibility of the cell-based viability assay."
        },
        {
          "question_id": 31,
          "question_text": "Give information on where the test system differs from the mimicked human tissue and which gaps of analogy need to be considered.",
          "answer_text": "The assay uses conventional 2D cell culture conditions that do not fully capture the complexity of human tissue. In human tissue, cells exist in a three-dimensional environment with complex cell\u2013cell and cell\u2013matrix interactions, exposure to dynamic nutrient and waste gradients, and influences from stromal, immune, and vascular components. In contrast, the assay\u2019s 2D monolayer conditions lack these aspects, which means that factors such as tissue architecture, the tumor microenvironment, and metabolic dynamics are not reflected. These gaps include the absence of multicellular interactions and extracellular matrix components, as well as the static nature of the culture, which may affect cell proliferation, drug penetration, and overall drug response compared to actual human tissue (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)."
        },
        {
          "question_id": 34,
          "question_text": "Provide an exposure scheme (graphically, show timelines, addition of medium supplements and compounds, sampling, etc.), within the context of the overall cell culture scheme (e.g. freshly re-plated cells or confluent cells at start, certain coatings, etc.). Include medium changes, cell re-plating, whether compounds are re-added in cases of medium change, critical medium supplements, etc.",
          "answer_text": "Below is one version of an exposure scheme based on the optimized resazurin assay described in the document (see Fig.\u00a02 and Methods sections):\n\n\u2022 Day 0 \u2013 Plating and Setup:\n\u2003\u2013 Freshly plate cells in 96\u2010well flat\u2010bottom culture plates. For example, seed 7.5\u202f\u00d7\u202f10\u00b3 cells in 100\u202f\u00b5l of growth medium supplemented with 10% FBS. No special coating is used; the cells are plated on standard tissue culture-treated plates.  \n\u2003\u2013 Cells are allowed to adhere and enter the exponential phase (non-confluent) overnight.  \n\u2003\u2003_(Source: Methods and Table 1 in Optimization of cell viability assays\u2026pdf)_\n\n\u2022 Day 1 \u2013 Compound/DMSO Treatment:\n\u2003\u2013 Approximately 24 hours after plating, add the pharmaceutical compounds. These agents (e.g. bortezomib or cisplatin) are first dissolved in DMSO (with matched vehicle control wells containing equivalent DMSO concentrations) and then diluted in 1\u00d7PBS to the desired working concentrations before addition to the wells.\n\u2003\u2013 Do not change the medium before compound addition; the compounds are simply added to the existing 100\u202f\u00b5l growth medium.  \n\u2003\u2013 Under these optimized conditions no medium renewal is performed during the exposure period, and compounds are not re\u2010added if a medium change were to occur.  \n\u2003\u2003_(Source: Results and Methods sections in Optimization of cell viability assays\u2026pdf)_\n\n\u2022 Day 1\u2013Day 2 \u2013 Incubation and Exposure:\n\u2003\u2013 After addition of compounds at T = +24 hours, incubate the cells for the desired drug exposure period (commonly 24 hours; in some experiments, exposure up to 48 or 72 hours has been tested).  \n\u2003\u2013 No medium change or re-plating is performed during the drug exposure period. Maintaining the medium (with its critical supplement = 10% FBS) ensures that the experimental conditions remain consistent and minimizes variability.\n\u2003\u2003_(Source: Assay optimization paragraphs and Table 1 in Optimization of cell viability assays\u2026pdf)_\n\n\u2022 Day 2 \u2013 Sampling (Resazurin Assay):\n\u2003\u2013 At the end of the exposure period (e.g. after 24 hours of drug treatment), add a resazurin solution. Typically, 10\u202f\u00b5l of a 10% (w/v) resazurin solution is added to each well (representing 10% of the cell culture volume).\n\u2003\u2013 Incubate the plate at 37\u00b0C for at least 4 hours to allow conversion to resorufin.\n\u2003\u2013 After incubation, measure absorbance (or fluorescence) to quantify cell viability.\n\u2003\u2003_(Source: Methods section on Resazurin-based cell viability assay in Optimization of cell viability assays\u2026pdf)_\n\n\u2022 Graphical Timeline Summary:\n\n\u2003\u2003Time Axis \u2192\n\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003T = \u201324 hr              T = 0              T = +24 hr               T = +28 hr\n\u2003\u2003\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\n\u2003\u2003Day \u20131: Cells in culture (freshly plated, 7.5\u00d710\u00b3 cells/well in 100 \u00b5l medium + 10% FBS)\n\u2003\u2003(no coating required; cells adhere and spread)\n\u2003\u2003Day 0: Cells incubate overnight in the same medium (no medium change)\n\u2003\u2003T = 0 (~24 hr post-plating): Add drug compounds (prepared in DMSO and diluted in PBS) with matched DMSO vehicle controls. No medium change; compounds are added directly.\n\u2003\u2003T = +24 hr: End of drug exposure period.\n\u2003\u2003T = +24 hr: Add 10% resazurin solution to each well.\n\u2003\u2003T = +28 hr: (After \u22654 hr incubation with resazurin) Sample absorbance/fluorescence for cell viability readout.\n\nThis scheme illustrates that cells (freshly plated and in exponential growth) are treated with compounds only once and maintained without medium renewal during the exposure period. Critical factors such as the 10% FBS supplement, matched DMSO controls, and avoidance of edge effects (by not changing the medium or re-adding compounds mid-exposure) are maintained to ensure assay replicability and reproducibility.\n\n_(Sources: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens.pdf)_"
        },
        {
          "question_id": 35,
          "question_text": "Define the specific endpoint(s) of the test system that you use for toxicity testing (e.g. cytotoxicity, cell migration, etc.). Indicate whether cytotoxicity is the primary endpoint. What are secondary/further endpoints? Also describe here potential reference/normalization endpoints (e.g. cytotoxicity, protein content, housekeeping gene expression) that are used for normalization of the primary endpoint.",
          "answer_text": "The assay\u2019s primary endpoint is cytotoxicity, as measured by cell viability using the resazurin reduction assay. In this test, the conversion of resazurin (a non\u2011fluorescent dye) to its fluorescent and colorimetric product, resorufin, directly indicates cell metabolic activity and viability, making cytotoxicity the focal parameter.\n\nSecondary endpoints include dose\u2013response metrics such as IC50, GR50, and GRmax, which are used to evaluate drug potency and efficiency. In addition, the assay includes further biochemical and cellular endpoints\u2014for example, inhibition of proteasome activity (assessed after short exposure to bortezomib) and cell cycle analysis (e.g. detecting G2/M arrest in response to treatment) to support the cytotoxicity data.\n\nFor normalization of the primary endpoint (cell viability), the assay employs reference endpoints based on matched vehicle controls. Specifically, absorbance values from DMSO vehicle control wells (matched to the concentration present in each drug-treated condition) are used along with background signal measurements (wells without cells). In practice, the percentage viability is calculated as follows: \n\n\u2003\u2003Percentage viability = 100% \u00d7 ([Absorbance of treated cells] \u2013 [Background absorbance]) / ([Absorbance of matched DMSO controls] \u2013 [Background absorbance]).\n\nThis approach ensures that variations due to solvent effects and baseline optical signals are accounted for, thereby normalizing the cell viability measurements.\n\n(Sources: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 36,
          "question_text": "Define and describe the principle(s) of the analytical methods used. Provide here a general overview of the method\u2019s key steps (e.g. cells are fixed or not, homogenized sample or not, etc.), sufficient for reviewers/regulators to understand what was done, but not in all detail for direct repetition. If you have two or more endpoints (e.g. viability and neurite outgrowth), do you measure both in the same well, under same conditions in parallel, or independently of each other? For imaging endpoints: Explain in general how quantification algorithm or how semi-quantitative estimates are obtained and how many cells are imaged (roughly).",
          "answer_text": "The assay is based on the resazurin reduction method to assess cell viability. In brief, cells are grown in 96\u2010well plates under defined culture conditions, and after treatment (generally for 24 hours), a resazurin solution (added as 10% of the culture volume) is applied without any fixation or homogenization. Viable cells metabolically convert non-fluorescent resazurin to the fluorescent and colorimetric product resorufin during an incubation period (typically around four hours). The resulting signal in each well is then measured using a plate reader via absorbance (using a 560\u202fnm excitation/615\u202fnm emission setup) or fluorescence detection. Only this viability endpoint is measured per well under the same conditions, so no additional endpoints (for example, imaging-based neurite outgrowth) are performed in parallel in the same well. For imaging-based endpoints\u2014which are not used in this study\u2014quantification would generally involve algorithms to segment and count cells or measure fluorescence intensity across a defined number of cells, but here the reliable spectrophotometric readout is sufficient to evaluate metabolic activity and, by extension, cell viability. \n\n(Sources: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens.pdf)"
        },
        {
          "question_id": 38,
          "question_text": "MCC are chemicals/manipulations that show biologically plausible changes of the endpoint. List such controls (up to 10), indicate why you consider them as MCC, and describe expected data on such controls. Highlight the compounds to be used for testing day-to-day test performance, i.e. for setting acceptance criteria (AC). If available, indicate MCC that each increase or decrease the activity of the relevant pathway. Do pathway inhibitions or activations correlate with the test method response?",
          "answer_text": "Below is an example list of MCC\u2010type controls that can be applied to a resazurin\u2010based cell viability assay for cancer drug testing \u2013 along with the rationale for their inclusion, the expected data trends, and the identification of candidates for day\u2010to\u2010day performance testing (i.e. those used to set acceptance criteria):\n\n1. Bortezomib  \n\u2003\u2022 Why MCC: It is a well\u2010characterized proteasome inhibitor that consistently reduces cell viability, induces G2/M arrest, and quickly inhibits proteasome activity.  \n\u2003\u2022 Expected data: A clear, dose\u2010dependent reduction in the resazurin signal; reproducible IC50, GR50, and GRmax values; and a matching drop in proteasome activity measured two hours after treatment.  \n\u2003\u2022 Performance control: This compound is recommended as a day\u2010to\u2010day AC candidate because its effects are robust and reproducible across runs.  \n\u2003\u2022 Pathway effect: Its inhibition of proteasome activity (a central pathway in protein degradation) correlates with a decrease in the assay\u2019s viability endpoint.  \n\u2003\u2003(Source: \u201cOptimization of cell viability assays\u2026\u201d \u2013 sections on bortezomib results and Figure 6)\n\n2. Cisplatin  \n\u2003\u2022 Why MCC: A DNA\u2010crosslinking agent widely used in cancer drug screening that reliably decreases cell viability in a dose\u2010dependent manner.  \n\u2003\u2022 Expected data: Predictable decreases in resazurin conversion with defined IC50/GR50 values, even though the absolute potency may differ slightly from published datasets.  \n\u2003\u2022 Pathway effect: Although its mode of action involves DNA damage rather than a direct \u201con\u2013off\u201d pathway switch, the reduction in viability still correlates with expected cytotoxic effects.  \n\u2003\u2003(Source: \u201cOptimization of cell viability assays\u2026\u201d \u2013 results describing cisplatin dose-response curves and GR metrics)\n\n3. Carboplatin  \n\u2003\u2022 Why MCC: Similar in chemical class and mode of action to cisplatin, it serves as a useful comparator in platinum-based drug screens.  \n\u2003\u2022 Expected data: Dose\u2013response data showing a decrease in cell viability, with trends comparable to cisplatin, albeit sometimes with a right-shifted (less potent) IC50 curve.  \n\u2003\u2022 Pathway effect: As a DNA-damaging agent, its impact on viability follows the expected damage-response outcome.  \n\u2003\u2003(Source: \u201cOptimization of cell viability assays\u2026\u201d \u2013 sections comparing platinum agents)\n\n4. Matched DMSO Vehicle Controls  \n\u2003\u2022 Why MCC: Although not a \u201cdrug\u201d per se, using a range of DMSO concentrations is critical because DMSO itself may affect cell viability when exceeding ~1% (v/v).  \n\u2003\u2022 Expected data: Matched vehicle controls should yield near-baseline resazurin signal (close to 100% viability) at low, non-toxic concentrations; any deviations at higher concentrations can confirm the assay\u2019s sensitivity to solvent-induced effects.  \n\u2003\u2022 Performance control: These are essential for setting acceptance criteria when drugs are dissolved in DMSO, ensuring that each drug dose is compared to a matched solvent control.  \n\u2003\u2022 Pathway effect: Changes in DMSO concentration do not modulate a defined biochemical pathway but serve as an internal control for experimental bias that might mimic pathway \u201cactivation\u201d (if viability is overestimated) or suppression.  \n\u2003\u2003(Source: \u201cOptimization of cell viability assays\u2026\u201d \u2013 discussion and Figure 3d on DMSO effects)\n\n5. Serum Condition Manipulation (e.g. Growth Medium +10% FBS vs. HuMEC Serum-Free Medium)  \n\u2003\u2022 Why MCC: Changing medium composition is known to affect cell proliferation and drug sensitivity; using two defined conditions can serve as a manipulation control.  \n\u2003\u2022 Expected data: In medium with 10% FBS, cells should exhibit a stable, reproducible growth rate with consistent resazurin signals, whereas serum\u2010free conditions may alter the baseline leading to different dose-response curves.  \n\u2003\u2022 Pathway effect: Although not a direct pathway inhibitor/activator, this manipulation highlights the influence of growth factor signaling on viability, which in turn is reflected in the assay outcome.  \n\u2003\u2003(Source: \u201cOptimization of cell viability assays\u2026\u201d \u2013 Table 1 and related discussion)\n\n6. Controlled Evaporation (Plate-edge vs. Center Wells)  \n\u2003\u2022 Why MCC: Evaporation is a technical manipulation that affects the concentration of drugs and components; using edge wells versus central wells as controls can demonstrate how evaporation alters viability measurements.  \n\u2003\u2022 Expected data: Perimeter wells are expected to show higher resazurin readings (often >100% apparent viability) compared to center wells.  \n\u2003\u2022 Performance control: In everyday assay runs, exclusion or special handling of perimeter wells (or the use of PBS-filled edge wells) serves as a quality control measure.  \n\u2003\u2022 Pathway effect: This is a technical rather than a biochemical manipulation; however, the observed changes in resazurin signal occur regardless of actual biological pathway status, underscoring the importance of controlling for non-biological factors.  \n\u2003\u2003(Source: \u201cOptimization of cell viability assays\u2026\u201d \u2013 Figures 3b and 3c)\n\n7. Seeding Density Manipulation  \n\u2003\u2022 Why MCC: Variation in cell number (e.g. 5.0 \u00d7 10^3, 7.5 \u00d7 10^3, versus 1.0 \u00d7 10^4 cells per well) impacts the dynamic range and consistency of assay readouts.  \n\u2003\u2022 Expected data: An optimal seeding density (e.g. 7.5 \u00d7 10^3 cells per well) produces stable, low-variation dose-response curves, whereas too few or too many cells can shift the baseline signal and widen error bars.  \n\u2003\u2022 Performance control: Standardizing cell density is a control for assay variability rather than a chemical effect per se, yet it is critical for consistent performance day-to-day.  \n\u2003\u2022 Pathway effect: Again, while this does not \u201cactivate\u201d or \u201cinhibit\u201d a specific biochemical pathway, it ensures that any pathway-related response to drugs is measured against a reproducible and optimal baseline.  \n\u2003\u2003(Source: \u201cOptimization of cell viability assays\u2026\u201d \u2013 Table 1 and associated discussion)\n\nSummary of Correlation between Pathway Modifications and Test Readout:\n\u2022 Inhibitors like bortezomib cause a direct impact on their target pathways (proteasome activity), which correlates with reduced cell viability on the resazurin assay.  \n\u2022 For cytotoxic agents such as cisplatin and carboplatin, the degree of DNA damage (and subsequent cell death) is reflected in a decrease in the resazurin signal.  \n\u2022 Although not every manipulation (e.g. DMSO levels, cell density, or evaporation effects) acts on a defined biological pathway, the resultant changes in assay output underscore the sensitivity of the test method to both biological and technical factors.\n\nOverall, the most robust performance controls for day-to-day assay acceptance criteria are bortezomib (for its reproducible proteasome inhibition and viability reduction) and matched DMSO controls (to account for vehicle effects), as these offer clear, quantifiable changes that correlate with the underlying biological response measured in the assay.\n\n(Sources: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens.pdf)"
        },
        {
          "question_id": 39,
          "question_text": "What chemicals/manipulations are used as positive controls? Describe the expected data on such controls (signal and its uncertainty)? How good are in vivo reference data on the positive controls? Are in vivo relevant threshold concentrations known?",
          "answer_text": "The paper does not describe a separate \u201cpositive control\u201d reagent per se. Instead, it uses well\u2010characterized chemotherapeutic agents that are expected to induce a reproducible reduction in cell viability as internal controls for assay performance. For example, bortezomib (a proteasome inhibitor) and platinum\u2010based drugs (cisplatin and carboplatin) are used in the screening experiments. These drugs serve as reference compounds because their dose\u2010dependent effects on cell viability are well documented, and they are routinely used to benchmark assay performance and reproducibility. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)\n\nRegarding the expected data on these reference compounds, the optimized assay settings yielded robust quality control metrics. When the assay is working optimally with these drugs, the dose\u2013response curves show:\n\u2022 A large signal dynamic range (for instance, a signal window [SW] above 10, meaning that active cytotoxic effects can be clearly distinguished from the background),  \n\u2022 Low variability (with coefficient of variation [CV] values typically below 5%), and  \n\u2022 High assay quality as indicated by Z\u2010factor values close to 1 (most data points in the study had Z\u2010factors well above the 0.4 cutoff).  \n\nIn practical terms, a positive control using one of these drugs would be expected to reproducibly result in a significant reduction in cell viability with little run\u2010to\u2010run variability (small uncertainty) under the optimized conditions. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)\n\nAs for the in vivo reference data on these putative controls, the study notes that while clinical data (e.g. pharmacokinetic profiles and half\u2011lives) are available for drugs like bortezomib and cisplatin, there remains considerable variability across studies and patient populations. In other words, although in vivo data on drug action and dosage exist, establishing a direct \u201cthreshold\u201d concentration that correlates with a positive in vitro response is challenging. The paper discusses that the half\u2011life and effective concentration in patients may range substantially (for bortezomib, proteasome inhibition was noted at around 10 nM in vitro with near\u2010complete inhibition at 100 nM, and the in vivo half\u2011life in patients is on the order of 20\u201330 hours). However, in vivo relevant threshold concentrations are not clearly defined, and the conversion between in vitro effective doses and in vivo concentrations is not straightforward. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 40,
          "question_text": "What chemicals/manipulations are used as negative controls? Describe the expected data on such controls (signal and its uncertainty)? (Such data define the background noise of the test method) What is the rationale for the concentration setting of negative controls? Do you use unspecific controls? If yes, indicate the compounds and the respective rationale for their use and the concentration selection.",
          "answer_text": "The study uses matching vehicle\u2010controls as the negative control. In practice, this means that whenever drugs are added in DMSO (with the final v/v concentration varying along the dilution series), a corresponding well is set up in which only the appropriate concentration of DMSO is added (instead of drug). This matched DMSO control is essential because a single \u201cfixed\u201d 1% DMSO control was found to lead to dose\u2013response curves with starting viabilities >100% and large error bars\u2014as the toxic effect of DMSO is dose\u2010dependent (see Fig.\u00a03d and Table\u00a01 in the document). Thus, by using a DMSO control matched to each drug dilution, the background (i.e. the unspecific signal associated with the solvent alone) is reliably defined, yielding a \u201cbaseline\u201d signal of approximately 100% viability with minimal uncertainty (CV, SW and Z\u2010factor values all within recommended ranges).  \n\nIn addition, unspecific controls were also used. For example, cells were incubated with PBS or only culture medium (plus 10% FBS) to verify that no cross\u2010reactivity occurs between the resazurin reagent and the tested compounds and to serve as a measure of background absorbance. The concentration (or rather, the condition) for these unspecific controls is chosen to mimic the non\u2010perturbed assay environment: for DMSO, concentrations are kept at less than 1% (v/v) because even low concentrations can affect cell viability (as shown in Fig.\u00a03d), and PBS is used to check that any signal observed is not due to interference by the drug diluent itself. The rationale here is to ensure that any change in measured viability is due to the drug\u2019s effect and not due to non\u2010specific chemical or handling effects.  \n\n(Sources: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 42,
          "question_text": "Which rule do you apply to test whether a test run is within the normal performance frame? How do you document this decision? Indicate actions if the AC are not met.",
          "answer_text": "The assay performance is evaluated using quality control metrics (QCM), meaning that we test every run against the following acceptance criteria: a Z\u2010factor greater than 0.4, a signal window (SW) exceeding 2, and a coefficient of variation (CV) less than 20% (Sources: Results section, Table 1, Figure 5 of the document). \n\nThis decision is documented by recording the calculated QCM values for each run in the laboratory records (for example, in summarized tables and plots generated with ggplot2) so that any deviation from the established thresholds is clearly noted.\n\nIf the AC are not met\u2014that is, if the Z-factor is below 0.4, SW is less than 2, or CV exceeds 20%\u2014the run is flagged as out of the normal performance range. In such cases, a root cause analysis is performed to identify potential confounding factors (such as evaporation effects, mismatched DMSO controls, or cell seeding issues) and the assay is either repeated or optimized further before the data are accepted (Sources: Discussion and Methods sections)."
        },
        {
          "question_id": 43,
          "question_text": "Indicate 'real data points per month' (not per week/per quarter, etc.): count three working weeks per month. Each concentration is a data point. Necessary controls that are required for calibration and for acceptability criteria are NOT counted as data points. All technical replicates of one condition are counted as one single data point (see notes for explanation) Indicate possibility/extent of repeated measures (over time) from same dish. Explain your estimate.",
          "answer_text": "Based on the study\u2019s assay design, one can estimate the following:\n\n\u2022 In a typical drug\u2010screen experiment the drugs are tested in a dilution series such that, for example, nine different drug concentrations are used (see \u201cPharmaceutical compounds were screened at nine concentrations using a 2\u2010fold dilution series\u2026\u201d in the Methods section). Each of these nine concentrations constitutes one \u201creal\u201d data point even though several wells (technical replicates) are run per concentration\u2014these replicates are later combined and are counted as one single data point. Necessary calibration or control wells (e.g., matched DMSO controls) are not counted.\n\n\u2022 Assuming the lab runs one experiment per working day and that you count three full working weeks per month (\u224815 working days), you would have 15 independent experiments. With nine data points per experiment, that gives roughly 15 \u00d7 9 = 135 real data points per month.\n\n\u2022 Regarding repeated measures from the same dish: In this assay the resazurin readout is typically used as an endpoint measure (e.g. after 24 hours of drug exposure) so that for a drug screen each dish provides one set of concentration\u2010dependent data. In contrast, for experiments monitoring growth or metabolic conversion (as described where the assay was repeated daily over five days to determine doubling time) it is possible to obtain repeated measurements from the same dish. However, in the usual drug sensitivity assay these repeated time\u2010point measures are not considered additional independent data points for the screening because the assay is optimized as an endpoint assay. \n\nThus, in summary, you can expect about 135 \u201creal\u201d data points per month (with each concentration from an experiment counted once), and although in some experimental setups repeated measures over time (e.g. for growth curves) are possible, a standard drug screen readout is typically an endpoint measure from one dish per condition. \n\n(Sources: Optimization of cell viability assays\u2026 Methods section and related protocol details in the document)"
        },
        {
          "question_id": 44,
          "question_text": "Give an overview of the range of volumes, particular lab ware/instruments for dispensing, temperature/lighting considerations, particular media/buffers for dilution, decision rules for the solvent, tests of solubility as stocks and in culture medium, etc. How are compound stocks prepared (fold concentration, verification, storage, etc.)? How are dilutions prepared? What solvent is used? Is filtering used to obtain sterility? How does the final addition to the test system take place? Give details of addition of test compounds to test systems (e.g. in which compartment of compartmentalized cultures, in which volume, before after or during medium change, etc.).",
          "answer_text": "Based on the provided document, here\u2019s a summary of the preparation and handling details for the compounds used in the assay:\n\n\u2022 Compound stocks are prepared by dissolving the poorly water\u2010soluble drugs in DMSO. In this study, platinum agents (cisplatin and carboplatin) were prepared as 10\u202fmM stocks and bortezomib as a 1\u202fmM stock in DMSO. These stocks are made according to the manufacturer\u2019s instructions and then stored at \u201380\u202f\u00b0C. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)\n\n\u2022 For working solutions, the concentrated DMSO stocks are further diluted with 1\u00d7PBS to reach the desired working concentrations. Dilutions are prepared in a 2\u2010fold series and are performed using standard pipetting techniques. The diluted drugs are plated into lab ware such as 96\u2010well PCR plates (for storage at \u201320\u202f\u00b0C) before final use. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)\n\n\u2022 The assay uses 96\u2010well flat\u2010bottom culture microplates (e.g., Falcon TC-treated plates) for cell plating (typically 100\u202f\u00b5l per well). The drugs, once diluted, are added to the cells using pipettes. The document indicates that, when added to cells, the compounds are at room temperature (approximately 18\u201325\u202f\u00b0C). There is no specific mention of automation for dispensing, so it appears manual (or standard pipetting instruments) methods are employed. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)\n\n\u2022 No explicit details are given about testing solubility (either in the stock solution or after dilution in culture medium), nor are there notes regarding additional filtering steps to achieve sterility. Consequently, filtering is not mentioned as part of the protocol, and the decision rule for solvent use is based on the compounds\u2019 low aqueous solubility (hence DMSO is chosen). (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)\n\n\u2022 Other physical conditions include storage of the diluted compounds: although drugs can be stored at 4\u202f\u00b0C or \u201320\u202f\u00b0C for short periods (up to 48\u201372 hours), the risk of evaporation is a noted confounder. Thus, drugs stored in flat-bottom culture microplates are sealed (with Parafilm) or stored in PCR plates sealed with aluminum tape to minimize evaporation. Temperature conditions for cell culture (37\u202f\u00b0C, humidified 5% CO\u2082) and ambient lab temperature (18\u201325\u202f\u00b0C) during compound addition are specified. There is no mention of specific lighting conditions for dispensing. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)\n\n\u2022 Regarding the final addition to the test system, the compounds are added to cells that have been plated (usually in 96\u2010well flat-bottom plates) containing 100\u202f\u00b5l of growth medium (commonly supplemented with 10% FBS). Drugs are added using a dilution series together with matched DMSO vehicle controls. There is no mention that the compounds are added into a specific compartment of a compartmentalized culture system; rather, the addition is made directly into the wells containing pre-plated cells. The document does not specify if the compounds are added before, during, or after a medium change\u2014rather, cells are generally allowed to adhere (24 hours post-plating) prior to drug treatment. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)\n\nIn summary, drug stocks are made at high concentration in DMSO and verified per manufacturer\u2019s instructions, stored at \u201380\u202f\u00b0C, and diluted in 1\u00d7PBS using a 2-fold dilution series (with lab ware such as PCR plates for short-term storage at \u201320\u202f\u00b0C). The final compounds\u2014kept at room temperature\u2014are manually dispensed into 96\u2010well flat-bottom microplates that already contain cells in 100\u202f\u00b5l medium, with matched DMSO controls used to account for solvent effects. Filtering for sterility and additional solubility tests are not explicitly described in the document."
        },
        {
          "question_id": 47,
          "question_text": "How is the concentration range of test compounds defined (e.g. only single concentrations, always 1:10 serial dilutions or variable dilution factors, ten different concentrations, etc.)? Is there a rule for defining starting dilutions? For functional endpoints that may not provide full concentration-response, how is the test concentration defined? E.g. EC10 of viability data are usually tested for gene expression endpoints. Detailed information should also be included in an SOP, preferably in DB-ALM format (see link in 6.6).",
          "answer_text": "The test compounds in the assay are not evaluated at a single concentration but over a dilution range. In our study, compounds were dissolved in DMSO and then diluted with PBS to reach the desired working concentrations. For example, bortezomib was tested from 1 to 10,000\u202fnM and cisplatin/carboplatin from 2 to 1024\u202f\u00b5M. In practice, a nine\u2010point, twofold dilution series was used. There isn\u2019t a \u201cone\u2010size\u2010fits\u2010all\u201d rule for the starting dilution; rather, the initial concentration is chosen based on compound solubility, manufacturer\u2019s instructions, and prior pilot experiments to ensure that the full dynamic range of response can be captured. For functional endpoints where a full concentration\u2013response curve is not achieved (for example, when using viability data to define an EC10 value for gene expression endpoints), the test concentration is chosen from these initial experiments (such as using the EC10) and then validated further. Detailed procedural information and any deviation from these guidelines should be recorded in an SOP, preferably in DB\u2011ALM format as recommended (Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens)."
        },
        {
          "question_id": 48,
          "question_text": "What types of compounds are problematic, e.g. interference with analytical endpoint, low solubility, precipitation of medium components, etc.? What experimental variables are hard to control (e.g. because they are fluorescent)? What are critical handling steps during the execution of the assay? Robustness issues, e.g. known variations of test performance due to operator training, season, use of certain consumable or unknown causes, etc. Describe known pitfalls (or potential operator mistakes).",
          "answer_text": "The study shows that compounds with poor aqueous solubility\u2014such as the platinum agents and proteasome inhibitor used (bortezomib)\u2014require dissolution in DMSO, and that can be problematic. Drugs that must be dissolved in DMSO can raise two issues. First, if DMSO is not properly matched across doses, its inherent cytotoxicity (with cytotoxic effects seen even at 1% v/v) can interfere with the analytical endpoint (e.g. cell viability readouts that may start at >100%). Second, poorly soluble compounds may precipitate or have uneven distribution, especially if evaporation occurs during storage or incubation. For example, evaporation not only concentrates the drug, affecting dose\u2013response curves, but also causes \u201cedge effects\u201d in 96\u2010well plates that lead to inconsistent fluorescent (or absorbance) readouts with the resazurin assay. (Sources: Optimization of cell viability assays\u2026, Fig.\u00a03d and Table\u00a01)\n\nOther experimental variables that are hard to control include:\n\u2022\u2003Evaporation rates in the plates \u2013 the outer wells show higher resazurin-based absorbance because of less evaporation in the center, leading to variable signals.\n\u2022\u2003The use of diluted drugs which, when stored, are prone to evaporation depending on the plate type and sealing method (e.g. flat-bottom culture plates sealed with Parafilm experience more evaporation than PCR plates sealed with aluminum tape). (Sources: Optimization of cell viability assays\u2026, Fig.\u00a03a\u2013c)\n\nCritical handling steps include:\n\u2022\u2003Ensuring that drugs are dissolved and stored properly (e.g. using correct DMSO dilutions, storing at 4 \u00b0C or \u221220 \u00b0C for controlled periods, and using appropriate plate types/seals) to minimize evaporation.\n\u2022\u2003Using matched DMSO concentration controls for each drug dose (instead of a single control) to avoid artifactual viability readings.\n\u2022\u2003Avoiding the use of perimeter wells for experimental measurements by either excluding them or filling them with PBS to control for the edge effect.\n\u2022\u2003Maintaining consistent cell seeding densities and culture volumes because even small deviations can alter drug potency readouts. (Sources: Optimization of cell viability assays\u2026, Methods and Results sections)\n\nRegarding robustness issues and known pitfalls, the assay is vulnerable to operator-induced errors (such as inconsistent pipetting, uneven cell seeding, misapplication of solvent controls, or poor plate sealing) and factors like variations in consumables (different plate types yielding different evaporation rates) or even uncontrolled environmental factors. Such mistakes may lead to signal variation (e.g. curve shifts, high error bars, or viability estimates beginning above 100%), all of which affect replicability and reproducibility across experiments or laboratories. (Sources: Optimization of cell viability assays\u2026, Discussion and Results sections)\n\nIn summary, problematic compounds include those requiring solvents due to low solubility and which can precipitate or interfere with the endpoint if mishandled. Critical environmental and handling variables include evaporation (especially in outer wells), inadequate matching of DMSO controls, and inconsistent cell seeding/plate handling\u2014all factors that can produce robustness issues if not stringently monitored."
        },
        {
          "question_id": 49,
          "question_text": "Ideally the SOP follows the DB-ALM or a comparable format: https://ecvam-dbalm.jrc.ec.europa.eu/home/contribute Refer to additional file(s) (containing information covered in sections 3 and 4), containing all details and explanations. Has the SOP been deposited in an accessible data base? Has the SOP been reviewed externally and if yes, how?",
          "answer_text": "The article does not mention that the SOP was deposited in the DB\u2010ALM or a comparable database. However, all of the experimental details along with the dataset and source codes have been made publicly available through a CodeOcean capsule (https://doi.org/10.24433/CO.8346890.v1) and supplementary files, which together provide the complete experimental protocol and associated explanations (Source: Optimization of cell viability assays\u2026 Scientific Reports, 2020). \n\nIn addition, as the work was published in Scientific Reports, the assay protocol underwent external peer review as part of the standard review process of that journal. There is no indication of any additional external review beyond this peer review process. (Source: Optimization of cell viability assays\u2026 Scientific Reports, 2020)."
        },
        {
          "question_id": 50,
          "question_text": "Does the method require specialized instrumentation that is not found in standard laboratories? Is there a need for custom-made instrumentation or material? Is there a need for equipment that is not commercially available (anymore)?",
          "answer_text": "There is no indication that the assay requires any non-standard or custom-made instrumentation. The method is performed using commonly available equipment such as standard 96-well flat-bottom microplates, commercially available PCR plates, routine microplate readers (e.g., the Wallac 1420 VICTOR2), incubators, and standard flow cytometry instruments. No special or non-commercial equipment is required according to the document.  \n_(Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens.pdf)_"
        },
        {
          "question_id": 51,
          "question_text": "Describe possible variations, modifications and extensions of the test method: a) other endpoints, b) other analytical methods for same endpoint, c) other exposure schemes (e.g. repeated exposure, prolonged exposure, etc.), d) experimental variations (e.g. use of a specific medium, presence of an inhibitor or substrate that affects test outcome, etc.)",
          "answer_text": "a) Other endpoints: In addition to measuring cell viability via the resazurin reduction assay, alternative endpoints could be evaluated. These include using pharmacodynamic biomarkers such as proteasome activity (as measured by the Proteasome-Glo assay), cell cycle arrest (for instance, assessing G2/M phase arrest by flow cytometry), or apoptosis/necrosis markers. One can also quantify drug response using various metrics such as IC50, GR50, GRmax, Emax, AUC, and others to capture different aspects of drug potency and efficiency. (Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens)\n\nb) Other analytical methods for the same endpoint: While the assay described here uses resazurin conversion measured via absorbance (or fluorescence, since both methods produced comparable results), other analytical methods can be employed to determine cell viability. For example, MTT/MTS assays, ATP-based luminescent assays like CellTiter-Glo, or even live-cell imaging based systems could serve as viable alternatives to measure metabolic activity and viability under similar experimental conditions. (Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens)\n\nc) Other exposure schemes: Although the study primarily discusses drug exposure for 24 hours (with comparisons made at 48 and 72 hours), the assay design can be modified to include repeated exposures, prolonged exposures, or even intermittent dosing. These modifications may better mimic clinical treatment regimens or capture cumulative effects over time, thus potentially altering the response metrics (e.g. changes in IC50 or growth rate inhibition values) due to differences in cell doubling times and kinetics of drug action. (Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens)\n\nd) Experimental variations: Variability in experimental design can be introduced through changes in culture conditions and other assay parameters. For example, using alternative media (such as serum-free versus growth medium supplemented with variable FBS concentrations), altering seeding densities, or selecting different plate types (e.g. PCR plates with aluminum sealing tape to reduce evaporation versus traditional flat\u2011bottom culture plates) are all potential modifications. Additional variations include adjusting the concentration of DMSO with matched vehicle controls, incorporating inhibitors or substrates that influence drug metabolism or signaling pathways, and even modifying drug storage conditions prior to application. Such changes can influence assay readouts and should be carefully optimized for each cell type and drug condition.  (Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens)"
        },
        {
          "question_id": 52,
          "question_text": "Indicate the names (and database names) of related tests and give a short description (including a brief comment on differences to the present method). If the test method has been used for high throughput transcriptomics or deep sequencing as alternative endpoint, this should be indicated.",
          "answer_text": "1. MTT Assay (no specific database): This conventional cell viability test uses tetrazolium salt reduction to form a colored formazan product. Unlike the present resazurin\u2010based method\u2014which is non\u2010toxic, allows both absorbance and fluorescence detection, and requires no additional solubilization steps\u2014the MTT assay typically involves extra processing steps that can introduce variability. (Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens.pdf)\n\n2. Pharmacogenomic Screening Databases (e.g., Cancer Cell Line Encyclopedia [CCLE], Cancer Genome Project [CGP], and pharmacoDB): These initiatives collect drug sensitivity data from various cell\u2010based screens. While these databases integrate viability, transcriptomic, and other genomic endpoints, their primary focus in relation to drug response is not based on the direct measurement of cell viability as optimized in the present resazurin assay. Moreover, although such resources offer a broader omics perspective\u2014including high throughput transcriptomics\u2014the present method directly targets cell viability rather than deep sequencing\u2010based endpoints. (Sources: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens.pdf)"
        },
        {
          "question_id": 55,
          "question_text": "How are raw data processed to obtain summary data (e.g. EC50, BMC15, ratios, PoD, etc.) in your lab? Describe all processing steps from background correction (e.g. measurement of medium control) to normalization steps (e.g. if you relate treated samples to untreated controls).",
          "answer_text": "According to the work described, the processing of the raw resazurin assay data in our lab follows these steps:\n\n1. First, raw absorbance is measured in each well using a microplate reader with the appropriate excitation (560 nm) and emission (615 nm) settings.\n\n2. A background correction is performed by subtracting the absorbance values of wells with only medium (medium control or blank) from both treated and control wells.\n\n3. Next, the corrected absorbance from the treated wells is normalized to that of the matched DMSO vehicle (untreated) controls. In our assay the percentage cell viability is calculated as:  \n\u2003\u2003100% \u00d7 (Absorbance_treated \u2013 Absorbance_background) \u00f7 (Absorbance_control \u2013 Absorbance_background).\n\n4. For growth rate analyses, untreated cells are also measured at the time of treatment (t = 0) to assess normalized growth rate inhibition (using GR metrics).\n\n5. The processed and normalized data are then used to generate dose\u2010response curves (fitting the data via nonlinear regression, for example with the GRmetrics package in R/Bioconductor) from which summary metrics such as IC50, GR50, GRmax, or other endpoints (e.g. EC50 if applicable) are determined.\n\n6. In addition, quality control metrics (such as coefficient of variation, signal window, and Z-factor) are calculated to ensure the assay meets robust signal dynamic range criteria before summary metrics are finalized.\n\nAll these steps\u2014from background correction (using medium control wells) to normalizing treated sample data to the untreated DMSO controls\u2014are crucial for transforming raw absorbance values into reliable summary drug response parameters.  \n_(Sources: \u201cResazurin-based cell viability assay\u201d and \u201cStatistical analysis\u201d sections in Optimization of cell viability assays to improve replicability and reproducibility.pdf)_"
        },
        {
          "question_id": 56,
          "question_text": "How are data normally handled to obtain the overall test result (e.g. concentration response fitting using model X, determination of EC50 by method Y, use of EC50 as final data)? How do you model your concentration response curve (e.g. LL.4 parameter fit) and which software do you use (e.g. GraphPad Prism, R, etc.)? Do you usually calculate an uncertainty measure of your summary data (e.g. a 95% confidence interval for the BMC or a BMCL), and with which software? Can you give uncertainty for non-cytotoxicity or no-effect? How do you handle non-monotonic curve shapes or other curve features that are hard to describe with the usual mathematical fit model?",
          "answer_text": "The study\u2019s approach is to take the raw viability data (normalized against matched DMSO controls) for each cell line\u2010drug experiment and to use non\u2010linear regression to generate a concentration\u2013response curve. For example, they fit the data using a logistic (four\u2011parameter, sometimes designated as LL.4) curve in R (with tools such as ggplot2 for plotting and the GRmetrics package for calculating GR50, GRmax, and related values) to then derive endpoints such as the IC50 or GR50. In this work, the IC50 (or GR50) is used as a summary measure of drug potency. \n\nAs for uncertainty, the assay quality control metrics (for instance, the coefficient of variation, signal window, and Z\u2011factor) are used to assess the robustness of the curve and the reliability of the estimated potency values. Error bars (typically standard errors of the mean) are shown in the figures, although the paper does not provide explicit calculation of a 95% confidence interval (e.g. for a benchmark concentration) \u2013 particularly since for non-cytotoxic compounds (or in cases where no effect is reached) a conventional uncertainty measure cannot be reliably determined. \n\nFor curves that are non\u2011monotonic or that display atypical features, the authors note that the usual four-parameter logistic fit may be inadequate. In such cases, these curves are flagged (for example, when an IC50 is \u201cnot reached\u201d) and alternative or qualitative assessments are applied because the standard mathematical models do not capture the behavior accurately.\n\n_(Sources: Optimization of cell viability assays to improve replicability and reproducibility.pdf)_"
        },
        {
          "question_id": 60,
          "question_text": "What is the scientific rationale to link test method data to a relevant in vivo adverse outcome? Which toxicological target (organ, tissue, physiological/biochemical function, etc.) is modelled? Which biological process(es) (e.g. neurite outgrowth, differentiation) are modelled/reflected by your test method? Which toxicological events (e.g. oxidative stress, cell death) are modelled/reflected by your test method? To which (human) adverse outcome(s) is your test method related? Which hazard(s) do(es) your test method (potentially) predict?",
          "answer_text": "The rationale is to use a well\u2010controlled in vitro measure of cell metabolism and viability (by quantifying resazurin reduction) as a surrogate for the cellular damage that underlies adverse outcomes in vivo. In this assay the toxicological target is essentially the tumor cell itself \u2013 in these studies, breast cancer cell lines \u2013 reflecting the health of target tissue in response to anti\u2010cancer drugs. The method models key biological processes such as metabolic activity, cell proliferation, proteasome function, and cell cycle progression (for example, as seen where bortezomib inhibits proteasome activity and induces G2/M arrest). As a result, it reflects toxicological events like cytotoxicity and cell death that can be linked to adverse outcomes (for example, the reduction of tumor mass or the potential off\u2010target toxicity if normal cells are affected). In short, by reliably quantifying changes in these cellular processes, the assay helps predict hazards (e.g. cytotoxicity, mis-dosing effects) that would lead to adverse outcomes in an in vivo setting. \n\n(Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens.pdf)"
        },
        {
          "question_id": 61,
          "question_text": "Provide the statistics of your benchmark response (threshold and variance): (i) For dichotomized data, provide your prediction model. When do you consider the result as toxic or not toxic? (ii) For pseudo-dichotomized outcomes (two classes with borderline class in between): define borderline range. (iii) For multi-class or continuous outcomes: provide definitions and rationale. What is the rationale for your threshold? This can be on a mathematical (e.g. 3-fold standard deviation) or a biological basis (e.g. below 80% viability). Is there a toxicological rationale for the threshold settings and definitions of your prediction model? What are the limitations of your prediction model? What is a \u2018hit\u2019 if the test is used in screening mode (= hit definition, if different from above)?",
          "answer_text": "Below is a summary of how one could specify threshold\u2010 and variability\u2010based definitions in an optimized resazurin viability assay\u2014as described in the paper\u2014acknowledging that the study did not provide a single \u201cprediction model\u201d per se, but rather defined quality control and dose\u2013response parameters that limit technical variability and allow biological effects to be distinguished. The reported performance metrics serve as benchmarks for the assay; note that these are not \u201ctoxicity cut\u2010offs\u201d in a binary classifier but rather criteria to judge the reliability of the continuous dose\u2010response data.\n\n(i) For dichotomized outcomes (toxic versus non\u2011toxic):  \n\u2022 A common approach (although not explicitly set in the article) is to \u201cbin\u201d responses using a viability threshold\u2014for example, one might consider a compound \u201ctoxic\u201d if the measured viability falls below a cut\u2011off such as 80% of the matched vehicle control.  \n\u2022 In the study, rather than establishing a binary prediction model, the authors ensured that the assay was robust by requiring quality control metrics such as a Z\u2011factor > 0.4 (with many data points in their validations achieving >0.75), a signal window (SW) > 2, and a coefficient of variation (CV) < 20% (often <5%). In practice, a treatment response that drops well beyond the assay noise (i.e. outside these variability limits) would be interpreted as \u201ctoxic.\u201d  \n_(Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)_\n\n(ii) For pseudo\u2011dichotomized outcomes (with an intermediate \u201cborderline\u201d class):  \n\u2022 Although the paper does not explicitly define a \u201cborderline\u201d range, a pseudo\u2011dichotomy might be engineered by designating an intermediate viability region. For instance, one could define responses between 80% and 90% viability as \u201cborderline\u201d (indicating that the response is within the range where assay variability could obscure a clear toxic effect). Again, the selection of such a range could be informed by the observed assay standard deviations (e.g. a threefold standard deviation rule) and by controls that have been optimized to minimize noise (with CV values\u2009<\u200920%).  \n_(Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)_\n\n(iii) For multi\u2011class or continuous outcomes:  \n\u2022 The assay outcome is typically expressed as a continuous measure of viability, from which dose\u2013response curves are generated. Key summary parameters such as the IC50 (half\u2011maximal inhibitory concentration), GR50 (growth rate inhibition metric), and GRmax (maximum measured growth rate inhibition) are derived.  \n\u2022 The rationale for thresholds in these cases is two\u2010fold. Mathematically, one can use parameters such as a \u201c3\u2011fold standard deviation\u201d of the control signal to define the dynamic range of the assay. Biologically, one might reason that only a drop in viability below a given level (often around 80% of control, though this value varies with cell type and drug) represents a meaningful cytotoxic response. In this study the authors optimized conditions so that dose\u2013response values were consistent with expected profiles (e.g. HCC38 cells being more sensitive to bortezomib with an IC50 in the low nanomolar range).  \n_(Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)_\n\nRationale for the Thresholds and Prediction Model  \n\u2022 The quality control metrics (Z-factor, SW, CV) were chosen on statistical grounds to ensure that the assay has a wide signal dynamic range and minimal variability (e.g. a Z-factor near 1 is ideal, and the authors obtained values well above the minimum cutoff of 0.4).  \n\u2022 In parallel, a potential toxicological \u201cthreshold\u201d (such as a fixed percentage viability) is sometimes adopted based on biological reasoning. For example, many researchers consider results below ~80% viability as indicative of a drug effect, but this may be adjusted based on the cell type and assay specifics.  \n\u2022 No absolute toxicological threshold was imposed in the paper; rather, the focus was on optimizing experimental parameters so that the derived dose\u2013response metrics (IC50, GR50, GRmax) become reproducible and comparable across laboratories.\n\nLimitations of the Prediction Model  \n\u2022 The assay performance (and hence the prediction of \u201ctoxicity\u201d) is sensitive to several experimental factors: evaporation (with edge effects noted), the use of matched DMSO controls, cell seeding density, and cell\u2011line\u2013specific differences.  \n\u2022 Because the model is based on continuous dose\u2013response data rather than a preset binary classifier, it can be influenced by variations in growth conditions or by small shifts in assay conditions.  \n\u2022 Inter\u2010laboratory differences (even when using optimized parameters) remain a challenge, as slight deviations in protocol (medium, drug storage conditions, incubation times) can alter the calculated IC50 or GR metrics.\n\nDefinition of a \u2018Hit\u2019 in Screening Mode  \n\u2022 In screening mode, a \u201chit\u201d would be defined as a compound that produces a reduction in cell viability (or a shift in the dose\u2013response curve) that is statistically significant relative to the optimized assay noise.  \n\u2022 Typically, a hit would be one whose response falls outside the \u201cnormal\u201d range set by the quality control limits (for example, a treatment that produces a viability decrease exceeding the 3-fold standard deviation of control readings, or one that results in an IC50 or GR50 far lower than that of negative controls).  \n\u2022 In other words, a hit is identified when the compound\u2019s effect on viability is both reproducible (satisfies the QCM thresholds) and \u201cpotent\u201d enough\u2014relative to an established baseline\u2014to be considered biologically active.\n\nTo summarize, while the article does not provide one single \u201cblack\u2010box\u201d prediction model to turn a continuous assay into a binary toxic/non\u2011toxic output, it does lay the groundwork for doing so by (a) establishing stringent quality control thresholds (Z-factor, signal window, CV), (b) carefully controlling the experimental confounders, and (c) calculating quantitative parameters (IC50, GR50, GRmax) that can be interpreted in a toxicological context. The selection of thresholds therefore reflects both mathematical criteria (signal-to-noise, standard deviation rules) and biological reasoning (levels of viability reduction linked to effective drug action), with the understanding that these choices might need to be tailored for different assays or cell types.\n\n_(Sources: Optimization of cell viability assays to improve replicability and reproducibility.pdf)_"
        },
        {
          "question_id": 63,
          "question_text": "Indicate here basic performance parameters or, if possible, preliminary estimates (label as such): Baseline variation (noise) within assays AND between assays. What is the signal/noise ratio (signal = standard positive control)? Is the z-factor determined? Give the specificity of the test method. How is it determined? Give the sensitivity of the test method. How is it determined? Give measures of the uncertainty of your test method. How are they determined? What is the detection limit (required change of endpoint to become measurable)? If available, give limit of detection (LOD) and limit of quantification (LOQ). What are inter-operator variations? Are there data of \u2018historical controls\u2019 over a longer time period?",
          "answer_text": "\u2022 Baseline variation/noise and signal\u2010to\u2010noise ratios:  \n\u2013 The performance quality of the assay was evaluated using quality control metrics. In three independent experiments, the coefficient of variation (CV) was below 5% (with a threshold of <20%), and the signal window (SW) was >10. In other words, the dynamic range (signal) as measured by the positive control (i.e. a standard chemotherapeutic treatment resulting in a strong, reproducible effect) is high relative to baseline noise.  \n\u2013 Additionally, the Z\u2010factor was measured and most data points had Z\u2010factor values >0.75 (with the recommended cutoff being >0.4) _(Source: Optimization of cell viability assays\u2026 PDF, Fig. 5 and accompanying text)_.\n\n\u2022 Specificity of the test method:  \n\u2013 Specificity was established by confirming that the resazurin solution does not show cross-reactivity with the pharmaceutical drugs when incubated with growth medium in the absence of cells. This result demonstrates that the assay specifically reflects viable cells\u2019 metabolic conversion of resazurin to resorufin.  \n\u2013 The assessment was performed by incubating growth medium + 10% FBS (without cells) with bortezomib, carboplatin, or cisplatin and observing no cross\u2010reactivity _(Source: \u201cCross\u2010reactivity with pharmaceutical compounds\u201d description in Table 1 and text)_.\n\n\u2022 Sensitivity of the test method:  \n\u2013 Sensitivity was determined by performing detailed dose\u2013response experiments using several drug concentrations. This allowed detection of small changes in cell viability across different conditions and cell lines.  \n\u2013 Although no single numerical sensitivity threshold is provided, the observation that the assay accurately distinguishes between cell viability levels under varying drug doses (and the ability to detect significant decreases in viability at low drug or solvent concentrations) illustrates the assay\u2019s sensitivity _(Source: text under \u201cCell viability was negatively affected by evaporation and DMSO solvent\u201d and the dose\u2013response data in Figures 3\u20136)_.\n\n\u2022 Measures of uncertainty:  \n\u2013 Uncertainty in the assay is quantified by the CV, SW, and Z\u2010factor metrics as well as variance component analysis via one-way ANOVA. For example, the variance component analysis showed that approximately 90% of the variability was due to the drug and ~5.4% was due to the cell line, indicating that most of the uncertainty derives from the experimental treatment rather than technical noise.  \n\u2013 These metrics reflect the uncertainty in the assay readout and are determined by replicating experiments in triplicate and performing statistical analyses on the data _(Source: Statistical Analysis and Results sections)_.\n\n\u2022 Detection limit (and LOD/LOQ):  \n\u2013 Although the study does not explicitly state a limit of detection (LOD) or limit of quantification (LOQ) as numerical values, the detection limit in practical terms is reflected in the assay\u2019s ability to register changes in viability that exceed the noise level (with a recommended signal window of at least 2\u00d7 the standard deviation, and here SW >10).  \n\u2013 Thus, the required change in the endpoint to be considered measurable is governed by having a SW >2, which in this case was well exceeded _(Source: \u201cValidation of the optimized resazurin reduction assay using assay quality control metrics\u201d section)_.\n\n\u2022 Inter\u2010operator variation and historical controls:  \n\u2013 The article defines replicability as the same analyst re-performing the experiment and reproducibility as different analysts performing the same experiment. However, specific numeric data on inter\u2010operator variation are not provided.  \n\u2013 Similarly, while the assay was repeatedly validated (intra\u2010 and interexperimental reproducibility being discussed), the paper does not include data from \u201chistorical controls\u201d over an extended time period _(Source: General discussion on replicability/reproducibility in the text)_.\n\nIn summary, the assay displays low baseline noise (CV < 5%), a high signal window (>10), and Z-factors mostly above 0.75, with specificity demonstrated by the lack of drug cross\u2010reactivity. Sensitivity is realized through robust dose\u2013response performance, and uncertainty is quantified via statistical quality metrics and ANOVA. Detailed limits of detection (LOD/LOQ) and inter\u2010operator variation data, as well as historical control datasets, were not explicitly reported in the document."
        },
        {
          "question_id": 65,
          "question_text": "Which compounds is the test likely to pick up correctly, where is it likely to fail? How does the test method react to mixtures and UVCBs? Are there areas (according to industry sector, compound chemistry, physical-chemical properties) that need to be excluded from testing, or that are particularly suitable? Which compound class cannot be detected (e.g. neurotransmitters for which the receptors are not expressed, endocrine disruptors in absence of respective pathway)? Are any compounds known to interfere with the test system (e.g. fluorescent or colored chemicals)?",
          "answer_text": "The study was designed and optimized for detecting drugs that reduce cell viability in cancer cell lines (for example, platinum\u2010based agents and bortezomib). Thus, cytotoxic compounds that directly decrease cell metabolic activity are the group most likely to be detected reliably, while agents that act via non\u2010cytotoxic mechanisms or require activation of specific pathways that are absent from the tested cell lines (for example, neurotransmitters or certain endocrine disruptors) are less likely to produce a measurable signal in this assay. \n\nFurthermore, the work does not discuss the behavior for testing complex mixtures or UVCBs. The assay was evaluated using individual, well\u2010characterized pharmaceutical agents, and there is no report on how mixtures would impact the dose\u2013response curve or whether the inherent variability of UVCBs might confound the results. \n\nIn addition, while the optimized conditions (including use of matched DMSO controls and assessment of cross-reactivity) indicated no interference from the tested drugs, there is no explicit mention of fluorescent or colored compounds interfering with the readout. However, because the assay relies on absorption/fluorescence of resazurin conversion, compounds with overlapping optical properties could theoretically pose a problem. \n\nNo specific chemical sectors, compound chemistries, or physical\u2013chemical properties were singled out for exclusion or highlighted as particularly well\u2010suited beyond the focus on cancer drugs. In other words, the assay is likely suitable for a range of cytotoxic compounds but is not intended to capture activity from compounds that require targets or signaling pathways not present in these cell lines. \n\n(Sources: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens \u2013 entire document)"
        },
        {
          "question_id": 66,
          "question_text": "Does the test fit into a test battery? If yes, into which test battery and are there any restrictions? Indicate potential strengths and weaknesses of the system in a test battery (e.g. method is a good confirmation assay, good for creating alerts, mechanistic follow-up, screening, etc.). Compare performance to similar tests. Which gaps in a known or potential battery does the test method fill? Should the test preferentially be used in the first tier or later tiers, are complementary assays required or is it a stand-alone method?",
          "answer_text": "The optimized resazurin cell viability assay is intended to be one component within an integrated test battery for preclinical drug screening. In the context of a battery it can serve as a primary screening tool to flag compound activity based on dose\u2010response curves (via IC50 and growth rate (GR) metrics) while offering robust quality control through metrics such as Z-factor, signal window, and coefficient of variation. This makes the method suitable as a first-tier assay for screening compounds, after which more mechanism\u2010focused tests (for example, assays of proteasome activity or cell cycle analysis) can be applied for mechanistic follow-up.\n\nStrengths of the assay in a test battery are:\n\u2022 It provides high replicability/reproducibility when experimental parameters are optimized (e.g., use of matched DMSO controls, proper sealing to minimize evaporation, and appropriate seeding densities).\n\u2022 Its use of QCM metrics (Z-factor, SW, CV) allows reliable identification of active compounds with a wide dynamic range.\n\u2022 It compares favorably with conventional assays (such as the MTT assay) in that it can yield consistent IC50 and GR values across different cell lines and labs.\n\nWeaknesses or restrictions include:\n\u2022 Sensitivity to technical variables (e.g., evaporation, edge effects) that require careful experimental design (such as excluding perimeter wells or using specialized plate sealing).\n\u2022 Necessity for complementary assays when in-depth mechanistic information is needed (for example, post-screening assays of proteasome inhibition or cell cycle arrest can provide insights that the viability assay itself does not).\n\nCompared to similar tests, the optimized resazurin assay fills the gap of reproducibility and robustness that is often lacking in conventional 2D cytotoxicity assessments. It is particularly useful for standardizing conditions in high-throughput drug screens and ensuring that interlaboratory comparisons are valid.\n\nIn summary, in a test battery the optimized resazurin assay is best used as a first-tier screening method to generate alerts and identify cytotoxic hits in cancer drug sensitivity screens, while subsequent tiers could use complementary or mechanistic assays for confirmation and follow-up studies.\n\n(Sources: Optimization of cell viability assays to improve replicability and reproducibility, Scientific Reports, 2020)"
        },
        {
          "question_id": 67,
          "question_text": "Refer to published literature on the test AND indicate in detail deviations from published descriptions (e.g. plastic plate supplier, cell number, endpoint measurement, timing, etc.). Provide the most relevant publications that describe/give a comprehensive overview of (a) your test system and/or (b) your test method. Describe what aspects are covered therein. Give a prioritized (according to importance) list of further publications on the test method or its application. Give short comments on which type(s) of information can be obtained from these publications (e.g. contains test chemical lists, contains more positive/negative controls, contains validation against other tests, contains incorporation in test battery, demonstrates use by other lab, etc.).",
          "answer_text": "Below is a summary of how our optimized resazurin\u2010based cell viability assay deviates from previously published descriptions, followed by a list of key publications that provide an overview of the test system/test method and further related studies.\n\n1. Deviations from published descriptions  \n\u2003\u2022 Our protocol deviates from earlier descriptions by adjusting several core parameters. For example, whereas some published methods used a seeding density of 1.0\u00d710^4 cells per 96\u2010well with serum\u2010free medium (e.g. HuMEC basal medium) and a single DMSO vehicle control, our optimized protocol uses 7.5\u00d710^3 cells per well in 100\u2005\u00b5l growth medium supplemented with 10% FBS and employs matched DMSO concentration controls. This change addresses the tendency for dose\u2013response curves to start at >100% viability and reduces variability.  \n\u2003\u2022 We also noted that endpoint measurement differs: instead of a shorter incubation time (often 1\u20132 hours) suggested elsewhere, we incubate with 10% (w/v) resazurin for four hours to ensure stable signal generation.  \n\u2003\u2022 Additionally, deviations involve practical aspects such as the choice of plasticware and sealing methods. Whereas earlier protocols might not have detailed the impact of evaporation or specified the plate supplier, we have compared flat\u2010bottom culture microplates (e.g. from VWR) versus PCR plates sealed with aluminum tape and avoided using perimeter wells (or compensated them by filling with PBS) to minimize the edge effect.  \n\u2003\u2022 Storage of diluted drugs also differs: instead of merely storing drugs at 4\u2009\u00b0C, we evaluated storage at both 4\u2009\u00b0C and \u201320\u2009\u00b0C (using different sealing methods) to determine the impact of evaporation on drug concentration and thus on cell viability.  \n\u2003\u2003_(Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens, Scientific Reports, 2020)_\n\n2. Most relevant publications describing the test system/method  \n\u2003(a) Test system (cancer drug sensitivity screens using cell viability assays)  \n\u2003\u2003\u2013 Hafner et al. (2016, Nat Methods) provides a comprehensive overview of growth rate inhibition metrics (e.g. GR50, GRmax) and discusses how differences in cell growth rates influence drug potency estimates. This publication is fundamental because it shows how alternative endpoints (beyond the conventional IC50) can be used to assess drug response in cell\u2010based assays.  \n\u2003\u2003\u2003\u2003Information provided: Benchmarking drug sensitivity metrics, discussion of reproducibility challenges, and validation across multiple cell lines.  \n\u2003\u2003\u2003\u2003_(Sources: Optimization paper; Hafner et al., 2016)_  \n\n\u2003(b) Test method (resazurin reduction assay optimization)  \n\u2003\u2003\u2013 O\u2019Brien et al. (2000, Eur J Biochem) is a widely cited reference for the resazurin (alamarBlue) assay and includes evaluations of assay linearity, reproducibility, and sensitivity. It helps to understand limitations of the assay as well as best practices for endpoint measurement based on resazurin reduction.  \n\u2003\u2003\u2003\u2003Information provided: Detailed technical description of the resazurin assay, its sensitivity to cell density and incubation time, and guidance on reproducibility.  \n\u2003\u2003\u2003\u2003_(Source: Optimization paper; O\u2019Brien et al., 2000)_  \n\n3. Additional publications prioritized by relevance and type of information they provide  \n\u20031. Hafner et al. (2016, Nat Methods)  \n\u2003\u2003\u2022 Priority: High  \n\u2003\u2003\u2022 Comment: Contains a comprehensive discussion on drug response metrics adjusted for cell growth, provides validation against other methods, and is widely used for benchmark comparisons in drug sensitivity screens.  \n\u2003\u2003\u2003\u2003_(Sources: Optimization paper; Hafner et al., 2016)_  \n\n\u20032. O\u2019Brien et al. (2000, Eur J Biochem)  \n\u2003\u2003\u2022 Priority: High  \n\u2003\u2003\u2022 Comment: Offers detailed information on the resazurin-based assay including sensitivity, incubation timing, and technical reproducibility.  \n\u2003\u2003\u2003\u2003_(Source: Optimization paper; O\u2019Brien et al., 2000)_  \n\n\u20033. Iversen et al. (2006, J Biomol Screen)  \n\u2003\u2003\u2022 Priority: Medium  \n\u2003\u2003\u2022 Comment: Provides an evaluation of assay performance measures such as Z-factor, signal window, and coefficient of variation. This is useful for understanding and benchmarking assay quality control metrics.  \n\u2003\u2003\u2003\u2003_(Source: Optimization paper; Iversen et al., 2006)_  \n\n\u20034. Smirnov et al. (2018, Nucleic Acids Res) via PharmacoDB  \n\u2003\u2003\u2022 Priority: Medium  \n\u2003\u2003\u2022 Comment: Offers an integrative database view on in vitro anticancer drug screening studies, including matching of cell viability data across different studies, which helps in cross-validating our optimized system.  \n\u2003\u2003\u2003\u2003_(Source: Optimization paper; Smirnov et al., 2018)_  \n\nIn summary, our optimized protocol features several key deviations from older or conventional descriptions\u2014including adjustments to cell seeding density, medium composition, control strategies for solvent effects, plate selection to minimize evaporation, and extended resazurin incubation times\u2014all aimed at reducing variability and improving reproducibility. The listed publications provide comprehensive guidelines and validation metrics that underpin the improvements made in our test system and method.\n\n_(Sources: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens, Scientific Reports, 2020; Hafner et al., 2016; O\u2019Brien et al., 2000; Iversen et al., 2006; Smirnov et al., 2018)_"
        },
        {
          "question_id": 69,
          "question_text": "Indicate/summarize information on mechanistic validation, e.g. by omics approaches or by use of endpoint specific controls (MCC; section 5.5). Has it been explored in how far the system reflects human biology, signaling, tissue organization relevant to the form of toxicity to be assessed (e.g. nigrostriatal neurons should contain dopamine, liver tests relevant to cholestasis may need to contain bile canalicular structures, etc.)?",
          "answer_text": "The paper does not report any omics\u2010based or endpoint\u2010specific controls aimed at validating mechanistic aspects of the assay (for instance, evaluating whether a cell system recapitulates tissue\u2010specific features such as dopaminergic signaling or bile canalicular structures). Instead, the study focuses on optimizing general cell viability readouts (using the resazurin assay), supplementing this with measurements of proteasome activity and cell cycle arrest as indicators of drug effect. These endpoints are used to validate the cytotoxic action of drugs such as bortezomib and cisplatin, but there is no exploration of whether the system reflects detailed human biology, signaling, or tissue organization relevant to particular toxicity forms. \n\n(Source: Optimization of cell viability assays to improve replicability and reproducibility of cancer drug sensitivity screens.pdf)"
        },
        {
          "question_id": 70,
          "question_text": "Indicate/summarize activities for test qualification, pre-validation or validation. Indicate e.g. ring trials, full (pre-)validations. Give an overview of compounds or libraries that have been tested.",
          "answer_text": "The assay was rigorously qualified by first identifying and optimizing key experimental parameters (e.g., cell seeding density, drug storage conditions, DMSO solvent effects, evaporation and edge effects) to improve both replicability and reproducibility. In the pre\u2010validation/validation phase, the optimized resazurin reduction assay was performed using several independent experiments (each executed at least three times in triplicate) on multiple breast cancer cell lines (MCF7, HCC38, MCF-10A, and MDA-MB-436). This allowed the determination of dose-response metrics (IC50, GR50, and GRmax) for pharmacological compounds such as bortezomib, cisplatin, and carboplatin. Additionally, quality control metrics (Z-factor > 0.75, signal window >10 SD, and CV <5%) were used to confirm the assay\u2019s performance, and the data were compared with external datasets (from pharmacoDB and results by Hafner et al.) as a further layer of validation. No formal ring trial is reported; instead, repeated experiments under varied conditions and cross-comparisons with published data served to validate the test robustness and reproducibility.  \n(Sources: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        },
        {
          "question_id": 76,
          "question_text": "Are special permits (e.g. genetic work, stem cells, radioactivity, etc.) required? Are special facilities required? Is special ethical approval necessary (indicate approval document).",
          "answer_text": "The document makes no mention of the need for special permits (such as those for genetic work, use of stem cells, or radioactivity), nor does it indicate that special facilities or additional ethical approvals (with associated approval documents) are required. The work was performed using established cancer cell lines in standard cell culture laboratories. (Source: Optimization of cell viability assays to improve replicability and reproducibility.pdf)"
        }
      ]
    }
  ]
}