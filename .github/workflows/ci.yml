name: CI

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker
        uses: docker/setup-docker-action@v4

      - name: Build Docker image
        run: |
          # Ideally, we find a way to use caching here
          export GIT_HASH=$(git rev-parse HEAD)
          docker compose --profile test build \
          --build-arg TESTING=true \
          --build-arg GIT_HASH=$GIT_HASH \
          --build-arg ENV_FILE=.env.dummy
        env:
          POSTGRES_HOST: postgres_test_for_django

      - name: Test in Docker
        run: |
          echo "üî® Starting Docker compose tests‚Ä¶"
          export GIT_HASH=$(git rev-parse HEAD)

          # 1) bring up only Postgres in the background
          docker compose --profile test up -d postgres_test_for_django

          # 2) run pytest inside djangoapp, capturing its exit code
          docker compose --profile test run --rm \
            -e DJANGO_SETTINGS_MODULE=myocyte.settings \
            -e TESTING=true \
            djangoapp \
            sh -c "pytest -v \
              --junitxml=/home/myocte/test-results/results.xml \
              --cov=toxtempass \
              --cov-report=xml:/home/myocte/test-results/coverage.xml"
          code=$?

          # 3) tear down all containers & networks
          docker compose down

          # 4) report
          if [ $code -eq 0 ]; then
            echo "‚úÖ Tests passed!"
          else
            echo "‚ùå Tests failed with exit code $code"
          fi
          exit $code
        env:
          TESTING: "true"
          ENV_FILE: .env.dummy # Use dummy env for testing
          POSTGRES_HOST: postgres_test_for_django
          # OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} # Uncomment if you need to use OpenAI API in tests

      - name: Code Coverage Summary Report
        if: always()
        uses: irongut/CodeCoverageSummary@v1.3.0
        with:
          filename: test-results/coverage.xml

      - name: Surface failing tests
        if: always()
        uses: pmeier/pytest-results-action@main
        with:
          # A list of JUnit XML files, directories containing the former, and wildcard
          # patterns to process.
          # See @actions/glob for supported patterns.
          path: test-results/results.xml

          # (Optional) Add a summary of the results at the top of the report
          summary: true

          # (Optional) Select which results should be included in the report.
          # Follows the same syntax as `pytest -r`
          display-options: fEX

          # (Optional) Fail the workflow if no JUnit XML was found.
          fail-on-empty: true

          # (Optional) Title of the test results section in the workflow summary
          title: Test results

      - name: Upload test results
        if: always() # even on failure
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: test-results/
